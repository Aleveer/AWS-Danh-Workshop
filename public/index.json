[
{
	"uri": "http://localhost:1313/workshop-template/3-blogstranslated/3.1-blog1/",
	"title": "Blog 1",
	"tags": [],
	"description": "",
	"content": "Offline caching with AWS Amplify, Tanstack, AppSync and MongoDB Atlas In this blog we demonstrate how to create an offline-first application with optimistic UI using AWS Amplify, AWS AppSync, and MongoDB Atlas. Developers design offline first applications to work without requiring an active internet connection. Optimistic UI then builds on top of the offline first approach by updating the UI with expected data changes, without depending on a response from the server. This approach typically utilizes a local cache strategy.\nApplications that use offline first with optimistic UI provide a number of improvements for users. These include reducing the need to implement loading screens, better performance due to faster data access, reliability of data when an application is offline, and cost efficiency. While implementing offline capabilities manually can take sizable effort, you can use tools that simplify the process.\nWe provide a sample to-do application that renders results of MongoDB Atlas CRUD operations immediately on the UI before the request roundtrip has completed, improving the user experience. In other words, we implement optimistic UI that makes it easy to render loading and error states, while allowing developers to rollback changes on the UI when API calls are unsuccessful. The implementation leverages TanStack Query to handle the optimistic UI updates along with AWS Amplify. The diagram in Figure 1 illustrates the interaction between the UI and the backend.\nTanStack Query is an asynchronous state management library for TypeScript/JavaScript, React, Solid, Vue, Svelte, and Angular. It simplifies fetching, caching, synchronizing, and updating server state in web applications. By leveraging TanStack Query’s caching mechanisms, the app ensures data availability even without an active network connection. AWS Amplify streamlines the development process, while AWS AppSync provides a robust GraphQL API layer, and MongoDB Atlas offers a scalable database solution. This integration showcases how TanStack Query’s offline caching can be effectively utilized within a full-stack application architecture.\nFigure 1. Interaction Diagram\nThe sample application implements a classic to-do functionality and the exact app architecture is shown in Figure 2. The stack consists of:\nMongoDB Atlas for database services. AWS Amplify the full-stack application framework. AWS AppSync for GraphQL API management. AWS Lambda Resolver for serverless computing. Amazon Cognito for user management and authentication. Figure 2. Architecture\nDeploy the Application To deploy the app in your AWS account, follow the steps below. Once deployed you can create a user, authenticate yourself, and create to-do entries – see Figure 8.\nSet up the MongoDB Atlas cluster Follow the link to the setup the MongoDB Atlas cluster, Database, User and Network access\nSet up the user\nConfigure User\nClone the GitHub Repository Clone the sample application with the following command git clone https://github.com/mongodb-partners/amplify-mongodb-tanstack-offline\nSetup the AWS CLI credentials (optional if you need to debug your application locally) If you would like to test the application locally using a sandbox environment, you can setup temporary AWS credentials locally: export AWS_ACCESS_KEY_ID=\rexport AWS_SECRET_ACCESS_KEY=\rexport AWS_SESSION_TOKEN= Deploy the Todo Application in AWS Amplify Open the AWS Amplify console and Select the Github Option Figure 3. Select Github option\nConfigure the GitHub Repository Figure 4. Configure repository permissions\nSelect the GitHub Repository and click Next Figure 5. Select repository and branch\nSet all other options to default and deploy Figure 6. Deploy application\nConfigure the Environment Variables Configure the Environment variables after the successful deployment\nFigure 7. Configure environment variables\nOpen the application and test Open the application through the URL provided and test the application.\nFigure 8. Sample todo entries\nMongoDB Atlas Output\nFigure 9. Data in Mongo\nReview the Application Now that the application is deployed, let’s discuss what happens under the hood and what was configured for us. We utilized Amplify’s git-based workflow to host our full-stack, serverless web application with continuous deployment. Amplify supports various frameworks, including server side rendered (SSR) frameworks like Next.js and Nuxt, single page application (SPA) frameworks like React and Angular, and static site generators (SSG) like Gatsby and Hugo. In this case, we deployed a SPA React based application. We can include feature branches, custom domains, pull request previews, end-to-end testing, and redirects/rewrites. Amplify Hosting provides a Git-based workflow enables atomic deployments ensuring that updates are only applied after the entire deployment is complete.\nTo deploy our application we used AWS Amplify Gen 2, which is a tool designed to simplify the development and deployment of full-stack applications using TypeScript. It leverages the AWS Cloud Development Kit (CDK) to manage cloud resources, ensuring scalability and ease of use.\nBefore we conclude, it is important to understand our application’s updates concurrency. We implemented a simple optimistic first-come first-served conflict resolution mechanism. The MongoDB Atlas cluster persists updates in the order it receives them. In case of conflicting updates, the latest arriving update will override previous updates. This mechanism works well in applications where update conflicts are rare. It is important to evaluate how this may or may not suit your production needs, requiring more sophisticated approaches. TanStack provides capabilities for more complex mechanisms to handle various connectivity scenarios. By default, TanStack Query provides an “online” network mode, where Queries and Mutations will not be triggered unless you have network connection. If a query runs because you are online, but you go offline while the fetch is still happening, TanStack Query will also pause the retry mechanism. Paused queries will then continue to run once you re-gain network connection. In order to optimistically update the UI with new or changed values, we can also update the local cache with what we expect the response to be. This is approach works well together with TanStack’s “online” network mode, where if the application has no network connectivity, the mutations will not fire, but will be added to the queue, but our local cache can be used to update the UI. Below is a key example of how our sample application optimistically updates the UI with the expected mutation.\nconst createMutation = useMutation({\rmutationFn: async (input: { content: string }) =\u0026gt; {\r// Use the Amplify client to make the request to AppSync\rconst { data } = await amplifyClient.mutations.addTodo(input);\rreturn data;\r},\r// When mutate is called:\ronMutate: async (newTodo) =\u0026gt; {\r// Cancel any outgoing refetches\r// so they don\u0026#39;t overwrite our optimistic update\rawait tanstackClient.cancelQueries({ queryKey: [\u0026#34;listTodo\u0026#34;] });\r// Snapshot the previous value\rconst previousTodoList = tanstackClient.getQueryData([\u0026#34;listTodo\u0026#34;]);\r// Optimistically update to the new value\rif (previousTodoList) {\rtanstackClient.setQueryData([\u0026#34;listTodo\u0026#34;], (old: Todo[]) =\u0026gt; [\r...old,\rnewTodo,\r]);\r}\r// Return a context object with the snapshotted value\rreturn { previousTodoList };\r},\r// If the mutation fails,\r// use the context returned from onMutate to rollback\ronError: (err, newTodo, context) =\u0026gt; {\rconsole.error(\u0026#34;Error saving record:\u0026#34;, err, newTodo);\rif (context?.previousTodoList) {\rtanstackClient.setQueryData([\u0026#34;listTodo\u0026#34;], context.previousTodoList);\r}\r},\r// Always refetch after error or success:\ronSettled: () =\u0026gt; {\rtanstackClient.invalidateQueries({ queryKey: [\u0026#34;listTodo\u0026#34;] });\r},\ronSuccess: () =\u0026gt; {\rtanstackClient.invalidateQueries({ queryKey: [\u0026#34;listTodo\u0026#34;] });\r},\r}); We welcome any PRs implementing additional conflict resolution strategies.\nTry out MongoDB Atlas on AWS MarketPlace. Get familiar with AWS Amplify, Amplify Gen2 and AppSync. For detailed instructions on deploying the application, refer to the deployment section of our documentation. Submit a PR with your enhancements. "
},
{
	"uri": "http://localhost:1313/workshop-template/3-blogstranslated/3.2-blog2/",
	"title": "Blog 2",
	"tags": [],
	"description": "",
	"content": "Introducing AWS CDK Community Meetings We are excited to announce our new community meetings series for the AWS Cloud Development Kit (CDK) project. These meetings are designed to give everyone from seasoned contributors to new users a recurring opportunity to learn, ask questions, and share feedback directly with the AWS CDK team. We also see this as a great opportunity to engage with other members of the AWS CDK community.\nWhy Community Meetings? We originally considered forming a more formal governance model called the “Contributor Council”, but decided to pivot to a less formal, and more open and accessible format for a couple of reasons:\nBroader Inclusivity: We want to encourage participation from all interested developers, not just a small group. Simpler Engagement: A virtual meeting allows anyone in the community to attend, ask questions, and provide feedback without formalizing ownership or governance structures. By hosting virtual community meetings, we can continue delivering the transparency and collaboration that the community values while retaining the flexibility to make decisions and drive the project forward with AWS as the maintainer of the project. We believe this is the best way to engage with our community in a meaningful way.\nHow Frequently Can We Meet? In order to ensure we make the best use of everyone’s time, we aim to meet twice every quarter. However this schedule can get impacted by ongoing events and holidays, so we will publish the date of the next meeting along with the agenda in GitHub issues marked “[Community Meeting] – Date of meeting”.\nWhat to Expect We are planning to cover a mix of the following topics based on interest and relevancy:\nRoadmap Updates: Get a sneak peek at upcoming features, enhancements, and project priorities. Team Demos: Our engineers will showcase new functionality or best practices in action. RFC \u0026amp; Vision/Feature Reviews: Provide direct input on proposed features or directions for the AWS CDK project. Open Q\u0026amp;A: A chance for you to ask questions directly to the AWS CDK team and fellow community members. Future Topics \u0026amp; Sessions: Additional events like community-led sessions or office hours may be introduced based on interest. All sessions will be recorded and shared on YouTube, ensuring that anyone who can’t attend live can still catch up. We will also keep track of agendas and notes in GitHub issues marked “[Community Meeting] – 6/24/25”, so you can easily follow along with meeting highlights. If you have any questions or topics that you’d like to see covered in the meeting, please add a comment to the issue for the upcoming meeting.\nFirst Session: June 24, 2025, 8am – 9am PDT / June 24, 2025, 5pm – 6pm PDT To support our global community, we’ll have two sessions on June 24 – the first meeting will take place at 8am PDT (.ics file), followed by the second session at 5pm PDT (.ics file). We’ll share meeting details, including joining instructions, in the cdk.dev Slack Workspace and on the CDK GitHub issues marked “[Community Meeting] – 6/24/25” beforehand.\nHow You Can Participate Mark Your Calendar: Save the date for June 24, 2025, 8am – 9am PDT OR 5pm – 6pm PDT, and look out for our meeting link. Join us in Slack: For more information and to keep up to date on upcoming community meetings, join the #community-meetings channel in the cdk.dev Slack Workspace. Join the Live Session: Bring any questions or feedback you have about AWS CDK development, usage, or roadmap. Stay Updated via GitHub: Check our GitHub issues marked [Community Meeting] – “Date”, for the meeting agenda and notes. Watch the Recordings: If you can’t make the live event, watch the YouTube recordings at your convenience. Comment with questions or topics: Add comments to the GitHub issue for the upcoming meeting with any questions or topics you’d like us to cover. Why It Matters The AWS CDK team remains dedicated to driving the project forward and maintaining ownership, while continuing to work in the open with the community. We value the input and perspective from the community as they are vital for shaping the future of the AWS CDK. These meetings provide a recurring forum for everyone in the community to have their voice heard in a direct and interactive way, beyond the traditional avenues like GitHub issues or pull requests.\nTake our survey As a part of our commitment to build a strong and thriving community around the AWS CDK, we want to hear from you! Over the course of the year, we’ll be sending out surveys periodically to gain more insights into what’s working and where we can improve. Our first survey kicks off with this blog and will close on July 1, 2025.\nTake the survey now\nPlease save the date (June 24, 2025 @ 8am – 9am PDT OR 5pm – 6pm PDT) and plan to join us for the first AWS CDK community meeting. If you have ideas for agenda items or want to share a demo, feel free to propose them on the GitHub issue for the upcoming meeting. We look forward to connecting with you, answering your questions, and working together to make the AWS CDK even better.\nSee you then!\n"
},
{
	"uri": "http://localhost:1313/workshop-template/3-blogstranslated/3.3-blog3/",
	"title": "Blog 3",
	"tags": [],
	"description": "",
	"content": "Secure your Express application APIs in 5 minutes with Cedar Today, the open source Cedar project announced the release of authorization-for-expressjs, an open source package that simplifies using the Cedar policy language and authorization engine to verify application permissions. This release allows developers to add policy-based authorization to their Express web framework APIs within minutes, and without any remote service calls.\nExpress is a minimal and flexible Node.js web application framework that provides a robust set of features for web and mobile applications. This standardized integration with Cedar requires 90% less code compared to developers writing their own integration patterns, saving developers time and effort and improving application security posture by reducing the amount of custom integration code.\nFor example, if you are building a pet store application using the Express framework, using the authorization-for-expressjs feature you can create authorization policies so that only store employees can access the API to add a pet. This standardized implementation for Express authorization middleware replaces the need for custom code and automatically maps client requests into their principals, actions, and resources components, and then into Cedar authorization requests.\nWhy Externalize Authorization with Cedar? Traditionally, developers implemented authorization within their application by embedding authorization logic directly into application code. This embedded authorization logic is designed to support a few permissions, but as applications evolve, there is often a need to support more complex use cases with additional authorization requirements. Developers incrementally update the embedded authorization logic to support these complex use cases, resulting in code that is complex and difficult to maintain. As code complexity increases, further evolving the security model and performing audits of permissions becomes more challenging, resulting in an application that continuously becomes more difficult to maintain over its lifecycle.\nCedar allows you to decouple authorization logic from your application. Externalizing authorization from your application code yields multiple benefits including freeing up development teams to focus on application logic and simplifying application and resource access audits. Cedar is an open source language and software development kit (SDK) for writing and enforcing authorization policies for your applications. You specify fine-grained permissions as Cedar policies, and your application authorizes access requests by calling the Cedar SDK. For example, you can use the below Cedar policy permit employee users to call the POST /pets API in a sample PetStore application.\npermit (\rprincipal,\raction in [Action::\u0026#34;POST /pets\u0026#34;], resource\r) when {\rprincipal.jobLevel = \u0026#34;employee\u0026#34;\r}; One potential challenge in adopting Cedar can be the upfront effort required to define Cedar policies and update your application code to call the Cedar SDK to authorize API requests. This blog post shows how web application developers using the Express framework can easily implement API-level authorization with Cedar—adding just tens of lines of code in your applications, instead of hundreds.\nThis step-by-step guide uses the sample PetStore application to show how access to API’s can be restricted based on user groups. You can find the sample Pet Store application in the cedar-policy repository on GitHub.\nPet Store application API overview The PetStore application is used to manage a pet store. The pet store is built using Express on Node.js and exposes the API’s here:\nGET /pets – returns a page of available pets in the PetStore. POST /pets – adds the specified pet to the PetStore. GET /pets/{petId} – returns the specified pet found in the PetStore. POST /pets/{petId}/sale – marks a pet as sold. This application does not allow all users to access all APIs. Instead, it enforces the following rules:\nBoth customer users and employees are allowed to perform read operations.\nGET /pets\nGET /pets/{petId}\nOnly employees are allowed to perform write operations.\nPOST /pets\nPOST /pets/{petId}/sale\nImplementing authorization for the Pet Store APIs Let’s walk through how to secure your application APIs using Cedar using the new package for Express. The initial application, with no authorization, can be found in the start folder; use this to follow along with the blog. You can find the completed application, with authorization added, in the finish folder.\nAdd the Cedar Authorization Middleware package The Cedar Authorization Middleware package will be used to generate a Cedar schema, create sample authorization policies, and perform the authorization in your application.\nRun this npm command to add the @cedar-policy/authorization-for-expressjs dependency to your application.\nnpm i --save @cedar-policy/authorization-for-expressjs Generate a Cedar Schema from your APIs A Cedar schema defines the authorization model for an application, including the entities types in the application and the actions users are allowed to take. Your policies are validated against this schema when you run the application.\nThe authorization-for-expressjs package can analyze the OpenAPI specification of your application and generate a Cedar schema. Specifically the paths object is required in the your specification.\nNote: If you do not have an OpenAPI spec you can generate one using the tool of your choice. There are a number of open source libraries to do this for Express; you may need to add some code to your application, generate the OpenAPI spec, and then remove the code. Alternatively, some generative AI based tools such as the Amazon Q Developer CLI are effective at generating OpenAPI spec documents. Regardless of how you generate the spec, be sure to validate the correct output from the tool.\nFor the sample application, an OpenAPI spec document named openapi.json has been included.\nWith an OpenAPI spec you can generate a Cedar schema by running the generateSchema command listed here.\n// schema is stored in v4.cedarschema.json file in the package root. npx @cedar-policy/authorization-for-expressjs generate-schema --api-spec openapi.json --namespace PetStoreApp --mapping-type SimpleRest Define authorization policies If no policies are configured, Cedar denies all authorization requests. We will add policies that grant access to APIs only in authorized user groups.\nRun this command to generate sample Cedar policies. You can then customize these policies based on your use case.\nnpx @cedar-policy/authorization-for-expressjs generate-policies --schema v4.cedarschema.json In the PetStore application two sample policies are generated, policy_1.cedar and policy_2.cedar.\npolicy_1.cedar provides permissions for users in the admin user group to perform any action on any resource.\n// policy_1.cedar\r// Allows admin usergroup access to everything\rpermit (\rprincipal in PetStoreApp::UserGroup::\u0026#34;admin\u0026#34;,\raction,\rresource\r); policy_2.cedar provides more access to all the individual actions defined in the Cedar schema with a place holder for a specific group.\n// policy_2.cedar\r// Allows more granular user group control, change actions as needed\rpermit (\rprincipal in PetStoreApp::UserGroup::\u0026#34;ENTER_THE_USER_GROUP_HERE\u0026#34;,\raction in\r[PetStoreApp::Action::\u0026#34;GET /pets\u0026#34;,\rPetStoreApp::Action::\u0026#34;POST /pets\u0026#34;,\rPetStoreApp::Action::\u0026#34;GET /pets/{petId}\u0026#34;,\rPetStoreApp::Action::\u0026#34;POST /pets/{petId}/sale\u0026#34;],\rresource\r); Note that if you specified an operationId in the OpenAPI specification, the action names defined in the Cedar Schema will use that operationId instead of the default “ /” format. In this case ensure the naming of your Actions in your Cedar Policies matches the naming of your Actions in your Cedar Schema.\nFor example if you wish to call your action AddPet instead of POST /pets you could set the operationId in your OpenAPI specification to AddPet. The resulting action in the Cedar policy would be PetStoreApp::Action::\u0026quot;AddPet\u0026quot;\nSince we don’t have an admin user in our use case, we can just replace the contents of policy_1.cedar with the policies used for the customer user group.\nIn a real use case, consider renaming your Cedar policy files based on their contents. For example, allow_customer_group.cedar\n// policy_1.cedar\r// Allows customer user group access getAllPets and getPetById\rpermit (\rprincipal in PetStoreApp::UserGroup::\u0026#34;customer\u0026#34;,\raction in\r[PetStoreApp::Action::\u0026#34;GET /pets\u0026#34;,\rPetStoreApp::Action::\u0026#34;GET /pets/{petId}\u0026#34;],\rresource\r); The employee user has access to all API operations. We can simply add the employee group in the policy_2.cedar file to fulfill the authorization requirements for employee users.\n// policy_2.cedar\r// Allows employee user group access to all API actions\rpermit (\rprincipal in PetStoreApp::UserGroup::\u0026#34;employee\u0026#34;,\raction in\r[PetStoreApp::Action::\u0026#34;GET /pets\u0026#34;,\rPetStoreApp::Action::\u0026#34;POST /pets\u0026#34;,\rPetStoreApp::Action::\u0026#34;GET /pets/{petId}\u0026#34;,\rPetStoreApp::Action::\u0026#34;POST /pets/{petId}/sale\u0026#34;],\rresource\r); Note: For large applications with complex authorization policies, it can be challenging to analyze and audit the actual permissions provided by the many different policies. We also recently open sourced the Cedar Analysis CLI to help developers perform policy analysis on their policies. You can find out more about this new tool in the blog post Introducing Cedar Analysis: Open Source Tools for Verifying Authorization Policies.\nUpdate the application code to call Cedar and to authorize API access The application will use the Cedar middleware to authorize every request against the Cedar polices. Earlier we installed the dependency, now we need to update the code.\nFirst add the package to the project and define the CedarInlineAuthorizationEngine and ExpressAuthorizationMiddleware. This block of code can be added to the top of the app.js file.\nconst { ExpressAuthorizationMiddleware, CedarInlineAuthorizationEngine } = require (\u0026#39;@cedar-policy/authorization-for-expressjs\u0026#39;);\rconst policies = [\rfs.readFileSync(path.join(__dirname, \u0026#39;policies\u0026#39;, \u0026#39;policy_1.cedar\u0026#39;), \u0026#39;utf8\u0026#39;),\rfs.readFileSync(path.join(__dirname, \u0026#39;policies\u0026#39;, \u0026#39;policy_2.cedar\u0026#39;), \u0026#39;utf8\u0026#39;)\r];\rconst cedarAuthorizationEngine = new CedarInlineAuthorizationEngine({\rstaticPolicies: policies.join(\u0026#39;\\n\u0026#39;),\rschema: {\rtype: \u0026#39;jsonString\u0026#39;,\rschema: fs.readFileSync(path.join(__dirname, \u0026#39;v4.cedarschema.json\u0026#39;), \u0026#39;utf8\u0026#39;),\r}\r});\rconst expressAuthorization = new ExpressAuthorizationMiddleware({\rschema: {\rtype: \u0026#39;jsonString\u0026#39;,\rschema: fs.readFileSync(path.join(__dirname, \u0026#39;v4.cedarschema.json\u0026#39;), \u0026#39;utf8\u0026#39;),\r},\rauthorizationEngine: cedarAuthorizationEngine,\rprincipalConfiguration: {\rtype: \u0026#39;custom\u0026#39;,\rgetPrincipalEntity: principalEntityFetcher\r},\rskippedEndpoints: [\r{httpVerb: \u0026#39;get\u0026#39;, path: \u0026#39;/login\u0026#39;},\r{httpVerb: \u0026#39;get\u0026#39;, path: \u0026#39;/api-spec/v3\u0026#39;},\r],\rlogger: {\rdebug: s =\u0026gt; console.log(s),\rlog: s =\u0026gt; console.log(s),\r}\r}); Next add the Express Authorization middleware to the application\nconst app = express();\rapp.use(express.json());\rapp.use(verifyToken()) // validate user token\r// ... other pre-authz middlewares\rapp.use(expressAuthorization.middleware);\r// ... other pre-authz middlewares Add application code to configure the user The Cedar authorizer requires user groups and attributes to authorize requests. The authorization middleware relies on the function passed to getPrincipalEntity in the initial configuration to generate the principal entity. You need to implement this function to generate the user entity.\nThis example code provides a function to generate a user entity. It assumes that the user has been authenticated by a previous middleware and the relevant information stored in the request object. It also assumes user sub has been stored in req.user.sub field and user groups have been stored in req.user.groups field.\nasync function principalEntityFetcher(req) {\rconst user = req.user; // it\u0026#39;s common practice for the authn middleware to store the user info from the decoded token here\rconst userGroups = user[\u0026#34;groups\u0026#34;].map(userGroupId =\u0026gt; ({\rtype: \u0026#39;PetStoreApp::UserGroup\u0026#39;,\rid: userGroupId }));\rreturn {\ruid: {\rtype: \u0026#39;PetStoreApp::User\u0026#39;,\rid: user.sub\r},\rattrs: {\r...user,\r},\rparents: userGroups };\r} Update the authentication middleware For the sample PetStore application, the authentication middleware is provided by the code in middleware/authnMiddleware.js which parses a JSON web token (JWT) included in the Authorization header of the request and stores the relevant values in the request object.\nNote: authnMiddleware.js is just used for demonstrative purposes and should not replace your actual token validation middleware in a real application.\nTo update the authentication middleware to use your own OpenID Connect (OIDC) identity provider, update the jwksUri in the following code block of middleware/authnMiddleware.js to include the JSON web key set (JWKS) uri of your identity provider.\nconst client = jwksClient({\rjwksUri: \u0026#39;\u0026lt;jwks uri for your oidc identity provider\u0026gt;\u0026#39;,\rcache: true,\rcacheMaxEntries: 5,\rcacheMaxAge: 600000 // 10 minutes\r}); Next update the issuer in the following code block to include the issuer uri of your identity provider.\njwt.verify(token, getSigningKey, {\ralgorithms: [\u0026#39;RS256\u0026#39;],\rissuer: `\u0026lt;issuer uri for your oidc identity provider\u0026gt;`\r}, (err, decoded) =\u0026gt; {\rif (err) {\rconsole.error(\u0026#39;JWT verification error:\u0026#39;, err);\rreturn res.status(401).json({ message: \u0026#39;Invalid token\u0026#39; });\r}\r// Add the decoded token to the request object\rreq.user = decoded;\rnext();\r}); If you do not have access to an OIDC identity provider to use with this sample, for testing purposes you can replace the entire verifyToken function and just map a sample user entity to the request object. For example replace verifyToken with this:\nconst verifyToken = (req, res, next) =\u0026gt; {\r// Add a sample user entity to the request object\r// To test an employee group change \u0026#34;customer\u0026#34; to \u0026#34;employee\u0026#34;\rreq.user = {\r\u0026#34;sub\u0026#34;: \u0026#34;some-user-id\u0026#34;,\r\u0026#34;groups\u0026#34;: \u0026#34;customer\u0026#34;\r};\r}; Validating API security You can validate your policies and API access by calling the application using terminal-based curl commands. We assume that the application is using an OIDC identity provider for user management and JWT tokens are passed in the authorization header for API requests.\nFor readability, a set of environment variables are used to represent the actual values. TOKEN_CUSTOMER contains valid identity tokens for users in the employee group. API_BASE_URL is the base URL for the tiny PetStore API .\nTo test that a customer is allowed to call GET /pets, run this curl command. The request should complete successfully.\ncurl -H \u0026#34;Authorization: Bearer ${TOKEN_CUSTOMER}\u0026#34; -X GET ${API_BASE_URL}/pets The successful request will return the list of pets. To begin with, the Pet Store has one pet and returns a response similar to this.\n[{\u0026#34;id\u0026#34;:\u0026#34;6da5d01b-89fd-49b9-acb2-b457b79669d5\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;Fido\u0026#34;,\u0026#34;species\u0026#34;:\u0026#34;Dog\u0026#34;,\u0026#34;breed\u0026#34;:null,\u0026#34;age\u0026#34;:null,\u0026#34;sold\u0026#34;:false}] To test that a customer is not allowed to call POST /pets, run this curl command. You should receive an error message that the request is unauthorized.\ncurl -H \u0026#34;Authorization: Bearer ${TOKEN_CUSTOMER}\u0026#34; -X POST ${API_BASE_URL}/pets The unauthorized request will return Not authorized with explicit deny\nConclusion The new authorization-for-expressjs package allows developers to integrate their applications with Cedar in order to decouple authorization logic from code in just a few minutes. By decoupling your authorization logic and integrating your application with Cedar, you can both improve developer productivity, and simplify permissions and access audits.\nThe frameworks packages are open source and available on GitHub under the Apache 2.0 license, with distribution through NPM. To learn more about Cedar and try it using the language playground, visit https://www.cedarpolicy.com/. Feel free to submit questions, comments, and suggestions via the public Cedar Slack workspace, https://cedar-policy.slack.com.\n"
},
{
	"uri": "http://localhost:1313/workshop-template/3-blogstranslated/3.4-blog4/",
	"title": "Blog 4",
	"tags": [],
	"description": "",
	"content": "Amazon Bedrock baseline architecture in an AWS landing zone As organizations increasingly adopt Amazon Bedrock to build and deploy large-scale AI applications, it’s important that they understand and adopt critical network access controls to protect their data and workloads. These generative AI-enabled applications might have access to sensitive or confidential information within their knowledge bases, Retrieval Augmented Generation (RAG) data sources, or models themselves, which could pose a risk if exposed to unauthorized parties. Additionally, organizations might want to limit access to certain AI models to specific teams or services, making sure only authorized users can use the most powerful capabilities. Another important consideration is cost optimization, because organizations need to be able to monitor and control access to manage various aspects of their cloud spending.\nIn this post, we explore the Amazon Bedrock baseline architecture and how you can secure and control network access to your various Amazon Bedrock capabilities within AWS network services and tools. We discuss key design considerations, such as using Amazon VPC Lattice auth policies, Amazon Virtual Private Cloud (Amazon VPC) endpoints, and AWS Identity and Access Management (IAM) to restrict and monitor access to your Amazon Bedrock capabilities.\nBy the end of this post, you will have a better understanding of how to configure your AWS landing zone to establish secure and controlled network connectivity to Amazon Bedrock across your organization using VPC Lattice.\nSolution overview Addressing the aforementioned challenges requires a well-designed network architecture and security controls. For this, we use the standard AWS Landing Zone Accelerator networking configuration. It provides a good starting point for managing network communication across multiple accounts. On top of the AWS Landing Zone Accelerator network design, we add two shared accounts.\nIn this solution design, we create a centralized architecture for managing organization AI capabilities across different accounts. The architecture consists of three main parts that work together to provide secure and controlled access to AI services:\nService network account – This account serves as the central networking hub for the organization, managing network connectivity and access policies. Through this account, network administrators can centrally manage and control access to AI services across the organization. The account follows AWS Landing Zone Accelerator networking practices that scale with enterprise organizational needs. Generative AI account – This account hosts the organization’s Amazon Bedrock capabilities and serves as the central point for AI/ML management. The organization’s AI/ML scientists and prompt engineers will centrally build and manage Amazon Bedrock capabilities. The account provides access to various large language models (LLMs) through Amazon Bedrock by using VPC interface endpoints, while also enabling centralized monitoring of cost consumption and access patterns. Workload accounts (dev, test, prod) – These accounts represent different environments where teams develop and deploy applications that consume AI services. Through secure network connections established through the service network account, these workload accounts can access the AI capabilities hosted in the generative AI account. This separation enforces proper isolation between development, testing, and production workloads while maintaining secure access to AI services.\nAmazon Bedrock baseline architecture in an AWS landing zone\nThe following diagram illustrates the solution architecture.\nThe service network account has its own VPC Lattice service network—a centralized networking construct that enables service-to-service communication across your organization, which is shared with workload accounts using AWS Resource Access Manager (AWS RAM) to enable VPC Lattice Service network sharing.\nWorkload accounts (dev, test, prod) establish VPC associations with the shared VPC Lattice service network by creating a service network association in their VPC. When an application in these accounts makes a request, it first queries the VPC resolver for DNS resolution. The resolver routes the traffic to the VPC Lattice service network.\nAccess control is implemented through an VPC Lattice auth policy. The service network policies determine which accounts can access the VPC Lattice service network, and service-level policies control access to specific AI services and define what actions each account can perform.\nIn the central AI services account, we find the proxy layer, we create a VPC Lattice service that points to a proxy layer, which acts as a single entry point, providing workload accounts access to Amazon Bedrock. This proxy layer then connects to Amazon Bedrock through VPC endpoints. Through this setup, the AI team can configure which foundation models (FMs) are available and manage access permissions for different workload accounts. After the necessary policies and connections are in place, workload accounts can access Amazon Bedrock capabilities through the established secure pathway. This setup enables secure, cross-account access to AI services while maintaining centralized control and monitoring.\nNetwork components We use VPC Lattice, which is a fully managed application networking service that helps you simplify network connectivity, security, and monitoring for service-to-service communication needs. With VPC Lattice, organizations can achieve a centralized connectivity pattern to control and monitor access to the services required for building generative AI applications.\nFor details about VPC Lattice, refer to the Amazon VPC Lattice User Guide. The following is an overview of the constructs you can use in setting up the centralized pattern in this solution:\nVPC Lattice service network – You can use the VPC Lattice service network to provide central connectivity and security to the central AI services account. The service network is a logical grouping mechanism that simplifies how you can enable connectivity across VPCs or accounts, and apply common security policies for application communication patterns. You can create a service network in an account and share it with other accounts within or outside AWS Organizations using AWS RAM. VPC Lattice service – In a service network, you can associate a VPC Lattice service, which consists of a listener (protocol and port number), routing rules that allow for control of the application flow (for example, path, method, header-based, or weighted routing), and target group, which defines the application infrastructure. A service can have multiple listeners to meet various client capabilities. Supported protocols include HTTP, HTTPS, gRPC, and TLS. The path-based routing allows control to various high-performing FMs and other capabilities you would need to build a generative AI application. Proxy layer – You use a proxy layer for the VPC Lattice service target group. The proxy layer can be built based on your organization’s preference of AWS services, such as AWS Lambda , AWS Fargate , or Amazon Elastic Kubernetes Service (Amazon EKS). The purpose of the proxy layer is to provide a single entry point to access LLMs, knowledge bases, and other capabilities that are tested and approved according to your organization’s compliance requirements. VPC Lattice auth policies – For security, you use VPC Lattice auth policies. VPC Lattice auth policies are specified using the same syntax as IAM policies. You can apply an auth policy to VPC Lattice service network as well as to the VPC Lattice service. Fully Qualified Domain Names – To facilitate service discovery, VPC Lattice supports custom domain names for your services and resources, and maintains a Fully Qualified Domain Name (FQDN) for each VPC Lattice service and resource you define. You can use these FQDNs in your Amazon Route 53 private hosted zone configurations, and empower business units or teams to discover and access services and resources. Service network VPC – Business units or teams can access generative AI services in a service network using service network VPC associations or a service network VPC endpoint. Monitoring – You can choose to enable monitoring at the VPC Lattice service network level and VPC Lattice service level. VPC Lattice generates metrics and logs for requests and responses, making it more efficient to monitor and troubleshoot applications\nThe preceding guidance takes a “secure by default” approach—you must be explicit about which features, models, and so on should be accessed by which business unit. The setup also enables you to implement a defense-in-depth strategy at multiple layers of the network:\nThe first level of defense is that business team needs to connect to the service network in order to get access to the generative AI service through the central AI service account. The second level includes network-level security protections in the business team’s VPC for the service network, such as security groups and network access control lists (ACLs). By using these, you can allow access to specific workloads or teams in a VPC. The third level is through the VPC Lattice auth policy, which you can apply at two layers: at the service network level to allow authenticated requests within the organization, and at the service level to allow access to specific models and features. VPC Lattice auth policy This solution makes it possible to centrally manage access to Amazon Bedrock resources across your organization. This approach uses an VPC Lattice auth policy to centrally control Amazon Bedrock resources and manage it from one location across all the organization accounts.\nTypically, the auth policy on the service network is operated by the network or cloud administrator. For example, allowing only authenticated requests from specific workloads or teams in your AWS organization. In the following example, access is granted to invoke the generated AI service for authenticated requests and to principals that are part of the o-123456example organization:\n{\r\u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;,\r\u0026#34;Statement\u0026#34;: [\r{\r\u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;,\r\u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;,\r\u0026#34;Action\u0026#34;: \u0026#34;vpc-lattice-svcs:Invoke\u0026#34;,\r\u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34;,\r\u0026#34;Condition\u0026#34;: {\r\u0026#34;StringEquals\u0026#34;: {\r\u0026#34;aws:PrincipalOrgID\u0026#34;: [ \u0026#34;o-123456example\u0026#34;\r]\r}\r}\r}\r]\r} The auth policy at the service level is managed by the central AI service team to set fine-grained controls, which can be more restrictive than the coarse-grained authorization applied at the service network level. For example, the following policy restricts access to claude-3-haiku for only business-team1:\n{\r\u0026#34;Version\u0026#34;:\u0026#34;2012-10-17\u0026#34;,\r\u0026#34;Statement\u0026#34;:[\r{\r\u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;,\r\u0026#34;Principal\u0026#34;: {\r\u0026#34;AWS\u0026#34;: [\r\u0026#34;arn:aws:iam::\u0026lt;account-number\u0026gt;:role/businss-team1\u0026#34;\r]\r},\r\u0026#34;Action\u0026#34;: \u0026#34;vpc-lattice-svcs:Invoke\u0026#34;,\r\u0026#34;Resource\u0026#34;: [\r\u0026#34;arn:aws:vpc-lattice:\u0026lt;aws-region\u0026gt;:\u0026lt;account-number\u0026gt;:service/svc-0123456789abcdef0/*\u0026#34;\r],\r\u0026#34;Condition\u0026#34;: {\r\u0026#34;StringEquals\u0026#34;: {\r\u0026#34;vpc-lattice-svcs:RequestQueryString/modelid\u0026#34;: \u0026#34;claude-3-haiku\u0026#34; }\r}\r}\r]\r} Monitoring and tracking This design employs three monitoring approaches, using Amazon CloudWatch, AWS CloudTrail, and VPC Lattice access logs. This strategy provides a view of service usage, security, and performance.\nCloudWatch metrics offer real-time monitoring of VPC Lattice service performance and usage. CloudWatch tracks metrics such as request counts and response times for Amazon Bedrock related endpoints, allowing for the setup of alarms for proactive management of service health and capacity. This enables monitoring of overall usage patterns of Amazon Bedrock models across different business units, facilitating capacity planning and resource allocation. CloudTrail provides detailed API-level auditing of Amazon Bedrock related actions. It logs cross-account access attempts and interactions with Amazon Bedrock services, providing a compliance and security audit trail. This tracking of who is accessing which Amazon Bedrock models, when, and from which accounts helps organizations adhere to their organizational policies.VPC Lattice access logs provide detailed insights into HTTP/HTTPS requests to Amazon Bedrock services, capturing specific usage patterns of AI models by different business teams. These logs contain client-specific information, which for example can be used to enable organizations to implement capabilities such as charge-back models. This allows for accurate attribution of AI service usage to specific teams or departments, facilitating fair cost allocation and responsible resource utilization across the organization. These services work together to enhance security, optimize performance, and provide valuable insights for managing cross-account Amazon Bedrock access.\nConclusion In this post, we explored the importance of securing and controlling network access to Amazon Bedrock capabilities within an organization’s AWS landing zone. We discussed the key business challenges, such as the need to protect sensitive information in Amazon Bedrock knowledge bases, limit access to AI models, and optimize cloud costs by monitoring and controlling Amazon Bedrock capabilities. To address these challenges, we outlined a multi-layered network solution that uses AWS networking services, including a VPC Lattice auth policy to restrict and monitor access to Amazon Bedrock capabilities. Try out this solution for your own use case, and share your feedback in the comments.\n"
},
{
	"uri": "http://localhost:1313/workshop-template/3-blogstranslated/3.5-blog5/",
	"title": "Blog 5",
	"tags": [],
	"description": "",
	"content": "Modernizing SAP Procurement Processes with Amazon Appflow, SAP BTP Integration Suite, and Amazon Bedrock Many of our customers are seeking guidance on how generative AI can support their enterprise-wide modernization strategy. Amazon Bedrock is a fully managed service that provides access to a wide range of high-performing foundation models (FMs) from leading AI companies such as Anthropic, AI21 Labs, Cohere, Stability AI, Mistral, Meta Llama, Amazon Nova and Amazon Titan.\nAmazon Bedrock offers a comprehensive set of capabilities to build generative AI applications, simplifying development while maintaining privacy and security. Key features include model customization with your own data, fine-tuning for specific tasks, and Retrieval Augmented Generation (RAG) to enhance response accuracy using your company’s knowledge base. Bedrock also supports building intelligent agents that can automate tasks by interacting with your enterprise systems.\nEnterprises have high expectations for security and infrastructure reliability, particularly for critical workloads such as those running on SAP. Amazon Bedrock meets these expectations by incorporating robust security measures, including data encryption and compliance with standards like SOC and ISO. This facilitates a secure environment, making Bedrock an ideal choice for developing innovative AI-driven applications that meet the stringent requirements of enterprise-grade solutions.\nEnhance Procurement: The Power of Generative AI in Sustainable Sourcing\nToday’s procurement practices face significant challenges, particularly in the time-consuming process of reviewing multiple supplier quotations and the complex task of gathering sustainability data from disparate sources. Generative AI is emerging as a transformative solution, with examples like the Sustainability Sourcing Assistant (powered by Amazon Bedrock) demonstrating how AI can leverage both SAP \u0026amp; Non-SAP data to transform procurement processes and enhance sustainability initiatives throughout the supply chain. Using Amazon Bedrock, SAP customers can provide natural language prompts to analyze quotation information from SAP systems. It then combines this data with non-SAP sustainability metrics to recommend the optimal quotation based on business-specific sustainability criteria. The system also enables users to create purchase orders in SAP using natural language commands, streamlining the entire procurement process.\nThe example addresses key challenges faced by sustainability sourcing specialists including:\nTime-consuming quotation reviews, the need to access multiple external data sources. Difficulties in obtaining comprehensive sustainability information. Difficulty in aligning quotations with specific organizational goals (in this case, sustainability). Solution Overview Exploring the Potential of AI in Procurement: A Sustainable Sourcing Assistant Use Case\nIn this blog post, we’ll explore an innovative example of how AI can transform business processes through a conceptual AI-powered assistant. It’s important to note that this is not a turn-key product or service, but rather a demonstration of how various technologies can be combined to address common business challenges in procurement. This use case serves as an inspiration for how similar approaches could be adopted and customized for different functional areas within your organization, whether it’s manufacturing, finance, human resources, or other business operations.\nThroughout this blog post, we’ll discuss various architectural options and AWS services that can be implemented through a combination of chatbot technology, natural language processing, Amazon Bedrock Agents, and integration with SAP BTP Integration Suite. While we focus on one specific example, this flexibility allows organizations to choose components that best align with their requirements, existing infrastructure, and technical preferences, making the underlying principles and technologies adaptable to a wide range of business processes and use cases.\nPotential Benefits of AI-Assisted Procurement\nIn our example use case, we demonstrate how generative AI could be used to:\nStreamline Evaluation: Automate the quotation assessment process, potentially reducing the time spent on evaluations. Integrate Data: Tap into both internal and external data sources, providing more comprehensive information for decision-making. Enhance Objectivity: Utilize Generative AI driven approaches to support unbiased selection processes that adhere to predefined principles. Streamlined Processes: Generative AI can help automate subsequent steps, such as generating purchase orders, seamlessly transitioning from Request for Quotation (RFQ) to creating a purchase order. Reduce human error: Generative AI can enhance accuracy and consistency in business processes by automatically validating data inputs, identifying patterns of errors, and providing real-time guidance to users before mistakes are committed. Now let’s delve into the technical architecture behind our conceptual AI-powered procurement use case. This example demonstrates how various AWS services and SAP technologies can be combined to create a powerful tool for procurement processes. We will be using a chatbot style interface to query RFQ’s in near real-time, augment this SAP data with third party sustainability data, and finally create a purchase order using the assistant. Again, it’s important to note that this is not a turn-key product, but rather an illustration of what’s possible when leveraging these technologies.\nDetailed Architecture: high level steps \u0026 architecture diagram\rThis solution has been built and tested on SAP S/4HANA 2023 and can be implemented across various deployment options including RISE with SAP, SAP on AWS, Native AWS installations, or on-premises environments. The solution architecture demonstrates a flexible approach to implementation, allowing organizations to adapt the components based on their specific needs and existing technology landscape. While our example showcases a specific combination of AWS and SAP services, it’s important to understand that there are three key layers where alternative technologies can be employed without compromising the overall functionality of the solution.\nCore Components and Their Alternatives:\nData Integration Layer Our Example: Amazon AppFlow with SAP OData Connector Alternative AWS Glue connection for SAP (using ODATA) Purpose: Handles near real-time data extraction from SAP systems Processing \u0026amp; Intelligence Layer Our Example: Amazon Bedrock with Agent functionality Alternative: SAP Generative AI Hub (with choice of models) Purpose: Manages AI processing and orchestration User Interface Layer Our Example: Streamlit Application Alternative: SAP AI Core and Build Apps Purpose: Provides the user interface for interaction Architectural Flow (Referenced in high level steps \u0026amp; architecture diagram):\nData Extraction: Amazon AppFlow connects to SAP OData Connector for near real-time data extraction Data Storage: Amazon S3 stores extracted data in JSON format User Interface: A chatbot assistant receives natural language inputs from users Processing: Bedrock Agent interprets user input, leveraging its chat history and underlying Foundation Model Action Orchestration: Bedrock Agent is configured with Action Groups to manage processing steps Data Querying: Lambda functions translate natural language to SQL queries for Athena database SAP Integration: AWS Lambda invokes BTP API to create Purchase Orders in SAP Secure Connectivity: SAP Cloud Connector creates a secure tunnel between AWS and BTP accounts Knowledge Enhancement: Amazon Bedrock’s Knowledge Base provides managed RAG for additional context Data Preparation: S3 bucket data is synced and transformed into embeddings for machine learning use Response Generation: The agent curates a final response, delivered via a Streamlit app Supporting Infrastructure:\nData Storage: Amazon S3 (JSON format) Query Processing: AWS Lambda with Amazon Athena Integration Components: SAP BTP Integration Suite, SAP Cloud Connector Knowledge Management: Amazon Bedrock Knowledge Base for RAG Data Preparation: S3 bucket with embedding transformation This modular approach allows organizations to mix and match components based on their existing technology stack, preferences, and requirements while maintaining the core functionality of the solution.\nLet’s explore the key AWS services that power this solution and their specific roles, before we demonstrate the Assistant in action through practical examples.\nAmazon S3 Amazon AppFlow AWS Glue Amazon Athena AWS Lambda Amazon Bedrock Amazon S3: S3 functions as a central data lake, receiving data directly from S/4HANA via AppFlow and storing it in analytics-optimized formats including CSV, JSON, or Parquet. The solution combines AppFlow for data ingestion, S3 for scalable storage, and Athena for SQL-based analysis, with AWS Glue cataloging the data for easier discovery.\nAmazon AppFlow: Amazon AppFlow extracts quotation and RFQ data from SAP into Amazon S3 in near real-time, using a secure SAP connector without requiring custom integrations. It employs Change Data Capture to detect and transfer only new or modified records, ensuring the S3 data lake maintains the most current information.\nAWS Glue: AWS Glue crawler automatically discovers, catalogs, and organizes data stored in S3 buckets, making it easily searchable and analyzable. It’s a fully managed ETL service that streamlines data preparation and loading for analytics purposes.\nAmazon Athena: Athena is a serverless query service that analyzes data directly in S3 using standard SQL, supporting various data formats and automatically parallelizing queries for distributed execution. It integrates with AWS Glue Data Catalog and supports complex analysis while maintaining a pay-per-query pricing model.\nAWS Lambda: Lambda functions as an intermediary between Amazon Bedrock Agents and SAP API, handling the technical integration for purchase order processing. It manages authentication, data transformation, and error handling, enabling scalable, cost-effective processing of AI-driven purchase order requests.\nAmazon Bedrock: Bedrock, utilizing Claude 3 Sonnet v1, provides advanced natural language processing capabilities for analyzing quotations based on price and sustainability. It offers serverless architecture for easy deployment and includes Model Evaluation features to ensure accuracy in tasks like quotation analysis.\nSecurity Considerations:\nIn this blog example, we have assumed LDAP groups via Single Sign-On to restrict access to authorized procurement users. For a production environment, we recommend implementing identity propagation for enhanced security.\nCost Considerations:\nWhile exact costs will vary based on usage and specific implementation details, here’s a general overview:\nSAP Integration Suite: Free 90-day trial, then its subscription based. Link to the SAP pricing. AWS Services: Estimated $900 per month for processing 30,000 purchase orders It’s crucial to note that these costs are illustrative and should be carefully evaluated for your specific use case. AWS recommends closely monitoring costs during any proof of concept and scaling accordingly.\nCost Optimization:\nTo maximize value, consider leveraging the SAP Integration Suite across multiple use cases within your organization. This approach can help achieve economies of scale and help justify the investment.\nDemonstration Overview Let’s examine the Sustainability Sourcing Assistant in operation. Through a series of interactive queries using natural language, we will demonstrate how the assistant processes and responds to various related inquiries. The system integrates SAP data, in near real-time pulled via Amazon AppFlow into an Amazon S3 data lake, while enriching responses with third-party sustainability metrics from the Bedrock Knowledge Base through Amazon Bedrock Agents orchestration.\nBy entering the following prompts, starting with “Please List RFQs”, you can see the output.\nPlease list RFQs? Were any RFQs created on 2024-09-03? How many quotations are received for RFQ 7000000026 and from whom and what is the total cost ? Do you have sustainability and other payment term details for these suppliers ? Recommend the best supplier for RFQ 7000000026 based on price and sustainability score. Create a purchase order for the recommended supplier. Create an email to the supplier to negotiate a better price based on other vendors. The assistant can also support various procurement functions, including automated communication tasks. For instance, we can demonstrate its ability to draft supplier negotiation correspondence, leveraging comparative pricing data from multiple vendor quotations.\nAll of this is possible via providing the Amazon Bedrock Agent the context. This is done by providing instructions to the Agent. You write instructions that describe what the agent is designed to do. With advanced prompts, you can further customize instructions for the agent at every step of orchestration and include Lambda functions to parse each step’s output. For example, in our case we provided the following.\nRole: You are Finance Operator using SAP S/4Hana system for querying Request for Quotations, and supplier bidder responses.\nObjective:\n– Return RFQ, and quotation information by querying athena and return data based on the provided user request.\n– If you need to create a purchase order, then use the /createPO function. If PO Creation Failed or Completed with error, return stating PO Creation Failed with error.\n– If you need to award quotation, then use the /awardquotation function.\nQuery Decomposition and Understanding: – Analyze the user’s request to understand the main objective.\n– Break down reqeusts into sub-queries that can each address a part of the user’s request, using the schema provided.\n– Always use previous output to determine the input of subsequent questions.\nSQL Query Creation: – For each sub-query, use the relevant tables and fields from the provided schema.\n– Construct SQL queries that are precise and tailored to retrieve the exact data required by the user’s request.\n– In case of duplicate RFQ or Quotation. Always fetch the record that has maximum value in the column rfglifecyclestatus and qtnlifecyclestatus\nQuery Execution and Response: – Execute the constructed SQL queries against the Amazon Athena database.\n– Return the results ensuring data integrity and accuracy. Do not include the table names or query execution code in the response.\n– Respond to the user as if you are a person and not executing queries from a database.\n– Please use natural names when returning data to the user. For example, instead of “requestforquotation” use “request for quotation number”.\n– Please respond in summary format.\nQuotation selection for an RFQ should also consider the sustainability details and payment term details received from the supplier for the material which is in knowledge base in addition to the total cost of the quotation. When asked for least expensive quotation, always take into consideration the sustainability carbon footprint and payment terms of the supplier in addition to cost. Respond with detailed reasoning on why a quotation is better by comparing other quotations, including the sustainability rationale from knowledge base.\nDisplay the cost in a dollar format, including cents, with the ‘$’ symbol prepended to the value.\nWe aim to provide AWS Solution guidance with sample code in the future for this Sustainability Sourcing Specialist solution.\nConclusion The Sustainability Sourcing Assistant exemplifies how generative AI, powered by Amazon Bedrock, can revolutionize procurement processes by integrating SAP data with external sustainability metrics. This conceptual solution demonstrates the potential for automating quotation analysis, enhancing decision-making with objective criteria, and streamlining purchase order creation—all while maintaining enterprise-grade security and scalability.\nOrganizations looking to modernize their SAP procurement workflows can start by exploring Amazon Bedrock’s capabilities for building custom AI agents. Combined with AWS services like Amazon AppFlow for seamless data integration and SAP BTP Integration Suite for secure connectivity, this approach offers a flexible foundation for innovation across various business functions.\nWe encourage you to experiment with these technologies in your own environment, adapting the architecture to your specific needs. Whether enhancing sustainability initiatives or optimizing other procurement aspects, generative AI holds immense promise for driving efficiency and strategic value in your operations.\n"
},
{
	"uri": "http://localhost:1313/workshop-template/3-blogstranslated/3.6-blog6/",
	"title": "Blog 6",
	"tags": [],
	"description": "",
	"content": "Optimizing Unicode Conversion Downtime for SAP Systems on Oracle to AWS Global organizations using SAP systems need to handle data in multiple languages. Unicode provides a standardized way to represent and process text data from these languages, ensuring consistent display and processing across different systems and platforms. SAP recommends using Unicode for all new installations and converting from non-Unicode systems to ensure compatibility, data integrity, and support for global operations. The latest SAP releases, such as SAP ERP 6.0 EHP8 and SAP S/4HANA, are only available in Unicode, making it a prerequisite for the source system to be Unicode-compatible before upgrading to a higher release.\nTo ensure a smooth transition to SAP S/4HANA, customers need to prioritize the adoption of Unicode in their systems, considering the mainstream maintenance support for ERP 6.0 is scheduled to end on December 31, 2027 as per SAP Note 1648480 – Maintenance for SAP Business Suite 7 Software including SAP NetWeaver (SAP Support login required).\nConverting SAP systems to Unicode involves managing multiple technical challenges: data conversion, code remediation, planned downtime, testing and validation, performance impact, and interface adjustments. Large SAP databases (10 TB+) require 1-3 days of downtime for Unicode conversion. The process needs additional hardware capacity to minimize downtime. Organizations with end-of-life infrastructure must balance hardware investments against downtime requirements.\nIn this blog, we address the challenges faced by a customer regarding system downtime and present a use case where Bell Canada, one of Canada’s largest telecommunications companies, followed an alternative approach to migrate their 11 TB non-Unicode SAP ERP system running with an Oracle database to AWS in under 5 hours.\nBackground Customers running their non-Unicode SAP ERP systems with Oracle databases must first convert their SAP non-Unicode (NUC) systems to Unicode (UC) before migrating to S/4HANA as per SAP Note 2033243. Additionally, when migrating SAP systems running on Oracle databases to AWS, customers must convert to Unicode systems for supportability reasons, as outlined in SAP Note 2358420.\nIn such case, customers undertake a Unicode conversion as an initial step when migrating to AWS. Once the system is operational on AWS, they upgrade from SAP ECC to S/4HANA, taking advantage of AWS’s scalable cloud services.\nFor Unicode conversions, the documented SAP procedure involves exporting the SAP system data from the on-premises source system, transferring the exported data to AWS using services such as AWS Direct Connect or AWS Site-to-Site VPN, and then importing it into the target system running on AWS. The exact duration varies based on factors such as hardware capacity, available network bandwidth, migration tools employed, and specific methods utilized.\nWhen dealing with SAP workloads, which involve multiple third-party integrations, minimizing downtime is necessary to ensure business continuity and prevent disruptions to critical processes. Consequently, it’s essential to adopt strategies and techniques that can minimize the migration downtime.\nUse Case Bell Canada was operating their SAP ERP system with an on-premises Oracle 19c Database with Red Hat Enterprise Linux 8 (RHEL8) as the operating system. The production database size was approximately 11 TB. Bell Canada wanted to retain the Oracle database and upon migrating to AWS, it is required to have an Oracle Enterprise Linux operating system for Oracle support, as per SAP Note 2358420. Additionally, the SAP system needed to be converted to Unicode. The approved maximum business downtime for the migration was 32 hours, with a maximum of 10 hours allowed for technical conversion.\nWe identified a migration approach that eliminates additional on-premises hardware and follows best practices for AWS transitions. The plan prioritizes minimal technical conversion downtime while allocating more time for cutover tasks. We also evaluated different migration options and the best practices such as utilizing advanced database features, parallel processing, or other optimization techniques.\nMigration approach Let’s explore the available approaches available for migrating a non-Unicode SAP system with an Oracle database (running on RHEL) to Oracle Enterprise Linux on AWS.\nApproach Description Steps Option 1 Unicode conversion along with migration to AWS staying on an Oracle database - Migrate the SAP system from the on-premises data center to AWS, converting from non-Unicode to Unicode during the process.\n- Perform remediation and testing of the SAP system on AWS, then decommission the on-premises SAP system. Option 2 Migrate non-Unicode SAP to AWS, then convert to Unicode while staying on an Oracle database - Migrate the SAP system from the on-premises data center to AWS using asynchronous database replication in a temporary system until the cutover.\n- Convert from non-Unicode to Unicode on AWS by exporting from the temporary system and importing into the final system.\n- Perform remediation and testing of the SAP system on AWS, then decommission the on-premises SAP system and the temporary system. Table 1: Unicode conversion options for SAP on Oracle to AWS\nBoth migration options require a business downtime window to transition to the target AWS environment. While Option 1 offers a more predictable execution approach, it requires additional on-premises hardware to support resource-intensive export processes during the SAP Unicode conversion and a stable network bandwidth to facilitate data transfer from on-premises to AWS. Furthermore, this option demands more than 24 hours of technical downtime. To address these challenges, we’ll explore Option 2 and discuss it’s migration approach and best practices.\nIn Option 2, a non-Unicode SAP vanilla system is provisioned on Amazon Elastic Compute Cloud (Amazon EC2) as a temporary instance, and a Unicode SAP installation is set up on another Amazon EC2 as the final instance. The staging instance should be configured with the same version of Oracle database software, including the release and patch set, as the on-premises environment. According to SAP Note 1571295 new non-Unicode SAP system installations are not permitted by default. Therefore, the initial non-Unicode SAP system is created by performing a system copy export from a small existing non-Unicode system. This system copy export is done only once, and the resulting export is reused to build any non-Unicode SAP systems across all environments. The source on-premises SAP system is migrated to the staging instance on AWS using asynchronous database replication. For Oracle database replication, Oracle Data Guard (ODG) is utilized, which is included in Oracle Enterprise Edition licenses which are included in purchases from SAP. This asynchronous replication facilitates the data transfer from on-premises to AWS without impacting ongoing business operations. This method allows terabyte-scale data transfer to AWS during normal business operations. At the start of the actual downtime window, the staging instance takes over and serves as the source system for the Unicode conversion. The SAP system export is performed on the staging instance, and the import is executed on the final SAP instance.\nLet’s examine a simplified architectural setup as shown in Figure 1 to understand the migration process for this approach.\nFigure 1: Simplified architectural setup for Unicode conversion and migration\rIn the first step, Oracle Data Guard is configured for data synchronization between the on-premises database and the staging instance on AWS. Data is transferred from the on-premises environment to the staging instance over AWS Direct Connect. The replication process continues until the start of the cutover process during the downtime window. After taking over the database on the staging instance, it becomes the source system for the SAP system export. In the second step, the SAP system copy export, including any necessary preparation like table splitting, is initiated on the staging database instance. The export dump is stored on an Amazon Elastic Block Store (Amazon EBS) volume attached to the staging instance (referred to as sapdump) and Oracle data files are stored on the EBS volumes (referred to as sapdata). In the third step, the export dump files are continuously transferred to the Amazon EBS volume attached to the final instance using File Transfer Protocol (FTP) without manual intervention. In the fourth step, the import process is started on the final database instance, running in parallel with the export process. By employing a parallel migration approach using SAP tools like Software Provisioning Manager (SWPM) and SAP Migration Monitor, export/import process are parallelized. After completing the export/import, the target SAP system is started, and post-processing steps are performed on the final instance. Subsequently, the staging instance is shut down and eventually terminated.\nThe above simplified diagram in Figure 1 does not include components related to high availability (HA) setup for the sake of simplicity. However, production systems need high availability configurations depending on business requirements. For information on setting up high availability with an Oracle database on AWS for SAP NetWeaver installations, please refer to the blog High availability design and solution for SAP NetWeaver installations with Oracle Data Guard (Fast-Start Failover).\nCustomers should obtain SAP’s concurrence for temporarily running the non-Unicode SAP system during the cutover duration (without any end-users on it) and validate the approach through a proof of concept (POC) at the beginning of the project.\nBest practices Let’s delve into the best practices which helped in minimizing downtime during the technical conversion. To better understand this, we will examine the guidelines and practices that are implemented in each phase of the system migration. Figure 2 shows the high-level phases for Unicode conversion and migration of an SAP system based on Oracle.\nFigure 2: Phases in an SAP system Unicode conversion and migration based on Oracle\rPlanning\nDuring the planning phase, we recommend considering strategies to reduce the size of large technical tables. Cross-check if standard reorganization jobs are scheduled to prevent the technical tables from growing indefinitely. For instance, following the guidelines in SAP Note 1616768 for the BALDAT and BALHDR tables will be beneficial if these log tables have become excessively large.\nThe Unicode conversion process requires specific preparatory checks, including long-running reports like SDBI_CLUSTER_CHECKS and UCCHECK. We advise running these reports months in advance before the actual conversion. This proactive approach helps identify and mitigate potential issues related to cluster tables and provides insights into the development efforts required for Unicode-related adjustments. These checks are part of the Unicode conversion guide as referred in SAP Note 551344.\nWhen transitioning to Unicode and preparing for the target infrastructure, you must assess the Unicode-related hardware requirements specified in SAP Note 1139642. Migrating terabytes of data will need a reliable network connection with required bandwidth between the on-premises source and AWS, which is achieved by AWS Direct Connect or AWS Site-to-Site VPN.\nOpen the required ports for SAP services and the Oracle database in the associated firewall and security group to maintain consistent network access throughout the migration process. For SAP services, such as the SAP Application Server, SAP Central Services, and SAP Software Provisioning Manager (SWPM), refer to the SAP documentation “ TCP/IP Ports of All SAP Products“. For Oracle database and Oracle Data Guard replication, open the Oracle Net Services Listener port configured for Oracle client connections to the database, LOCAL_LISTENER port configured for application pluggable databases (PDB) in a multi-tenant architecture container database (CDB), as well as ports for Oracle Data Guard listener, if not using the standard Oracle Net listener ports. All these ports are manually configurable, and you should obtain the actual port numbers in use from the existing setup.\nIf the source database is encrypted, create the required Oracle Wallet on the target systems and copy all necessary SSL certificates from the Oracle Wallet of the source system. Additionally, the listener services need to be configured to listen on the TCPS protocol instead of the standard TCP protocol. These protocol changes must be updated in the firewall configurations made earlier.\nDuring the migration process, it is essential to monitor the total CPU and IOPS (Input/Output Operations Per Second) usage. Depending on the anticipated usage of CPU and IOPS during the migration, request an increase in the AWS service limits. When upsizing the instance type to accommodate more CPU during migration, ensure that your AWS account’s EC2 instance limit quota for the target region allows for the desired instance type and CPU capacity. For example, migration requirements of 250,000 IOPS exceed the default service quotas of 100,000 for “IOPS for Provisioned IOPS SSD (io2) volume” and “IOPS modifications for Provisioned IOPS SSD (io2) volumes”. In such cases, you should proactively request an increase in these quotas to the required value before initiating the migration. Review quota and limit details in the Amazon EC2 quotas and Amazon EBS limits documentation.\nWhile migrating to Unicode, SAP system parameters for the target Unicode system will need to be adapted as outlined in SAP Note 790099. Depending on the support contract with SAP, you should consider using the “Going Live Functional Upgrade service” provided by SAP, which offers baseline recommendations for the Unicode system. SAP Note 2360708 provides more information about how to request support service from SAP. Consider implementing the parameter changes early in the non-production environment before commencing the user acceptance testing. This proactive approach allows the business and technical teams to assess the impact of these changes and plan better for similar parameter adjustments in the production environment.\nSystem build\nDuring the build phase, infrastructure is provisioned for both non-Unicode and Unicode SAP systems. The non-Unicode system serves as a temporary staging instance, utilized solely during the migration process, while the Unicode system serves as the final instance. Once the infrastructure is ready, SAP vanilla builds are performed for both the non-Unicode and Unicode systems. Building these vanilla systems allows for the execution of preparatory technical checks, such as high availability testing, in advance. As always, make sure to leverage the flexible benefits of the cloud to shut down your EC2 instances when they are not in use, which will help lower your hardware costs.\nWhen designing the database disk layout, plan the number of Amazon EBS volumes, the Amazon EBS volume types such as General Purpose SSD (gp3) or Provisioned IOPS SSD (io2), and the required striping with Logical Volume Manager (LVM) to optimize throughput performance.\nLastly, check your EC2 limitations. As stated in Amazon EBS-optimized instance types user guide, “An instance’s EBS performance is bounded by the instance type’s performance limits, or the aggregated performance of its attached volumes, whichever is smaller”. If your workload requires high and consistent IOPS performance, low latency, high durability, and you’re willing to pay a premium, io2 volumes are the better choice. However, if your workload has moderate IOPS requirements and cost is a concern, gp3 volumes are more cost-effective option while still providing good performance and durability. For more detailed information on the performance characteristics and use cases of gp3 and io2 volumes, you can refer to the AWS documentation on Amazon EBS volume types.\nProvisioned IOPS SSD (io2) volumes offer the highest durability and lowest latency among Amazon EBS volume types, making them suitable for mission-critical applications with demanding performance requirements. For more detailed information on Provisioned IOPS SSD (io2) volumes, including performance characteristics, provisioning, and best practices, please refer to the AWS documentation on Provisioned IOPS SSD (io2) Block Express volumes.\nAn example disk layout is provided for an Oracle database instance in Table 2, consisting of two io2 Amazon EBS volumes of 6,300 GB and 110,000 IOPS each. With striping, this configuration offers a throughput ranging from 3437.5 MB/s to 8,000 MB/s.\nEBS Volumes Volume Group Volume Type Description File system EBS Size (GB) IOPS EBS Throughput (at 16 KiB I/O size) EBS Throughput (at 256 KiB I/O size) Volume 1 VG-DB io2 Database data files /oracle//sapdata\u0026lt;1..n\u0026gt; 6,300 110,000 1,718.75 MB/s 4,000 MB/s Volume 2 VG-DB io2 Database data files 6,300 110,000 1,718.75 MB/s 4,000 MB/s Total 12,600 220,000 3,437.5 MB/s 8,000 MB/s Table 2: Sample disk layout for Oracle database instance\nODG setup / Replication / Monitoring\nOracle’s native solution, Oracle Data Guard (ODG), are employed to create a transactionally consistent copy of the on-premises database on a staging database server in AWS. Initially, Oracle Recovery Manager (RMAN) is utilized to create a clone of the on-premises database on the staging server. Subsequently, the Data Guard Broker is configured for change data capture (CDC), ensuring that all data changes made to the on-premises database are captured and applied to the target staging database in real-time.\nParallel channels are employed to maximize the copy throughput. Since RMAN utilizes a dedicated large pool for backup and restore operations, the target staging database needs to be configured with LARGE_POOL_SIZE and SGA_MAX_SIZE parameters. IO server processes need to be added to the database writer (DBWR) to optimize the write process during the restore operation by leveraging the async IO capabilities of the operating system.\nTo further enhance the conversion process, just before the cutover, scale up the staging and target database Amazon EC2 instances to meet conversion requirements. Additionally, adjustments are required for Oracle database parameters, for example MEMORY_TARGET and MEMORY_MAX_TARGET in databases configured with automatic memory management, to fully utilize the additional resources. During the conversion, both the staging and target databases are set to NOARCHIVELOG mode to minimize the overhead associated with log archiving.\nUnicode uptime / Pre-Migration steps\nThe Unicode conversion is carried out as part of the SAP system copy procedure. During this process, the system is exported from the source in Unicode format and then imported into the target SAP system. The SAP System Copy Guide, and the “Unicode conversion guide” as referred in SAP Note 551344), provides detailed instructions for this process.\nBefore the Unicode export and import can take place, the system must undergo a series of preparatory checks, both during uptime and downtime periods. These checks are outlined in the Unicode conversion guide. The uptime checks and reports are executed on the on-premises system, while reports and checks that require system downtime are performed on the staging instance on AWS, after the system takeover.\nRamp down\nDuring the scheduled business downtime, the systems are gracefully locked out for end-users. Once the communication interfaces are stopped, the systems are prepared for the migration process. In this phase, preparatory tasks are performed as outlined in the SAP system copy guide, including suspending batch jobs, disabling alerts, stopping background processing, addressing cancelled or pending updates, and clearing invalid temporary tables etc. These steps ensure a smooth migration by bringing the systems to a consistent state before the actual migration activities commence.\nTakeover\nAfter confirming that the primary and standby systems are synchronized, the staging system (on AWS) is taken over. This takeover is validated using the Oracle Data Guard (ODG) switchover procedure. Upon successful takeover, the ODG configuration is removed to prevent any reverse communication from the target system to the on-premises environment. Subsequently, the instance type and database-related parameters are adjusted before initiating the system copy procedure.\nSystem copy (Export / Import / Monitor)\nTo optimize the system copy procedure, start with identifying the top tables from the system DB02 list and plan for splitting large tables (e.g. tables more than 20 GB in size). Techniques such as Package Splitting and Table Splitting are used to enable parallel processing of table exports and imports. Running the SAP migration time analyzer can provide a consolidated view of package exports and imports, allowing for the identification of further optimization opportunities. This optimization exercise may require multiple test runs to identify the right sequence.\nFor large tables, splitting may be necessary to process exports and imports in parallel. While you have the option to use either SAPup/R3ta (as per SAP system copy guide section 2.6.4) or a supported database-specific splitter, in the case of an Oracle-based system migration, consider using the Oracle splitter for efficient table splitting as per SAP Note 1043380.\nOracle parameter settings and configurations (e.g., db_cache_size, pga_aggregate_target, processes, PSAPTEMP size, and disabling archive logging) for R3load-based system copy should be set according to the recommendations from SAP Note 936441.\nDuring the migration export/import process, an EC2 instance with a higher number of CPUs, RAM, network, and storage throughput capacity should be chosen. Additionally, it is recommended to temporarily use io2 EBS Volumes with boosted provisioned IOPS values to reduce import time, which are converted back to gp3 later, during online operations. The selected instance size should have at least the number of CPUs required to run multiple R3load processes concurrently and also supports sufficient EBS throughput and IOPS to ensure your high throughput volumes are not throttled. As a rule of thumb, use three times the number of CPUs. For example, if you plan to run 300 R3load processes, choose an Amazon EC2 instance with minimum 100 vCPUs.\nResize the instance and adapt the parameters in each subsequent proof of concept (POC) run based on the findings. Plan for multiple iterations of POCs in a test or sandbox system with a similar database size as production. After several POC runs, you should have the optimal configuration of CPU, RAM, network bandwidth, EBS throughput, and IOPS required for completing the Unicode conversion within the desired technical downtime. Before the export and import activity, the instance type are changed to achieve optimal performance with higher CPU, memory, network performance, and maximum bandwidth. For example, the EC2 r7i.48xlarge, powered by 4th Generation Intel Xeon Scalable processors, provides 192 CPUs, 1536 GB of memory, 50 Gbps Network Bandwidth, 5000 MB/s Max EBS Throughput, and 240000 Max IOPS.\nMonitoring the migration execution is necessary, and you can leverage Amazon CloudWatch to create a dashboard for monitoring during the cutover. This will help identify any bottlenecks or indicate opportunities to further fine-tune the Unicode conversion downtime.\nUsing Amazon CloudWatch dashboards, you can monitor relevant EBS metrics for throughput (VolumeReadBytes, VolumeWriteBytes, or VolumeThroughputPercentage), and IOPS (VolumeReadOps, VolumeWriteOps, or VolumeConsumedReadWriteOps) for important volumes such as the sapdata volume (used for the Oracle sapdata file) and the sapdump volume (used for storing export/import data from the SAP system). Additionally, you can monitor CPU usage along with other metrics like throughput and IOPS. If the limits are not reached and the SAP migration time analyzer shows some tasks taking longer, you should increase the number of R3load processes and allow those slower tasks to be processed first during the export. You can control the order of export/import by orderBy properties as described in section 2.6.7 of the SAP system copy guide.\nIt is recommended to monitor the volumes that hold the Oracle online redo logs, which might be used for logging mechanisms during export/import. This applies to both the staging database instance and the final database instance. By closely monitoring these metrics, you can identify potential bottlenecks and make informed decisions to optimize the Unicode conversion process, ensuring a smooth and efficient migration within the desired technical downtime.\nFigure 3 illustrates an example of an Amazon CloudWatch Dashboard displaying important metrics.\nFigure 3: Amazon CloudWatch Dashboard with important metrics\nPost-Migration steps\nIn this phase, all the post-Unicode conversion, post-migration activities as outlined in the SAP Unicode conversion guide as referred in SAP Note 551344 and SAP system copy guide are executed. Once these activities are completed, any optimizations made specifically for the migration (e.g., instance size, IOPS, throughput, SAP/database parameters) are reverted to their original settings.\nBefore handing over the system for user validation, ensure that a system backup is in place and fully operational. After the migration is completed and validated by the business, the staging system is shut down (and later terminated) to avoid incurring unnecessary costs.\nFollowing the prescribed steps precisely ensures successful post-migration testing and production deployment.\nConclusion In this blog post, you learned how Bell Canada leveraged the scalable infrastructure of AWS to complete the technical Unicode conversion of their 11 TB ERP Production system in less than 5 hours and successfully migrated to AWS. This approach resulted in a 75% reduction in technical downtime compared to a single-step Unicode conversion and migration process, which takes more than 24 hours. Additionally, it eliminated the need for additional on-premises hardware resources.\nBy adopting this approach, Bell Canada was able to optimize the Unicode conversion and migration process, minimizing business disruption and maximizing operational efficiency. The scalability and flexibility of AWS infrastructure played a crucial role in achieving this remarkable feat, enabling Bell Canada to complete the complex migration process within a significantly shorter timeframe.\nIn this blog, we used Oracle Data Guard to set up the staging instance. However, there are restrictions when changing endianness (Big Endian and Little Endian), such as from AIX (Big Endian) to X86_64 (Little Endian), as per Oracle’s documentation [Data Guard Support for Heterogeneous Primary and Physical Standbys in Same Data Guard Configuration](https://support.oracle.com/knowledge/Oracle Cloud/413484_1.html) (requires an Oracle support login). SAP Note 552464 guides on determining the exact endianness for different architectures and operating systems. In scenarios where customers need to migrate SAP non-Unicode system on Oracle database from a big-endian to a little-endian platform, Oracle Cross-Platform Transportable Tablespaces (XTTS) is utilized to migrate the non-Unicode system to AWS following the blog Reducing downtime with Oracle XTTS method for cross-platform SAP migrations. Once the non-Unicode system is available on AWS, you can perform the export and import processes on AWS following the steps outlined in this blog.\nJoin the SAP on AWS Discussion In addition to your customer account team and AWS Support channels, AWS provides public Question \u0026amp; Answer forums on our re:Post Site. Our SAP on AWS Solution Architecture team regularly monitor the SAP on AWS topic for discussion and questions that could be answered to assist you. If your question is not support-related, consider joining the discussion over at re:Post and adding to the community knowledge base.\nCredits We would like to thank the following team members for their contributions: Derek Ewell and Spencer Martenson.\n"
},
{
	"uri": "http://localhost:1313/workshop-template/3-blogstranslated/3.7-blog7/",
	"title": "Blog 7",
	"tags": [],
	"description": "",
	"content": "GNOME has a new infrastructure partner: welcome AWS! This post was contributed by Andrea Veri from the GNOME Foundation. It has been cross-posted from gnome.org with permission.\nGNOME has historically hosted its infrastructure on premises. That changed with an AWS Open Source Credits program sponsorship which has allowed our team of two SREs to migrate the majority of the workloads to the cloud and turn the existing Openshift environment into a fully scalable and fault tolerant one thanks to the infrastructure provided by AWS. By moving to the cloud, we have dramatically reduced the maintenance burden, achieved lower latency for our users and contributors and increased security through better access controls.\nOur original infrastructure did not account for the exponential growth that GNOME has seen in its contributors and userbase over the past 4-5 years thanks to the introduction of GNOME Circle. GNOME Circle is composed of applications that are not part of core GNOME but are meant to extend the ecosystem without being bound to the stricter core policies and release schedules. Contributions on these projects also make contributors eligible for GNOME Foundation membership and potentially allow them to receive direct commit access to GitLab in case the contributions are consistent over a long period of time in order to gain more trust from the community. GNOME recently migrated to GitLab, away from cgit and Bugzilla.\nIn this post, we’d like to share some of the improvements we’ve made as a result of our migration to the cloud.\nA history of network and storage challenges In 2020, we documented our main architectural challenges:\nOur infrastructure was built on OpenShift in a hyperconverged setup, using OpenShift Data Foundations (ODF), running Ceph and Rook behind the scenes. Our control plane and workloads were also running on top of the same nodes. Because GNOME historically did not have an L3 network and generally had no plans to upgrade the underlying network equipment and/or invest time in refactoring it, we would have to run our gateway using a plain Linux VM with all the associated consequences. We also wanted to make use of an external Ceph cluster with slower storage, but this was not supported in ODF and required extra glue to make it work. No changes were planned on the networking equipment side to make links redundant. That meant a code upgrade on switches would have required full service downtime. We had to work with with Dell support for every broken hardware component, which added further toil. With the GNOME user and contributor base always increasing, we never really had a good way to scale our compute resources due to budget constraints. Cloud migration improvements In 2024, during a hardware refresh cycle, we started evaluating the idea of migrating to the public cloud. We have been participating in the AWS Open Source Credits program for many years and received sponsorship for a set of Amazon Simple Storage Service (Amazon S3) buckets that we use widely across GNOME services. Based on our previous experience with the program and the people running it, we decided to request sponsorship from AWS for the entire infrastructure which was kindly accepted.\nI believe it’s crucial to understand how AWS resolved the architectural challenges we had as a small SRE team (just two engineers!). Most importantly, the move dramatically reduced the maintenance toil we had:\nUsing AWS’s provided software-defined networking services, we no longer have to rely on an external team to apply changes to the underlying networking layout. This also gave us a way to use a redundant gateway and NAT without having to expose worker nodes to the internet. We now use AWS Elastic Load Balancing (ELB) instances (classic load balancers are the only type supported by OpenShift for now) as a traffic ingress for our OpenShift cluster. This reduces latency as we now operate within the same VPC instead of relying on an external load balancing provider. This also comes with the ability to have access to the security group APIs which we can use to dynamically add IPs. This is critical when we have individuals or organizations abusing specific GNOME services with thousands of queries per minute. We also use Amazon Elastic Block Store (Amazon EBS) and Amazon Elastic File System (Amazon EFS) via the OpenShift CSI driver. This allows us to avoid having to manage a Ceph cluster, which is a major win in terms of maintenance and operability. With AWS Graviton instances, we now have access to ARM64 machines, which we heavily leverage as they’re generally cheaper than their Intel counterparts. Given how extensively we use Amazon S3 across the infrastructure, we were able to reduce latency and costs due to the use of internal VPC S3 endpoints. We took advantage of AWS Identity and Access Management (IAM) to provide granular access to AWS services, giving us the possibility to allow individual contributors to manage a limited set of resources without requiring higher privileges. We now have complete hardware management abstraction, which is vital for a team of only two engineers who are trying to avoid any additional maintenance burden. Thank you, AWS! I’d like to thank AWS for their sponsorship and the massive opportunity they are giving to the GNOME Infrastructure to provide resilient, stable and highly available workloads to GNOME’s users and contributors across the globe.\n"
},
{
	"uri": "http://localhost:1313/workshop-template/3-blogstranslated/3.8-blog8/",
	"title": "Blog 8",
	"tags": [],
	"description": "",
	"content": "Leveraging LLMs as an Augmentation to Traditional Hyperparameter Tuning When seeking to improve machine learning model performance, hyperparameter tuning is often the go-to recommendation. However, this approach faces significant limitations, particularly for complex models requiring extensive training times. In this post, we’ll explore a novel approach that combines gradient norm analysis with Large Language Model (LLM) guidance to intelligently redesign neural network architectures. This method can identify and resolve performance bottlenecks without the computational burden of traditional hyperparameter optimization. Through practical examples, we’ll show how our technique leverages LLMs as architecture advisors, providing targeted recommendations that directly address identified weaknesses in network design.\nBackground Traditional hyperparameter tuning typically uses Bayesian Optimization, which builds a mathematical model of parameter-performance relationships to efficiently find optimal solutions with fewer evaluations than alternatives like genetic algorithms.\nHowever, significant practical limitations exist. Without high performance computing, serial execution becomes prohibitively time-consuming—while lightweight models might complete optimization within 24 hours, complex tasks like reinforcement learning or large computer vision datasets often make traditional approaches impractical on local hardware. Cloud solutions like AWS Batch, AWS Parallel Computing Service, or AWS ParallelCluster enable massive parallelization that can deliver substantial ROI for complex models despite additional costs.\nTraditional tuning also requires explicit parameterization of everything you wish to optimize. While simple parameters like learning rate are straightforward to incorporate, evolving entire architectural structures presents significant implementation challenges. Approaches like Neuroevolution of Augmented Topologies (NEAT) can address architectural optimization but often sacrifice sample efficiency.\nThis is where Large Language Models (LLMs) offer a compelling alternative. LLMs function as surrogate universal experts, synthesizing knowledge across the entire spectrum of neural network development—effectively interpolating decades of human knowledge across specialized domains like computer vision, NLP, and reinforcement learning. They provide architecture recommendations based on collective wisdom embedded in technical literature rather than computationally intensive empirical searches.\nOur approach focuses specifically on using LLMs for neural architecture design and modification based on gradient norm analysis, complementing frameworks like Liu et al. (2022) AgentHPO that provide more general hyperparameter optimization across diverse ML tasks.\nAn agentic workflow for neural network design In our exploration of LLM-based code modifications, we implemented an agentic workflow to manage iterative reasoning and execution processes. Figure 1 illustrates the complete architecture of our system, which leverages LangGraph for workflow orchestration and decision routing between agents. Our implementation runs on EC2 g6.24xlarge instances equipped with 4x NVIDIA L4 Tensor Core GPUs to handle neural network training. LLM interactions occur through Python API calls to Amazon Bedrock using Boto3. We utilize different Claude models for specialized tasks: Claude 3.7 Sonnet for testing agents, Claude 3.7 Sonnet with reasoning activated for code writing, and a smaller/faster model such as Claude 3.0 Haiku for report generation and code validation.\nFigure 1: Multi-agent workflow for in-situ neural network design. LLM icon uses Amazon Bedrock API.\nOur workflow begins (step 1) with standard mini-batch training while implementing robust error handling—runtime errors are caught and forwarded to an inspector agent for code analysis and correction.\nAt regular intervals (every 50-100 epochs), we extract (step 2) key training insights including gradient norms (discussed in next section) and standard metrics like loss and accuracy. This state representation is formatted into a prompt (step 3) for an LLM that assesses health: training stability and issue identification.\nLangGraph is a framework for creating stateful, multi-step workflows with language models. A LangGraph-based decision router (step 4) analyzes the health assessment and directs the workflow toward one of three paths: hyperparameter optimization, neural network structural modifications, or training completion with report generation. For hyperparameter adjustments, the LLM provides recommendations as structured JSON that can be directly applied to training.\nFor architectural modifications, we employ two collaborating LLMs: a primary LLM generates new Python modules defining the modified network architecture, while a secondary tester/fixer LLM evaluates the generated code by attempting execution. Through an iterative process between the code writer and code tester/fixer, errors are caught, analyzed, and corrected before saving the finalized code. Once successfully validated with dummy inputs, training restarts (step 5) with the improved architecture.\nNeural network design through the lens of gradient norms While traditional metrics like loss and accuracy provide fundamental insights, gradient norm analysis reveals deeper understanding of architecture efficacy. Gradient norms—the magnitude of weight update signals during training—serve as powerful diagnostics that reveal whether a network is excessively deep, insufficiently wide, poorly normalized, or using suboptimal activation functions. Unusually large or small gradient norms often signal specific architectural problems that LLMs can interpret based on patterns observed across decades of neural network research.\nTo demonstrate this capability, we’ll experiment with a deliberately flawed computer vision classification model (shown below) trained on CIFAR, incorporating intentional design weaknesses with an excessively high learning rate (0.1) and basic SGD optimization without momentum or adaptive components.\nBy examining how the LLM interprets these gradient norm patterns across training epochs, we’ll observe its ability to diagnose specific architectural deficiencies and recommend appropriate modifications—effectively leveraging the same gradient-based signals that experienced practitioners use to debug network designs.\nimport torch import torch.nn as nn class CNN(nn.Module): \u0026#34;\u0026#34;\u0026#34; An extremely simple CNN with a single convolutional layer followed by a fully connected layer for classification. Uses sigmoid activation instead of ReLU. \u0026#34;\u0026#34;\u0026#34; def __init__(self): super(CNN, self).__init__() # Single convolutional layer self.conv1 = nn.Conv2d(3, 16, kernel_size=15, padding=1) # With no pooling, feature size remains 32x32, so: 32x32x16 = 16384 self.fc = nn.Linear(16384, 10) def forward(self, x): # Single convolutional layer with sigmoid activation x = torch.sigmoid(self.conv1(x)) # No pooling, so dimensions stay the same # Flatten for fully connected layer x = torch.flatten(x, 1) # Classification layer x = self.fc(x) return x After allowing 50-100 epochs of training to progress, we gave the optimizer sufficient time to locate a suitable optimization valley beyond the initial chaos of weight initialization. At this checkpoint, we presented the training history and gradient norms to Claude 3.7 via Amazon Bedrock, requesting an assessment of model performance and stability (see line 219 here). The LLM responded with a recommendation to alter the neural network architecture, providing the following analysis:\n\u0026#34;Explanation: While the model shows some minimal improvement (validation accuracy from 17.6% to 19.4% over 50 epochs), progress is extremely slow and erratic. The loss isn\u0026#39;t consistently decreasing, and the model appears to be struggling with fundamental architectural issues. The high learning rate (0.1) combined with SGD optimizer is likely causing training instability given the complex architecture. The lack of batch normalization is also problematic.\u0026#34; This assessment serves as a critical routing point within our LangGraph workflow. The LLM determined that a complete architectural overhaul was necessary, generating new PyTorch code that introduced significantly greater complexity and modified the loss function. The next code block shows this entirely revised neural network model, which was validated and error-checked by our secondary LLM before implementation.\nclass ConvBlock(nn.Module): \u0026#34;\u0026#34;\u0026#34; A convolutional block with batch normalization and residual connection to improve gradient flow and training stability \u0026#34;\u0026#34;\u0026#34; def __init__(self, in_channels, out_channels, kernel_size=3, stride=1): super(ConvBlock, self).__init__() self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=kernel_size//2, bias=False) self.bn = nn.BatchNorm2d(out_channels) # FIX: More explicit handling of skip connection to ensure dimension matching if in_channels == out_channels and stride == 1: # Identity shortcut when dimensions match self.residual = nn.Identity() else: # 1x1 conv to match both channel dimensions and spatial dimensions (if stride \u0026gt; 1) self.residual = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False) def forward(self, x): identity = x # Main branch out = self.conv(x) out = self.bn(out) out = F.relu(out) # Add residual connection out = out + self.residual(identity) # FIX: Added ReLU after addition (standard in ResNet blocks) out = F.relu(out) return out class CNN(nn.Module): \u0026#34;\u0026#34;\u0026#34; Improved CNN architecture that addresses gradient flow issues: 1. Replaces large kernel sizes (15x15) with smaller, more efficient 3x3 kernels 2. Adds batch normalization to stabilize gradients 3. Uses multiple smaller convolutional layers with residual connections 4. Balances FC layers to prevent bottlenecks 5. Implements proper pooling strategy to reduce parameter count 6. Uses ReLU activations instead of sigmoid for better gradient flow 7. Adds dropout for regularization \u0026#34;\u0026#34;\u0026#34; def __init__(self, num_classes=10): super(CNN, self).__init__() # Initial convolution with moderate number of filters self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1, bias=False) self.bn1 = nn.BatchNorm2d(32) # Convolutional blocks with gradually increasing filters # Input: 16x16x32, Output: 16x16x64 self.block1 = ConvBlock(32, 64) # Input: 8x8x64, Output: 8x8x128 self.block2 = ConvBlock(64, 128) # Input: 4x4x128, Output: 4x4x256 self.block3 = ConvBlock(128, 256) # Pooling layers to reduce spatial dimensions self.pool = nn.MaxPool2d(2, 2) # More balanced FC layer architecture # After 3 pooling operations (32 -\u0026gt; 16 -\u0026gt; 8 -\u0026gt; 4), feature map is 4x4x256 = 4096 self.fc1 = nn.Linear(4096, 512) self.fc2 = nn.Linear(512, 128) self.fc3 = nn.Linear(128, num_classes) # Dropout for regularization self.dropout = nn.Dropout(0.3) # Reduced from 0.5 to prevent too much regularization # Proper weight initialization for better gradient flow self._initialize_weights() def _initialize_weights(self): \u0026#34;\u0026#34;\u0026#34;Initialize weights using Kaiming initialization for ReLU activations\u0026#34;\u0026#34;\u0026#34; for m in self.modules(): if isinstance(m, nn.Conv2d): nn.init.kaiming_normal_(m.weight, mode=\u0026#39;fan_out\u0026#39;, nonlinearity=\u0026#39;relu\u0026#39;) elif isinstance(m, nn.BatchNorm2d): nn.init.constant_(m.weight, 1) nn.init.constant_(m.bias, 0) elif isinstance(m, nn.Linear): nn.init.kaiming_normal_(m.weight, mode=\u0026#39;fan_out\u0026#39;, nonlinearity=\u0026#39;relu\u0026#39;) if m.bias is not None: nn.init.constant_(m.bias, 0) def forward(self, x): \u0026#34;\u0026#34;\u0026#34; Forward pass with explicit shape tracking comments Input shape: [batch_size, 3, 32, 32] \u0026#34;\u0026#34;\u0026#34; # Initial convolution: [batch_size, 3, 32, 32] -\u0026gt; [batch_size, 32, 32, 32] x = self.conv1(x) x = self.bn1(x) x = F.relu(x) # First pooling: [batch_size, 32, 32, 32] -\u0026gt; [batch_size, 32, 16, 16] x = self.pool(x) # Block 1: [batch_size, 32, 16, 16] -\u0026gt; [batch_size, 64, 16, 16] x = self.block1(x) # Second pooling: [batch_size, 64, 16, 16] -\u0026gt; [batch_size, 64, 8, 8] x = self.pool(x) # Block 2: [batch_size, 64, 8, 8] -\u0026gt; [batch_size, 128, 8, 8] x = self.block2(x) # Third pooling: [batch_size, 128, 8, 8] -\u0026gt; [batch_size, 128, 4, 4] x = self.pool(x) # Block 3: [batch_size, 128, 4, 4] -\u0026gt; [batch_size, 256, 4, 4] x = self.block3(x) # Flatten: [batch_size, 256, 4, 4] -\u0026gt; [batch_size, 4096] x = torch.flatten(x, 1) # FC layers with dropout x = F.relu(self.fc1(x)) # [batch_size, 4096] -\u0026gt; [batch_size, 512] x = self.dropout(x) x = F.relu(self.fc2(x)) # [batch_size, 512] -\u0026gt; [batch_size, 128] x = self.dropout(x) # Output: [batch_size, 128] -\u0026gt; [batch_size, num_classes] x = self.fc3(x) return x def loss_function(self, outputs, targets): \u0026#34;\u0026#34;\u0026#34; Custom loss function that helps with gradient scaling. Uses label smoothing to prevent overconfidence and improve gradient flow. \u0026#34;\u0026#34;\u0026#34; return nn.CrossEntropyLoss(label_smoothing=0.1)(outputs, targets) Interestingly, when the agentic workflow (an autonomous, self-directed process where the AI system makes independent decisions without human intervention) initiates the training process with its revised model, we observe the loss immediately explodes, halting progress. The LLM detects this instability and autonomously implements critical modifications to the optimization strategy, including adjusting the learning rate and transitioning to the Adam optimizer. From this point forward, the LLM continuously (every 50 – 100 epochs) monitors the training dynamics, making iterative adjustments to either the optimization parameters or neural network architecture until it determines training has stabilized sufficiently for the LangGraph implementation to complete the process without further LLM intervention.\nFigure 2 presents a comprehensive analysis of the base model’s performance across 600 training epochs. The top-left panel shows the training loss dropping sharply in the first 25 epochs from approximately 10² to 10¹, after which it flattens completely around 10⁰, indicating the model has stopped meaningful learning despite continued training. This premature convergence is further evidenced in the top-right panel, where training accuracy (blue) oscillates wildly between 7-14% while validation accuracy (orange) remains fixed at approximately 10% throughout the entire training process, confirming no actual learning is occurring.\nThe bottom panels provide insight into the gradient behavior. The total gradient norm (bottom-left) initially spikes above 10² before rapidly decreasing and stabilizing around 10⁻¹ for the remainder of training. The layer-specific gradient norms (bottom-right) similarly stabilize after the initial epochs, with different layers’ gradients settling at various magnitudes between 10⁻³ and 10⁻¹. While these gradient patterns appear numerically stable, they fail to drive meaningful parameter updates that would improve model performance, suggesting optimization issues beyond simple gradient instability. Figure 2: Results when using base CNN with no LLM modifications.\nIn contrast, Figure 3 showcases the remarkably superior performance of the LLM-designed neural network with its carefully selected optimization hyperparameters. After around 4 LLM design iterations, this model achieves a validation accuracy of approximately 83%, representing a substantial improvement over the baseline. While this is significant progress, it’s worth noting that state-of-the-art models for CIFAR classification currently achieve accuracies above 99% (EfficientNet-L2), demonstrating room for further optimization. The gap between our results and cutting-edge manual human tuned likely stems from advanced techniques that our LLM may not have incorporated or been aware of. For instance, optimization approaches like Sharpness Aware Minimization (SAM) have pushed EfficientNet-L2 to record-breaking accuracies by improving generalization through flat minima optimization. Such specialized techniques might lie outside the LLM’s training data or weren’t considered within our experimental constraints. This suggests our approach could benefit significantly from the addition of a RAG system that incorporates recent academic advancements, allowing the LLM to leverage cutting-edge research that may have emerged after its knowledge cutoff. Nevertheless, the performance differential underscores the efficacy of allowing an LLM to dynamically adjust neural network design and training parameters in response to observed training dynamics.\nFigure 3: Results when using CNN design by LLM and optimization parameters chosen by the LLM.\nConclusion LLMs offer a powerful alternative to traditional hyperparameter tuning, leveraging synthesized knowledge to improve neural networks without exhaustive parameter searches. Our experimental LLM-designed architecture achieved 83% validation accuracy on CIFAR, substantially outperforming the baseline model. This approach proves especially valuable for practitioners with limited computational resources or those working with models requiring lengthy training cycles.\nA complementary strategy could combine LLMs for major architectural decisions with focused traditional hyperparameter tuning for final refinement, potentially offering the best of both worlds. The complete source code for this project, including our multi-agent LangGraph workflow implementation and all neural network models, is available in our AWS Samples repository on GitHub. We encourage researchers and practitioners to explore and build upon these findings.\n"
},
{
	"uri": "http://localhost:1313/workshop-template/3-blogstranslated/3.9-blog9/",
	"title": "Blog 9",
	"tags": [],
	"description": "",
	"content": "Your unified view of travelers and guests from Amazon Connect Customer Profiles Historically, travel and hospitality companies relied on loyalty programs to do exactly that—build loyalty. However, a McKinsey \u0026amp; Company report shows that loyalty is in decline. The same study from McKinsey advises that travel and hospitality companies can win loyalty by “offering distinctive, satisfying experiences” through more personalized engagement.\nThe key to delighting customers and providing the seamless experience they crave starts with accurate customer data. Today, we’re excited to announce Amazon Connect Customer Profiles for travel and hospitality, a new industry-specific capability within Amazon Connect that helps travel and hospitality companies deliver more personalized experiences across every customer touchpoint.\nIn this blog, we’ll explore how travel and hospitality companies can now use Amazon Connect Customer Profiles to create unified traveler and guest profiles, improve data collaboration for marketing campaigns, partnership, and measurement, and use generative AI to offer more personalized engagements and service.\nUnifying customer data is challenging While every organization aspires to a single view of the customer, those in the travel and hospitality industry know how challenging this can be. Travel and hospitality companies sit on a trove of first-party data. However, that customer data is siloed. It can live in booking tools, property management systems, loyalty systems, contact centers, and more. It can be in the cloud or on premises. It can be structured or unstructured. It can be used to enhance the traveler and guest journey at every step but only after it’s merged into a unified view of the customer. This fragmentation makes it difficult to deliver consistent, personalized experiences across channels and touchpoints.\nAmazon Connect Customer Profiles now addresses these challenges by providing a purpose-built mapping for common travel and hospitality applications. It comes with pre-configured data models specifically designed for travel and hospitality use cases, ensuring that companies can quickly implement a unified view of the customer that understands their unique needs. The service combines customer data from over 75 sources, including leading Customer Data Platforms, (CDPs) engagement platforms, and specialized travel and hospitality applications, through automated data ingestion and unification processes.\nEnriching customer data Real-time profile resolution uses rules-based deduplication to ensure accurate, unified customer profiles. This enables companies to have a comprehensive view of each customer at any given moment. Amazon Connect Customer Profiles are always up-to-date with real-time connections to data sources, ensuring you always know the latest about your customer.\nAmazon Connect Customer Profiles is built to handle enterprise-grade scale, supporting terabytes of data with sub-50ms performance to meet the needs of the world’s largest organizations. Additionally, it’s available in 10 regions globally with built-in compliance for standards such as GDPR, PCI, HIPAA, and CCPA, ensuring that companies can use the service while adhering to various regulatory requirements.\nActivating in Amazon Connect and beyond With unified Amazon Connect Customer Profiles, organizations can activate customer data across multiple channels and at multiple touchpoints. Customers using Amazon Connect will experience a more efficient self-service experience, through either more efficient routing or more personalized responses in chat. Amazon Connect agents can access complete customer context, enabling more personalized interactions where human reasoning is required.\nAmazon Connect Customer Profiles is further enhanced with generative AI capabilities, including an AI assistant to help create actionable segments or calculated attributes to help better predict customer behavior. These features allow companies to not only understand their customers better but also anticipate their needs and preferences.\nThe new Amazon Connect Customer Profiles explorer transforms how travel and hospitality businesses understand and engage with their customers. This intuitive interface unifies fragmented customer data into a comprehensive customer-360 view, enabling business customers to transform customer data into actionable insights that drive loyalty and revenue.\nFind customers instantly using multiple identifiers simultaneously (email, phone, booking reference etc.) with real-time search results Customize views to prioritize the most relevant information for specific business needs Access complete customer context including demographic data, communication history, behavioral interactions, and segment membership Leverage AI-powered insights with customer summaries highlighting key patterns, and personalized behavioral inferences. Get started Amazon Connect Customer Profiles for travel and hospitality is available today with flexible pay-as-you-go pricing based on profile utilization. This ensures that companies only pay for what they use, making it accessible for organizations of all sizes.\nTo learn more about how Customer Profiles for travel and hospitality can transform your customer experience strategy, visit our product page or contact your AWS account team. With these powerful new capabilities, the future of personalized travel and hospitality experiences is here, powered by AWS.\n"
},
{
	"uri": "http://localhost:1313/workshop-template/5-workshop/5.3-infra-terraform/5.3.1-provider/",
	"title": "Configure Provider",
	"tags": [],
	"description": "",
	"content": "Configure Provider.tf First, we need to configure the Terraform provider and backend. The provider.tf file defines the AWS provider and version requirements:\nterraform { required_version = \u0026#34;\u0026gt;= 1.5.0\u0026#34; required_providers { aws = { source = \u0026#34;hashicorp/aws\u0026#34; version = \u0026#34;~\u0026gt; 5.0\u0026#34; } } } provider \u0026#34;aws\u0026#34; { region = var.aws_region } The provider.tf file defines: Terraform version requirement (\u0026gt;= 1.5.0) AWS provider version (~\u0026gt; 5.0) AWS region to use (from aws_region variable) "
},
{
	"uri": "http://localhost:1313/workshop-template/5-workshop/5.5-deployfe/5.5.1-deploy-frontend/",
	"title": "Deploy Frontend with Amplify",
	"tags": [],
	"description": "",
	"content": "Prepare Github Repository We will prepare a Github Repository. This Project has a Monorepo structure, containing a frontend folder and a backend folder. Here, we will deploy the frontend folder to Amplify.\nStart deploying with Amazon Amplify In the AWS Console, access AWS Amplify and select Deploy an app. Continue by selecting your source code management platform; here I am using Github so I will select Github, then click Next. Amplify will show a new window, requesting GitHub authorization to authenticate for AWS Amplify. Continue to select the organizations containing the repository you need to deploy. Here, I only install for a specific repository, so I will choose “Only select repositories” and proceed to select the repo I need to deploy, then click “Install \u0026amp; Authorize”. At the screen below, you will select your repository (1) and select the branch (2). Depending on your folder structure, if your repository root folder is the folder containing the main source code to deploy, you will not need to check “My app is monorepo”. However, for my repository, the main source code to deploy is located in the frontend folder within the root directory, so I will check (3) and enter the frontend folder (4) to help Amplify identify the directory to focus on. Then click Next. Most of this part Amplify will automatically select for you. Proceed to enter the App name and you can review before clicking Next. You can click on Advanced Settings to view and tweak your Frontend build environment to suit the project. You just need to review and click Next. Review the important information and click Save and deploy. Wait a few minutes for Amplify to proceed with the deployment. After successful Deployment, Amplify will provide us with a random Domain; depending on needs, we will use this Domain or can add a Custom Domain as desired. We will proceed to copy that Domain and paste it into the browser to view the result. Thus, you have successfully deployed the Frontend to Amazon Amplify and can access it using the domain provided by Amplify. From now on, every time you update code on GitHub (frontend branch), Amplify will automatically rebuild and redeploy after a few minutes.\nSo the next step is: connecting the Frontend with the Backend running on ECS via ALB. Currently, the ALB returns an HTTP endpoint, while Amplify serves the Frontend via HTTPS. This causes a mixed content error and the browser will block the request.\nIn the next section, we will configure HTTPS for the ALB so that the Frontend and Backend communicate securely and correctly.\n"
},
{
	"uri": "http://localhost:1313/workshop-template/4-eventparticipated/4.1-event1/",
	"title": "Event 1",
	"tags": [],
	"description": "",
	"content": "Kick-off AWS FCJ Workforce - OJT FALL 2025 Event Objectives Celebrate the students selected for the AWS First Cloud Journey – On-the-Job Training (OJT FALL 2025) Program. Cultivate a new generation of skilled AWS Builders in Vietnam. Provide hands-on expertise in Cloud Computing, DevOps, AI/ML, Security, and Data \u0026amp; Analytics. Bridge the gap between academic knowledge, cutting-edge technology, and real-world career opportunities, enabling participants to thrive in today’s tech landscape. Speakers Name Role Organization Mr. Nguyen Tran Phuoc Bao Head of Corporate Relations (QHDN) University Nguyen Gia Hung Head of Solutions Architect AWS Vietnam Do Huy Thang DevOps Lead VNG Danh Hoang Hieu Nghi GenAI Engineer Renova Bui Ho Linh Nhi AI Engineer SoftwareOne Pham Nguyen Hai Anh Cloud Engineer G-Asia Pacific Nguyen Dong Thanh Hiep Principal Cloud Engineer G-Asia Pacific Key Highlights Opening Ceremony \u0026amp; Welcome Address Welcoming remarks delivered by the University Representative.\nVision for the Future Motivation from industry leaders and expanded professional connections in Cloud, AI, and DevOps.\nExploring a DevOps Career Mastery of real-world competencies in Cloud, DevOps, AI/ML, Security, and Data \u0026amp; Analytics.\nAlumni Stories \u0026amp; Insights From First Cloud Journey participant to professional GenAI Engineer. Inspiring “She in Tech” journey. A real-life look at “A Day in the Life of a Cloud Engineer”.\nKey Takeaways Technology Access: Gain access to the latest advanced cloud computing technology. Career Direction: Be inspired, connect with experts, and expand career opportunities in Cloud Computing, AI, and DevOps. General Skills: Be equipped with practical skills (Cloud, DevOps, AI/ML, Security, Data \u0026amp; Analytics). Growth Mindset: Launch of a continuous learning and building journey to propel Vietnam’s cloud computing ecosystem forward. Event Photos "
},
{
	"uri": "http://localhost:1313/workshop-template/",
	"title": "Internship Report",
	"tags": [],
	"description": "",
	"content": "Internship Report Student Information: Full Name: Nguyen Anh Danh\nStudent id: 3121410103\nPhone Number: 0384876891\nEmail: danhanh.nguyen1643@gmail.com\nMajor: Information Technology\nClass: AWS082025\nUniversity: Sai Gon University\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern.\nInternship Duration: From 09/09/2025 to 09/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "
},
{
	"uri": "http://localhost:1313/workshop-template/5-workshop/5.1-workshop-overview/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "Introduction to WebChat Application WebChat is a real-time messaging system that allows users to communicate with each other through a modern web interface. The system supports basic features such as sending and receiving text messages, file sharing, voice messages, group chat creation, and member permissions management within groups.\nThe application backend is built using the NestJS framework, a powerful Node.js framework using TypeScript, providing a modular and scalable architecture. Real-time messaging functionality is implemented through Socket.io, enabling efficient bidirectional communication between client and server without continuous polling.\nThe frontend is developed using VueJS with the Vite build tool. Pinia is used for state management, while Socket.io-client ensures stable WebSocket connection with the backend.\nUser authentication is implemented through JWT (JSON Web Tokens), allowing secure and stateless session management.\nWorkshop Overview In this workshop, we will build a complete WebChat application from start to finish, using AWS services and modern technologies. The workshop is divided into the following main sections:\nInfrastructure Setup with Terraform: Use Terraform to define and deploy the entire AWS infrastructure following Infrastructure as Code (IaC) principles. This includes creating S3 buckets for file storage, ECR repositories for Docker images, ECS clusters and services, Application Load Balancer (ALB) for traffic distribution, along with necessary IAM roles and CloudWatch log groups.\nBackend Application Development: Build the backend API using NestJS, including main modules such as authentication (JWT), chat with WebSocket support, file upload integrated with S3, and modules for managing users, groups, messages, authentication, and chat. The backend will be containerized using Docker and deployed to Amazon ECS Fargate, a serverless container orchestration service that automatically scales and manages containers.\nDeploy Backend to AWS: After building the Docker image, we will push the image to Amazon ECR (Elastic Container Registry) and configure the ECS service to run containers. Application Load Balancer (ALB) will be used to distribute HTTP/WebSocket traffic to ECS tasks, ensuring application availability and scalability.\nFrontend Application Development: Build the user interface with VueJS, integrate Socket.io-client to connect real-time with the backend, and use Pinia for state management. The frontend will include components such as chat interface, message list, file upload, and authentication pages.\nFrontend Deployment: Deploy the Frontend with Amplify, configure Route53 to point the domain to the Hosted Zone, and set up the Frontend domain to connect to the Backend domain.\nMonitoring and Logging: After deploying the entire Backend and Frontend to the AWS platform. We configure Amazon CloudWatch Logs to collect and monitor logs from ECS tasks, create CloudWatch dashboards to monitor metrics such as request count, error rates, and response times. AWS Budgets will be set up to track and alert on AWS service usage costs.\nDatabase: The system uses MongoDB Atlas as the main database to store user data, conversations, and messages.\nAdd overall architecture diagram "
},
{
	"uri": "http://localhost:1313/workshop-template/1-worklog/1.1-week1/",
	"title": "Week 1 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1 Objectives: Read and fully understand the procedures and regulations of the First Cloud Internship program. Connect and get acquainted with members of the internship program. Understand basic AWS services, how to use the AWS Management Console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Note 2 - Watch YouTube video guide on installing and deploying the Workshop template with Hugo server.\n- Review and grasp the learning materials. 09/08/2025 09/08/2025 https://gohugo.io/host-and-deploy/host-on-github-pages/ Familiarize with workshop environment 3 - Create an AWS account.\n- Set up MFA for the AWS root account. 09/09/2025 09/09/2025 https://000001.awsstudygroup.com/ Secure root account 4 - Watch YouTube Module 01 videos and take notes on key concepts: Data Center, Region, AZ, Edge Location.\n- Create Admin Group and Admin User.\n- Explore and get familiar with AWS services via AWS Management Console. 09/10/2025 09/10/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i Core infrastructure concepts 5 - Learn about AWS Command Line Interface (CLI).\n- Explore how to request support from AWS Support. 09/11/2025 09/11/2025 https://000009.awsstudygroup.com/ Basic CLI commands \u0026amp; support process 6 - Complete Lab 000001: Set up MFA, create Admin Group \u0026amp; User, explore support requests, navigate AWS Console, and get started with AWS CLI. 09/12/2025 09/12/2025 https://000001.awsstudygroup.com/ Full hands-on Lab completion Week 1 Achievements: Fully understood the rules, processes, and regulations of the First Cloud Internship program. Successfully connected and built relationships with internship team members. Set up the working environment: Installed and ran the Hugo-based Workshop template. Created AWS account and enabled MFA. Created Admin Group and Admin User with proper permissions. Acknowledged foundational AWS infrastructure concepts: Data Center, Region, Availability Zone (AZ), Edge Location. Gained hands-on experience with AWS management tools: Navigated and used the AWS Management Console. Understood and practiced basic AWS CLI commands. Learned the process of creating and managing AWS Support requests. Successfully completed Lab 000001 with all required tasks. "
},
{
	"uri": "http://localhost:1313/workshop-template/1-worklog/1.2-week2/",
	"title": "Week 2 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 2 Objectives: Review Lab 000001 and deepen understanding of AWS cost management. Acknowledge VPC networking concepts and build secure network architectures. Gain hands-on experience with EC2 connectivity, security, and reachability analysis. Practice secure access methods to EC2 instances. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Note 2 - Review Lab 000001.\n- Complete Lab 000007: Manage costs with AWS Budgets – create budgets using templates for Cost, Usage, and Savings Plans.\n- Perform cleanup of created resources.\n- (Earn $20 reward upon Budget creation) 09/15/2025 09/15/2025 https://000007.awsstudygroup.com/ Cost control \u0026amp; cleanup 3 - Watch Module 02 videos and take notes on core concepts:\n• VPC\n• Subnet (Public \u0026amp; Private)\n• Route Table\n• Interface/Gateway Endpoints\n• Internet Gateway / NAT Gateway 09/16/2025 09/16/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i (Module 02) Networking fundamentals 4 - Complete Lab 000003.3:\n• Create VPC\n• Create Public and Private Subnets\n• Attach Internet Gateway\n• Configure Route Table\n• Create Security Groups 09/17/2025 09/17/2025 https://000003.awsstudygroup.com/vi/ Network foundation setup 5 - Continue Lab 000003 (remaining parts):\n• Launch EC2 instances in Public and Private Subnets\n• SSH into Public EC2 → Private EC2\n• Test Internet connectivity from both instances\n• Use Reachability Analyzer to verify connectivity between the two EC2s 09/18/2025 09/18/2025 https://000003.awsstudygroup.com/ EC2 networking \u0026amp; troubleshooting 6 - Practice and compare 3 EC2 connection methods:\n1. Bastion Host (Jump Server)\n2. VPC Interface Endpoint + EC2 Instance Connect\n3. AWS Systems Manager – Session Manager\n- Document pros/cons and security implications 09/19/2025 09/19/2025 https://000003.awsstudygroup.com/ Secure remote access Week 2 Achievements: Successfully reviewed Lab 000001 and applied best practices in resource management. Acknowledged AWS Budgets: Created Cost, Usage, and Savings Plans budgets using templates. Performed full resource cleanup to control costs. Earned $20 credit from AWS Budget task. Gained deep understanding of AWS VPC networking: Clearly distinguished Public vs Private Subnets, Route Tables, Internet/NAT Gateways, and Endpoints. Built a complete, functional multi-tier VPC architecture. Hands-on with EC2 in isolated networks: Launched and configured EC2 instances in both public and private subnets. Validated SSH chaining and Internet access behavior. Used Reachability Analyzer to diagnose and confirm network paths. Acknowledged secure EC2 access methods: Implemented and compared: Bastion Host (traditional SSH jump) EC2 Instance Connect Endpoint (private-only access) Session Manager (agentless, IAM-controlled, audit-ready) Understood trade-offs in security, convenience, and auditability. Fully completed Labs 000007 and 000003 with all objectives met. "
},
{
	"uri": "http://localhost:1313/workshop-template/1-worklog/1.3-week3/",
	"title": "Week 3 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 3 Objectives: Reinforce knowledge of VPC, EC2, IAM through repeated practice and resource cleanup. Deeply understand network connectivity mechanisms: Internet Gateway → Public EC2, NAT Gateway → Private EC2. Secure EC2 access methods: Bastion Host, Connect Endpoint, Session Manager. Secure Site-to-Site VPN and IAM through Lab practice. Get familiar with Windows EC2, custom AMI, Snapshot, keypair recovery, and deploy Node.js on both Linux \u0026amp; Windows. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Note 2 - Delete all resources from the previous week.\n- Re-practice Lab 000003 from scratch (entire):\n• Create VPC, Subnet, IGW, NATGW, Route Table, SG\n• Launch Public \u0026amp; Private EC2\n• SSH via Bastion Host, Connect Endpoint, Session Manager\n• Test Internet connectivity:\n– Public EC2 → via Internet Gateway\n– Private EC2 → via NAT Gateway\n• Understand why configured this way 09/22/2025 09/22/2025 https://000003.awsstudygroup.com/ Reinforce network architecture 3 - Watch Module 02 (advanced part).\n- Practice Lab 000003 – Site-to-Site VPN:\n• Create Customer Gateway, Virtual Private Gateway\n• Configure VPN Connection\n• Verify connectivity\n• Clean up resources 09/23/2025 09/23/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i (Module 02)\nAWS VPN Docs Hybrid network connectivity 4 - Study and acknowledge AWS IAM (Users, Groups, Roles, Policies, MFA).\n- Practice Lab 000002:\n• Create IAM User, Group, Role\n• Attach Policy (inline \u0026amp; managed)\n• Enable MFA\n• Test access permissions\n• Clean up 09/24/2025 09/24/2025 https://000002.awsstudygroup.com/ Access management 5 - Translate personal blog (Vietnamese translation + original English). 09/25/2025 09/25/2025 6.1 - Watch Module 03.\n- Practice Lab 000004 (part 1):\n• Create VPC + Security Group prepared for Linux \u0026amp; Windows\n• Launch EC2 Windows Instance\n• Launch EC2 Linux Instance\n• Clean up resources 09/26/2025 09/26/2025 https://000004.awsstudygroup.com/ 6.2 - Complete Lab 000004 (part 2):\n• Change instance type\n• Create Snapshot → Custom AMI\n• Lost keypair → recover using Systems Manager\n• Deploy Node.js app on:\n– EC2 Linux (User Data + PM2)\n– EC2 Windows (IIS or direct Node)\n• Clean up everything 09/26/2025 09/26/2025 https://000004.awsstudygroup.com/ Week 3 Achievements: Fully Acknowledged multi-tier VPC architecture: Clearly understood why use IGW for Public, NATGW for Private. Proficiently practiced 3 EC2 connection methods: Bastion Host → traditional, requires port 22 open Connect Endpoint → private-only, no public IP needed Session Manager → no SSH key, audit log, IAM control Acknowledged Site-to-Site VPN: Configured on-premise ↔ AWS VPC connection. Verified ping, traceroute through tunnel. Acknowledge AWS IAM: Created and managed User, Group, Role, Policy. Applied MFA, Least Privilege, Policy Conditions. Completed high-quality blog translation with accurate summary and technical terminology. Acknowledged advanced EC2: Compared Linux vs Windows EC2 (RDP vs SSH). Created Custom AMI from Snapshot. Recovered access when keypair lost using Session Manager. Successfully deployed Node.js app on both Linux (systemd/PM2) and Windows (NSSM/IIS). Completed Lab 000002, 000003 (VPN), 000004 with all requirements. "
},
{
	"uri": "http://localhost:1313/workshop-template/1-worklog/1.4-week4/",
	"title": "Week 4 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 4 Objectives: Acknowledge the Auto Scaling mechanism with EC2: Launch Template, Auto Scaling Group, Load Balancer. Understand how to manage S3 access using IAM User + Access Key. Learn about Amazon S3 as a storage platform and static website deployment. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Note 2 - Re-practice Lab 000004 to review EC2 knowledge.\n- Continue Lab 000006: Create Launch Template, Auto Scaling Group, configure Load Balancer to ensure flexible scalability. 09/29/2025 09/29/2025 https://000006.awsstudygroup.com/ Auto Scaling \u0026amp; High Availability 3 - Practice Lab 000048: Create IAM User with Access Key / Secret Access Key.\n- Upload file to S3 bucket using AWS CLI or SDK.\n- Clean up resources (delete user, bucket). 09/30/2025 09/30/2025 https://000048.awsstudygroup.com/ S3 access management 4 - Watch YouTube Module 04 about Amazon S3 (concepts: bucket, object, versioning, lifecycle, static website). 10/01/2025 10/01/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i (Module 04) S3 foundational knowledge 5 - Practice Lab 000057:\n• Prepare website files (HTML, CSS, JS).\n• Create S3 bucket, enable Static Website Hosting.\n• Configure bucket policy to allow public read.\n• Upload files and access via S3 URL. 10/02/2025 10/02/2025 https://000057.awsstudygroup.com/ Static website deployment 6 - Review all Week 4 labs and clean up all resources to avoid charges. 10/03/2025 10/03/2025 Cost control Week 4 Achievements: Acknowledged EC2 Auto Scaling: Created and configured Launch Template. Set up Auto Scaling Group with min/max/desired capacity. Integrated Elastic Load Balancer (ALB/NLB) for automatic load distribution. Acknowledged S3 access via IAM: Created IAM User with Access Key / Secret Key. Successfully uploaded files via CLI/SDK. Cleaned up user and bucket completely. Deployed static website on S3: Understood S3 bucket, static website hosting, bucket policy. Completed Lab 000057 – website accessible via public URL. "
},
{
	"uri": "http://localhost:1313/workshop-template/1-worklog/1.5-week5/",
	"title": "Week 5 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 5 Objectives: Learn relational databases with Amazon RDS. Explore Amazon Lightsail: create VPS, deploy applications, and containers. Effectively monitor systems using Amazon CloudWatch. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Note 2 - Learn about Amazon RDS (MySQL/PostgreSQL).\n- Practice Lab 000005: Create RDS instance, configure Security Group, connect to database, backup snapshot. 10/06/2025 10/06/2025 https://000005.awsstudygroup.com/ Relational database 3 - Explore Amazon Lightsail.\n- Practice Lab 000045: Create VPS, deploy basic applications (WordPress, LAMP stack, Node.js). 10/07/2025 10/07/2025 https://000045.awsstudygroup.com/ Simple VPS \u0026amp; applications 4 - Continue with Lightsail Containers.\n- Practice Lab 000046: Create container service, deploy Docker image (e.g., Nginx, custom app). 10/08/2025 10/08/2025 https://000046.awsstudygroup.com/ Container deployment 5 - Learn about Amazon CloudWatch.\n- Practice Lab 000008: Create dashboard, set up alarms (CPU, memory), collect and view logs from EC2 or Lightsail. 10/09/2025 10/09/2025 https://000008.awsstudygroup.com/ Monitoring \u0026amp; alerting 6 - Review all Week 5 labs.\n- Clean up resources: delete RDS, Lightsail instances, containers, alarms to avoid charges. 10/10/2025 10/10/2025 Cost control Week 5 Achievements: Acknowledged Amazon RDS: Created and configured RDS instance (MySQL/PostgreSQL). Connected from application/local, executed queries. Enabled automatic backups and restored from snapshot. Acknowledged Amazon Lightsail: Successfully created VPS, installed WordPress, LAMP, Node.js. Deployed stable applications with public IP. Deployed containers on Lightsail: Created container service, pushed Docker image. Accessed application via public URL. Monitored systems with CloudWatch: Built dashboards to track CPU, memory, network. Set up alarms to send notifications via email/SNS. Collected and analyzed logs from instances. Completed Lab 000005, 000045, 000046, 000008. "
},
{
	"uri": "http://localhost:1313/workshop-template/1-worklog/1.6-week6/",
	"title": "Week 6 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 6 Objectives: Acknowledge DNS with Amazon Route 53: hosted zone, record, routing policy. Acknowledge AWS CLI: installation, configuration, basic command practice. Acknowledge DynamoDB via CLI: create table, perform CRUD operations. Deploy and use ElastiCache Redis: launch cluster, SET/GET commands. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Note 2 - Practice Lab 000010: Create public hosted zone on Route 53.\n- Add records (A, CNAME, MX).\n- Configure routing policy (Simple, Weighted, Latency). 10/13/2025 10/13/2025 https://000010.awsstudygroup.com/ DNS management 3 - Practice Lab 000011: Install AWS CLI.\n- Configure profile (Access Key, Secret Key, Region).\n- Practice basic commands: aws ec2 describe-instances, aws s3 ls, aws iam list-users, aws rds describe-db-instances. 10/14/2025 10/14/2025 https://000011.awsstudygroup.com/ Admin automation 4 - Practice Lab 000060: Create DynamoDB table via CLI (aws dynamodb create-table).\n- Perform CRUD: put-item, get-item, update-item, delete-item.\n- Query and Scan data. 10/15/2025 10/15/2025 https://000060.awsstudygroup.com/ NoSQL via CLI 5 - Practice Lab 000061: Launch Redis cluster on ElastiCache.\n- Connect using redis-cli or application.\n- Practice SET key value and GET key commands. 10/16/2025 10/16/2025 https://000061.awsstudygroup.com/ In-memory cache 6 - (Reserve) Review all Week 6 labs.\n- Clean up resources: delete hosted zone, DynamoDB table, Redis cluster, remove CLI profile if needed. 10/17/2025 10/17/2025 Cost control Week 6 Achievements: Acknowledged Route 53: Created and managed public hosted zone. Configured records and flexible routing policies. Acknowledged AWS CLI: Installed and configured profile successfully. Executed EC2, S3, IAM, RDS management commands smoothly. Managed DynamoDB via CLI: Created table with primary key. Performed full CRUD, Query, Scan operations. Deployed ElastiCache Redis: Launched cluster with stable connection. Used SET/GET commands to store and retrieve in-memory data. Completed labs 000010, 000011, 000060, 000061. "
},
{
	"uri": "http://localhost:1313/workshop-template/1-worklog/1.7-week7/",
	"title": "Week 7 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 7 Objectives: Complete AWS networking knowledge through hands-on workshop. Deploy CloudFront + Lambda@Edge to optimize content delivery. Deploy WordPress deployment on AWS Cloud infrastructure. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Note 2 - Complete Lab 000092: AWS networking workshop.\n10/20/2025 10/20/2025 https://000092.awsstudygroup.com/ 3 - Practice Lab 000094: Create CloudFront distribution for S3 static website.\n10/21/2025 10/21/2025 https://000094.awsstudygroup.com/ Basic CDN 4 - Practice Lab 000130: Apply Lambda@Edge.\n- Create Lambda function (Viewer Request/Response, Origin Request).\n10/22/2025 10/22/2025 https://000130.awsstudygroup.com/vi/ Edge computing 5 - Practice Lab 000101: Deploy WordPress on AWS.\n- Create EC2 (Linux), install MySQL, configure Apache/Nginx, upload WordPress.\n10/23/2025 10/23/2025 https://000130.awsstudygroup.com/vi/ CMS on Cloud 6 - Review all Week 7 labs.\n- Clean up resources: delete CloudFront distribution, Lambda@Edge, WordPress EC2, RDS, S3 bucket. 10/24/2025 10/24/2025 Week 7 Achievements: Completed networking workshop (Lab 000092): Built multi-tier VPC architecture, distinguished public/private subnets. Accurately configured route tables, gateways, security groups. Successfully deployed CloudFront (Lab 000094): Delivered static content from S3 via global CDN. Optimized load speed and reduced latency. Applied Lambda@Edge (Lab 000130): Customized request/response at edge locations. Deployed WordPress on AWS (Lab 000101): Installed and ran WordPress stably on EC2. Connected database (RDS or local MySQL), configured domain. Completed Labs 000092, 000094, 000130, 000101. "
},
{
	"uri": "http://localhost:1313/workshop-template/1-worklog/1.8-week8/",
	"title": "Week 8 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 8 Objectives: Plan and kick off the group web chat project. Study technologies: MongoDB, Vue.js, Nest.js, Socket.IO, Docker. Set up Git repository and GitHub Pages for the workshop. Adjust worklog according to the group\u0026rsquo;s overall progress. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Note 2 - Plan the web chat project: define requirements.\n- Create schema of MongoDB collections (users, rooms, messages, \u0026hellip;).\n- Select stack: MongoDB + Vue.js + Nest.js + Socket.IO + Docker. 10/27/2025 10/27/2025 Internal Project Docs\nMongoDB University 3 - Study MongoDB: local installation, practice CRUD, aggregation, indexing.\n- Study Vue.js: component, Vue Router, Pinia (state management). 10/28/2025 10/28/2025 - https://www.mongodb.com/docs/\n- https://doc.vueframework.com/ 4 - Study Nest.js: module, controller, service, DTO, JWT auth.\n- Study Socket.IO: WebSocket, rooms, events, real-time messaging. 10/29/2025 10/29/2025 - https://docs.nestjs.com/\n- https://socket.io/docs/v4 Backend \u0026amp; real-time 5 - Study Docker: Dockerfile, docker-compose, multi-container app.\n- Group project discussion: assign tasks (frontend, backend, DevOps).\n- Adjust worklog based on group progress. 10/30/2025 10/30/2025 - https://docs.docker.com/\nTeam Meeting 6 - Create Git repository (GitHub/GitLab).\n10/31/2025 10/31/2025 GitHub Week 8 Achievements: Completed web chat project planning: Fully defined functional and non-functional requirements. Designed MongoDB schema (users, chat rooms, messages). Finalized official technology stack. Mastered core technologies: MongoDB: practiced CRUD, aggregation pipeline. Vue.js: built components, routing, state management. Nest.js: module structure, REST API, authentication. Socket.IO: implemented real-time chat events. Docker: wrote Dockerfile, docker-compose.yml. Clearly assigned group tasks: Frontend: Vue.js + UI/UX. Backend: Nest.js + Socket.IO + MongoDB. DevOps: Docker + CI/CD + AWS deployment. Set up version control system: Created Git repository with branch strategy. Deployed GitHub Pages to store workshop documentation. Synchronized group worklog to ensure consistent progress. "
},
{
	"uri": "http://localhost:1313/workshop-template/1-worklog/1.9-week9/",
	"title": "Week 9 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 9 Objectives: Set up a complete local development environment. Build a monorepo project structure with Vue.js (frontend) and Nest.js (backend). Develop a basic chat interface and real-time connection. Build WebSocket backend, manage messages and users. Design MongoDB schema and Dockerize the entire application. Run local tests with Docker Compose, ensure real-time chat works. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Note 2 - Set up local development environment: Node.js, MongoDB, Docker, VS Code.\n- Initialize monorepo with 2 apps: frontend (Vue.js), backend (Nest.js). 11/03/2025 11/03/2025 - https://doc.vueframework.com/ - https://docs.nestjs.com/ 3 - Develop Vue.js frontend: basic chat UI (login, room list, chat box).\n- Integrate Socket.IO client to send/receive real-time messages. 11/04/2025 11/04/2025 https://socket.io/docs/v4 4 - Develop Nest.js backend: WebSocket gateway, message service, user controller, etc.\n- Handle user connections, broadcast messages by room. 11/05/2025 11/05/2025 5 - Design MongoDB schema: collections for users, rooms, messages, etc.\n- Connect Mongoose in Nest.js, perform message CRUD. 11/06/2025 11/06/2025 Database modeling 6 - Dockerize the application: Dockerfile for frontend, backend, mongo.\n- Write docker-compose.yml to run the full stack locally.\n- Test real-time chat across multiple tabs/users. 11/07/2025 11/07/2025 Containerization \u0026amp; local run Week 9 Achievements: Completed local development environment: Monorepo runs smoothly with Vue.js frontend and Nest.js backend. IDE, Git, Docker fully ready. Basic chat interface operational: Login, room selection, send/receive real-time messages. Responsive UI, state management with Pinia. Nest.js backend handles WebSocket stably: Manages user connections and chat rooms. Accurate message broadcasting, no data loss. Well-designed MongoDB schema: Stores user, room, message with optimized indexes. Message CRUD via API and WebSocket. Successfully Dockerized: 3 containers: vue-frontend, nest-backend, mongo-db. docker-compose up runs the entire local system. Real-time chat works across multiple users: Tested with 3–5 tabs/browsers, instant message sync. "
},
{
	"uri": "http://localhost:1313/workshop-template/1-worklog/",
	"title": "Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1: Onboard program, secure AWS account, and grasp core infrastructure\nWeek 2: Control budgets and build end-to-end VPC + EC2 connectivity\nWeek 3: Reinforce VPN/IAM labs and deploy Node.js across Linux \u0026amp; Windows\nWeek 4: Automate EC2 scaling and host static sites on Amazon S3\nWeek 5: Operate RDS, Lightsail apps, containers, and CloudWatch monitoring\nWeek 6: Configure Route 53, master AWS CLI + DynamoDB, and launch Redis\nWeek 7: Finish networking workshop, CloudFront + Lambda@Edge, and WordPress\nWeek 8: Plan realtime chat stack and align team workflows with GitHub Pages\nWeek 9: Build Dockerized monorepo chat app with Vue, Nest, and MongoDB\nWeek 10: Polish UI, finalize APIs, and document AWS deployment architecture\nWeek 11: Deploy full stack on AWS, monitor with CloudWatch, and live test\nWeek 12: Analyze costs, optimize resources, review all AWS knowledge, and complete the final AWS FCJ report\n"
},
{
	"uri": "http://localhost:1313/workshop-template/3-blogstranslated/3.10-blog10/",
	"title": "Blog 10",
	"tags": [],
	"description": "",
	"content": "Accelerate Microservices Development with DAPR and Amazon EKS Microservices and containers are revolutionizing how modern applications are built, deployed, and managed in the cloud. However, developing and operating microservices can introduce significant complexity, often requiring developers to spend valuable time on cross-cutting concerns like service discovery, state management, and observability.\nDapr (Distributed Application Runtime) is an open source runtime for building microservices on cloud and edge. It provides platform-agnostic building blocks like service discovery, state management, pub/sub messaging, and observability out of the box. Dapr graduated from the Cloud Native Computing Foundation (CNCF) on Oct. 30, 2024, marking a major milestone in the project’s journey.\nWhen combined with Amazon Elastic Kubernetes Service (Amazon EKS), a managed Kubernetes service from AWS, Dapr can accelerate the adoption of microservices and containers, enabling developers to focus on writing business logic without worrying about infrastructure plumbing. Amazon EKS makes managing Kubernetes clusters easy, enabling effortless scaling as workloads change. In this blog post we’ll explore how Dapr simplifies microservices development on Amazon EKS and quickly demonstrate how to deploy a sample application on EKS using Dapr.\nThe Power of Dapr and Amazon EKS The combination of Dapr and Amazon EKS provides a powerful platform for building robust and scalable microservices. Some of the benefits include:\nIncreased Portability: With Dapr and Amazon EKS, microservices can be deployed to any standard Kubernetes cluster, providing increased portability across environments. Improved Resiliency: Dapr’s resiliency constructs, such as retries and circuit breakers, combined with Amazon EKS’ automated container restarts, improve overall service availability and uptime. Seamless Observability: Dapr’s monitoring integrations and Amazon EKS’ native CloudWatch monitoring deliver end-to-end visibility into microservices. Accelerated Innovation: Dapr accelerates microservices development, while Amazon EKS streamlines cluster operations, enabling developers to focus on business logic rather than infrastructure. Separation of Concerns: Dapr handles cross-cutting concerns like service discovery and pub-sub messaging, enabling clearer separation of concerns in microservices architectures. AWS Service Integration: Dapr building block’s integration with AWS services provides access to a rich ecosystem of capabilities for building robust microservices on Amazon EKS. Some examples are Amazon Dynamo DB, AWS Secrets Manager, AWS Parameter Store, Amazon SNS, Amazon SQS, Amazon Kinesis and Amazon S3. Service Invocation and State Management with Dapr We’ll start by diving into two essential building blocks of Dapr functionality: service invocation and state management.\nService Invocation: Seamless and reliable communication between microservices is crucial. However, developers often struggle with complex tasks like service discovery, standardizing APIs, securing communication channels, handling failures gracefully, and implementing observability. Your services can effortlessly communicate with each other using industry-standard protocols like gRPC and HTTP/HTTPS. Service invocation handles all the heavy lifting, from service registration and discovery to request retries, encryption, access control, and distributed tracing.\nState Management : Dapr’s state management building block simplifies the way developers work with state in their applications. It provides a consistent API for storing and retrieving state data, regardless of the underlying state store (e.g., ElastiCache, AWS DynamoDB, Azure Cosmos DB). This abstraction enables developers to build stateful applications without worrying about the complexities of managing and scaling state stores.\nApplication Architecture In this diagram, we have two microservices: a Python app and a Node.js app. The Python app generates order data and invokes the /neworder endpoint exposed by the Node.js app. The Node.js app writes the incoming order data to a state store (in this case, Amazon ElastiCache) and returns an order ID to the Python app as a response.\nBy leveraging Dapr’s service invocation building block, the Python app can seamlessly communicate with the Node.js app without worrying about service discovery, API standardization, communication channel security, failure handling, or observability. It implements mTLS to have secure service to service communication. Dapr handles these cross-cutting concerns, allowing developers to focus on writing the core business logic.\nAdditionally, Dapr’s state management building block simplifies how the Node.js app interacts with the state store (Amazon ElastiCache). Dapr provides a consistent API for storing and retrieving state data, abstracting away the complexities of managing and scaling the underlying state store. This abstraction enables developers to build stateful applications without worrying about the intricacies of state store management.\nThe Amazon EKS cluster hosts a namespace called dapr-system, which contains the Dapr control plane components. The dapr-sidecar-injector automatically injects a Dapr runtime into the pods of Dapr-enabled microservices.\nApplication and Service invocations This application architecture diagram has these service invocation steps.\nThe order generator service (Python app) invokes Node app’s method /neworder. This request is sent to the local Dapr sidecar which is running in the same pod as the Python app. Dapr resolves the target app by using the Amazon EKS cluster’s DNS provider and sends the request to Node app’s sidecar. The Node app’s sidecar then sends the request to Node app microservice. Node app then writes the order ID received from Python app to Amazon ElastiCache. The Node app sends the response to its local Dapr sidecar. Node app’s side car forwards the response to Python app’s Dapr sidecar. Python app side car returns the response to Python app which had initiated the request to Node app’s method /neworder. Connect an application to Dapr on Amazon EKS Prerequisites In order to deploy a sample application on Amazon EKS using Dapr, follow along with this post. You should have the following:\nAn AWS account. If you don’t have one, you can sign up for one. An AWS Identity and Access Management (IAM) user with proper permissions – The IAM security principal that you’re using must have permissions to work with Amazon EKS IAM roles, service linked roles, AWS CloudFormation, a VPC, and related resources. For more information, see Actions, resources, and condition keys for Amazon Elastic Container Service for Kubernetes and create a service-linked role in the IAM User Guide. To understand how Dapr works on Amazon EKS, let’s set up an Amazon EKS cluster and deploy a sample application. Then we’ll install Dapr runtime and apply Dapr configurations to the sample microservices. We’ll next test out the sample application and visualize Dapr components on a Dapr dashboard.\n1. Set up Amazon EKS Cluster Install awscli, kubectl and eksctl which are needed to execute the commands in your terminal.\nCreate an Amazon EKS cluster named eksworkshop-dapr using this example cluster-config.yaml\napiVersion: eksctl.io/v1alpha5\rkind: ClusterConfig\rmetadata:\rname: eksworkshop-dapr\rregion: us-east-2\rversion: \u0026#34;1.31\u0026#34;\rtags:\rkarpenter.sh/discovery: eksworkshop-dapr\riam:\rwithOIDC: true\rmanagedNodeGroups:\r- name: default\rdesiredCapacity: 3\rminSize: 3\rmaxSize: 5\rinstanceType: m7i.large\rprivateNetworking: true\raddons:\r- name: vpc-cni attachPolicyARNs:\r- arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy\r- name: coredns\rversion: latest - name: kube-proxy\rversion: latest\r- name: aws-ebs-csi-driver\rwellKnownPolicies: ebsCSIController: true Run this command to apply yaml and create your Amazon EKS cluster.\neksctl create cluster -f cluster-config.yaml Before proceeding with your application deployment, you’ll need to configure the security settings to enable proper Dapr sidecar communication. Use the following command to update the security rule to allow your Amazon EKS cluster to communicate with the Dapr sidecar.\naws ec2 authorize-security-group-ingress --region [your_aws_region] \\\r--group-id [your_security_group] \\\r--protocol tcp \\\r--port 4000 \\\r--source-group [your_security_group] Next you will add a default storage class which is needed for the Dapr scheduler to function properly.\nkubectl patch storageclass gp2 -p \u0026#39;{\u0026#34;metadata\u0026#34;: {\u0026#34;annotations\u0026#34;:{\u0026#34;storageclass.kubernetes.io/is-default-class\u0026#34;:\u0026#34;true\u0026#34;}}}\u0026#39; Next run this command to confirm that Amazon EKS nodes are up and running.\nkubectl get nodes You should get output like this.\nip-192-168-117-234.us-east-2.compute.internal Ready \u0026lt;none\u0026gt; 7m46s v1.31.7-eks-473151a\rip-192-168-141-0.us-east-2.compute.internal Ready \u0026lt;none\u0026gt; 7m47s v1.31.7-eks-473151a\rip-192-168-179-109.us-east-2.compute.internal Ready \u0026lt;none\u0026gt; 7m47s v1.31.7-eks-473151a The sample application leverages the AWS Load Balancer Controller, a component that orchestrates AWS Elastic Load Balancers within your Kubernetes cluster. To get started, install the AWS Load Balancer Controller by following the steps here.\n2. Install DAPR on your Amazon EKS cluster In this step you will install Dapr CLI and use it as the tool for Dapr related tasks like installing Dapr and listing Dapr components.\nRun this command to install the latest Linux Dapr CLI to /usr/local/bin\nwget -q https://raw.githubusercontent.com/dapr/cli/master/install/install.sh -O - | /bin/bash You can verify the Dapr CLI installation by running this command.\ndapr -h Next, install Dapr by using this command in your cluster context.\ndapr init -k You can verify Dapr Installation by running this command.\ndapr status -k You should get output like this\ndapr-placement-server dapr-system True Running 1 1.15.4 1m 2025-05-12 17:38.08 dapr-dashboard dapr-system True Running 1 0.15.0 1m 2025-05-12 17:38.10 dapr-sentry dapr-system True Running 1 1.15.4 1m 2025-05-12 17:38.08 dapr-operator dapr-system True Running 1 1.15.4 1m 2025-05-12 17:38.08 dapr-sidecar-injector dapr-system True Running 1 1.15.4 1m 2025-05-12 17:38.08 dapr-scheduler-server dapr-system True Running 3 1.15.4 1m 2025-05-12 17:38.08 3. Set up Amazon ElastiCache as state store In this step, you’ll create a state store for your sample application using Amazon ElastiCache Serverless. This managed service automatically scales to meet your application’s traffic demands without requiring server management.\nTo keep it simple, create an Amazon ElastiCache serverless instance in the same VPC as your Amazon EKS cluster. Configure the security group to allow incoming connections from your Amazon EKS cluster. Note down the cache endpoint, you will need this in step 5 while applying Dapr configurations.\n4. Sample Application In this step you need to clone a sample application from Dapr Quickstarts. Dapr Quickstarts has sample applications to get you started quickly with Dapr. For this tutorial we’ll use the sample applications node app and Python app which are in the hello-kubernetes directory.\ngit clone https://github.com/dapr/quickstarts.git 5. Configuring the Dapr State Store Component In this step, you’ll create and configure the Dapr state store component to communicate with Amazon ElastiCache.\nCreate a file named redis-state.yaml with the following state store configurations. Replace the redisHost value with your Amazon ElastiCache endpoint from Step 3.\nUPDATE apiVersion: dapr.io/v1alpha1 kind: Component metadata: name: statestore namespace: default spec: type: state.redis version: v1 metadata:\nname: redisHost value: redisdaprd-7rr1vd.serv3rless.use1.cache.amazonaws.com:6379 name: enableTLS value: true Apply the configuration to your cluster.\nkubectl apply -f redis-state.yaml 6. Deploying Microservices with Dapr Sidecars In this step you will deploy two microservices: a Node app which processes orders and a Python app which has the business logic to generate new orders. You obtained these apps in step 4 from Dapr Quickstart.\nTo deploy the microservice Node app, navigate to /quickstarts/tutorials/hello-kubernetes/deploy/node.yaml file. Note the crucial Dapr annotations.\nThe annotation tells the Dapr control plane to inject a side car and assigns a name to the Dapr application.\nannotations:\rdapr.io/enabled: \u0026#34;true\u0026#34;\rdapr.io/app-id: \u0026#34;nodeapp\u0026#34;\rdapr.io/app-port: \u0026#34;3000 Add AWS Load Balancer annotation to create an internet-facing AWS Load Balancer.\nkind: Service\rapiVersion: v1\rmetadata:\rname: nodeapp\rannotations:\rservice.beta.kubernetes.io/aws-load-balancer-scheme: \u0026#34;internet-facing\u0026#34;\rlabels:\rapp: node\rspec:\rselector:\rapp: node\rports:\r- protocol: TCP\rport: 80\rtargetPort: 3000\rtype: LoadBalancer Next, deploy the node app using kubectl. Navigate to the directory /quickstarts/tutorials/hello-kubernetes/deploy and execute this command.\nkubectl apply -f node.yaml Next, retrieve the load balancer endpoint. It appears under External IP on the output of the following command.\nkubectl get svc nodeapp\rhttp://k8s-default-nodeapp-3a173exxxx-f7b14bedf0c4dd8.elb.us-east-2.amazonaws.com Now that your service is deployed, let’s verify that it is working correctly. You’ll test the order submission functionality using a sample JSON payload.\nNavigate to the directory /quickstarts/tutorials/hello-kubernetes which has sample.json file to execute this step.\ncurl --request POST --data \u0026#34;@sample.json\u0026#34; --header Content-Type:application/json http://k8s-default-nodeapp-3a173exxxx-f14bedff0c4dd8.elb.us-east-2.amazonaws.com/neworder You can verify the output by accessing /order endpoint using the load balancer in a browser.\nhttp://k8s-default-nodeapp-3a173exxxx-f7b14bedff0c4dd8.elb.us-east-2.amazonaws.com/order\nYou will get output similar to {“OrderId”:“42”}\nTo complete your microservices setup, you’ll now deploy a Python application that has a business logic to generate a new order id every second. It automatically invokes /neworder endpoint of the nodeapp application.\nNavigate to the directory /quickstarts/tutorials/hello-kubernetes/deploy and execute this command.\nkubectl apply -f python.yaml 7. Validating your Dapr enabled Microservices Let’s verify that your microservices architecture is functioning correctly by checking the logs and endpoints.\nMonitor the Python app service’s Dapr sidecar logs to confirm it’s generating and sending orders. Run the following command and you should get continuous HTTP API calls with method=”POST /neworder”\nkubectl logs --selector=app=python -c daprd --tail=-1\rtime=\u0026#34;2024-03-07T12:43:11.556356346Z\u0026#34; level=info msg=\u0026#34;HTTP API Called\u0026#34; app_id=pythonapp instance=pythonapp-974db9877-dljtw method=\u0026#34;POST /neworder\u0026#34; scope=dapr.runtime.http-info type=log useragent=python-requests/2.31.0 ver=1.12.5\rtime=\u0026#34;2024-03-07T12:43:12.563193147Z\u0026#34; level=info msg=\u0026#34;HTTP API Called\u0026#34; app_id=pythonapp instance=pythonapp-974db9877-dljtw method=\u0026#34;POST /neworder\u0026#34; scope=dapr.runtime.http-info type=log useragent=python-requests/2.31.0 ver=1.12.5 Next, check the Node app application logs to confirm order processing and state persistence. Run this command and you should get order processing confirmations.\nkubectl logs —selector=app=node -c node —tail=-1\rGot a new order! Order ID: 367\rSuccessfully persisted state for Order ID: 367\rGot a new order! Order ID: 368\rSuccessfully persisted state for Order ID: 368\rGot a new order! Order ID: 369\rSuccessfully persisted state for Order ID: 369 You can also confirm whether orders are being properly stored in Amazon ElastiCache. You can append /order to the application load balancer. It returns the latest order id which was generated by the Python app.\nhttp://k8s-default-nodeapp-3a173exxxx-f7b14beff0c4dd8.elb.us-east-2.amazonaws.com/order\nYou will get an output with most recent order as as {“OrderId”:“370”}\n8. Visualizing your Dapr Components with the Dashboard Dapr provides a powerful dashboard that helps you monitor your applications, configurations, components, and logs. You can set it up with external access through Elastic Load Balancing (ELB).\nIn this section you will create a yaml file for the dashboard and apply to your Amazon EKS cluster.\nCreate a yaml file called dashboard.yaml with these annotations.\nkind: Service\rapiVersion: v1\rmetadata:\rname: dapr-mydashboard\rannotations:\rservice.beta.kubernetes.io/aws-load-balancer-scheme: \u0026#34;internet-facing\u0026#34;\rnamespace: dapr-system labels:\rapp: dapr-dashboard\rspec:\rselector:\rapp: dapr-dashboard\rports:\r- protocol: TCP\rport: 80\rtargetPort: 8080\rtype: LoadBalancer Navigate to the directory where dashboard.yaml is placed and execute this command to create a service\nkubectl apply -f dashboard.yaml Next, retrieve the ELB endpoint from dapr-mydasboard by executing the following command. We’ll use this endpoint to access the dashboard.\nkubectl get svc dapr-mydashboard -n dapr-system Access the Dapr dashboard using the ELB endpoint which we just retrieved. Here is an example endpoint URL and screen capture.\nhttp://k8s-daprsyst-daprdash-ca3914xxxx-b507661502aa8eb.elb.us-east-2.amazonaws.com/overview\nThe Dapr dashboard provides a detailed view of all Dapr applications, components and configurations running on Amazon EKS from one single place.\nClean up Run the following command to delete the deployments Node app and Python app with the state store component.\nNavigate to the directory /quickstarts/tutorials/hello-kubernetes/deploy to execute the following commands.\nkubectl delete -f node.yaml\rkubectl delete -f python.yaml You can tear down your Amazon EKS cluster using the eksctl command and delete Amazon ElascticCache.\nNavigate to the directory which has cluster.yaml file use to create the cluster in first step.\neksctl delete cluster -f cluster-config.yaml Conclusion In this post, you’ve seen how the combination of Dapr and Amazon EKS simplifies microservices development and deployment.\nThis solution offers several key benefits:\nDapr abstracts away complex distributed system patterns Amazon EKS handles the heavy lifting of Kubernetes management Developers can focus purely on business logic Built-in support for critical capabilities like state management and service-to-service communication Where to go from here:\nExplore Dapr’s distributed tracing capabilities Check out Dapr’s additional building blocks for features like pub/sub and secrets management Scale your microservices architecture with Amazon EKS’s robust orchestration By leveraging Dapr on Amazon EKS, you can accelerate your microservices development while maintaining enterprise-grade reliability and scalability. The combination provides a production-ready foundation for modern, cloud-native applications.\nTo learn more, visit the Dapr documentation and Amazon EKS documentation.\n"
},
{
	"uri": "http://localhost:1313/workshop-template/3-blogstranslated/3.11-blog11/",
	"title": "Blog 11",
	"tags": [],
	"description": "",
	"content": "Monitor, analyze, and manage capacity usage from a single interface with Amazon EC2 Capacity Manager Today, I’m happy to announce Amazon EC2 Capacity Manager, a centralized solution to monitor, analyze, and manage capacity usage across all accounts and AWS Regions from a single interface. This service aggregates capacity information with hourly refresh rates and provides prioritized optimization opportunities, streamlining capacity management workflows that previously required custom automation or manual data collection from multiple AWS services.\nOrganizations using Amazon Elastic Compute Cloud (Amazon EC2) at scale operate hundreds of instance types across multiple Availability Zones and accounts, using On-Demand Instances, Spot Instances, and Capacity Reservations. This complexity means customers currently access capacity data through various AWS services including the AWS Management Console, Cost and Usage Reports, Amazon CloudWatch, and EC2 describe APIs. This distributed approach can create operational overhead through manual data collection, context switching between tools, and the need for custom automation to aggregate information for capacity optimization analysis.\nEC2 Capacity Manager helps you overcome these operational complexities by consolidating all capacity data into a unified dashboard. You can now view cross-account and cross-Region capacity metrics for On-Demand Instances, Spot Instances, and Capacity Reservations across all commercial AWS Regions from a single location, eliminating the need to build custom data collection tools or navigate between multiple AWS services.\nThis consolidated visibility can help you discover cost savings by highlighting underutilized Capacity Reservations, analyzing usage patterns across instance types, and providing insights into Spot Instance interruption patterns. By having access to comprehensive capacity data in one place, you can make more informed decisions about rightsizing your infrastructure and optimizing your EC2 spending.\nLet me show you the capabilities of EC2 Capacity Manager in detail.\nGetting started with EC2 Capacity Manager On the AWS Management Console, I navigate to Amazon EC2 and select Capacity Manager from the navigation pane. I enable EC2 Capacity Manager through the service settings. The service aggregates historical data from the previous 14 days during initial setup.\nThe main Dashboard displays capacity utilization across all instance types through a comprehensive overview section that presents key metrics at a glance. The capacity overview cards for Reservations, Usage, and Spot show trend indicators and percentage changes to help you identify capacity patterns quickly. You can apply filtering through the date filter controls, which include date range selection, time zone configuration, and interval settings.\nYou can select different units to analyze data by vCPUs, instance counts, or estimated costs to understand resource consumption patterns. Estimated costs are based on published On-Demand rates and do not include Savings Plans or other discounts. This pricing reference helps you compare the relative impact of underutilized capacity across different instance types—for example, 100 vCPU hours of unused p5 reservations represents a larger cost impact than 100 vCPU hours of unused t3 reservations.\nThe dashboard includes detailed Usage metrics with both total usage visualization and usage over time charts. The total usage section shows the breakdown between reserved usage, unreserved usage, and Spot usage. The usage over time chart provides visualization that tracks capacity trends over time, helping you identify usage patterns and peak demand periods.\nUnder Reservation metrics, Reserved capacity trends visualizes used and unused reserved capacity across the selected period, showing the proportion of reserved vCPU hours that remain unutilized compared with those actively consumed, helping you track reservation efficiency patterns and identify periods of consistent low utilization. This visibility can help you reduce costs by identifying underutilized reservations and helping you to make informed decisions about capacity adjustments.\nThe Unused capacity section lists underutilized capacity reservations by instance type and Availability Zone combinations, displaying specific utilization percentages and instance types across different Availability Zones. This prioritized list helps you identify potential savings with direct visibility into unused capacity costs.\nThe Usage tab provides detailed historical trends and usage statistics across all AWS Regions for Spot Instances, On-Demand Instances, and Capacity Reservations. Dedicated Hosts usage is not included. The Dimension filter helps you group by and filter capacity data by Account ID, Region, Instance Family, Availability Zone, and Instance Type, creating custom views that reveal usage patterns across your accounts and AWS Organizations. This helps you analyze specific configurations and compare performance across accounts or Regions.\nThe Aggregations section provides a comprehensive usage table across EC2 and Spot Instances. You can select different units to analyze data by vCPUs, instance counts, or estimated costs to understand resource consumption patterns. The table shows instance family breakdowns with total usage statistics, reserved usage hours, unreserved usage hours, and Spot usage data. Each row includes a View breakdown action for a detailed analysis.\nThe Capacity usage or estimated cost trends section visualizes usage trends, reserved usage, unreserved usage, and Spot usage. You can filter the displayed data and adjust the unit of measurement to view historical patterns. These filtering and analysis tools help you identify usage trends, compare costs across dimensions, and make informed decisions for capacity planning and optimization.\nWhen you choose View breakdown from the Aggregations table, you access detailed Usage breakdown based on the dimension filters you selected. This breakdown view shows usage patterns for individual instance types within the selected family and Availability Zone combinations, helping you identify specific optimization opportunities.\nThe Reservations tab displays capacity reservation utilization with automated analysis capabilities that generate prioritized lists of optimization opportunities. Similar to the Usage tab, you can apply dimension filters by Account ID, Region, Instance Family, Availability Zone, and Instance Type along with additional options related to the reservation details. On each of the tabs you can drill down to see data for individual line items. For reservations specifically, you can view specific reservations and access detailed information about On-Demand Capacity Reservations (ODCRs), including utilization history, configuration parameters, and current status. When the ODCR exists in the same account as Capacity Manager, you can modify reservation parameters directly from this interface, eliminating the need to navigate to separate EC2 console sections for reservation management.\nThe Statistics section provides summary metrics, including total reservations count, overall utilization percentage, reserved capacity totals, used and unused capacity volumes, average scheduled reservations, and counts of accounts, instance families, and Regions with reservations.\nThis consolidated view helps you understand reservation distribution and utilization patterns across your infrastructure. For example, you might discover that your development accounts consistently show 30% reservation utilization while production accounts exceed 95%, indicating an opportunity to redistribute or modify reservations. Similarly, you could identify that specific instance families in certain Regions have sustained low utilization rates, suggesting candidates for reservation adjustments or workload optimization. These insights help you make data-driven decisions about reservation purchases, modifications, or cancellations to better align your reserved capacity with actual usage patterns.\nThe Spot tab focuses on Spot Instance usage and displays the amount of time your Spot instances run before being interrupted. This analysis of Spot Instance usage patterns helps you identify optimization opportunities for Spot Instance workloads. You can use Spot placement score recommendations to improve workload flexibility.\nFor organizations requiring data export capabilities, Capacity Manager includes data exports to Amazon Simple Storage Service (Amazon S3) buckets for capacity analysis. You can view and manage your data exports through the Data exports tab, which helps you create new exports, monitor delivery status, and configure export schedules to analyze capacity data outside the AWS Management Console.\nData exports extend your analytical capabilities by storing capacity data beyond the 90-day retention period available through the console and APIs. This extended retention enables long-term trend analysis and historical capacity planning. You can also integrate exported data with existing analytics workflows, business intelligence tools, or custom reporting systems to incorporate EC2 capacity metrics into broader infrastructure analysis and decision-making processes.\nThe Settings section provides configuration options for AWS Organizations integration, enabling centralized capacity management across multiple accounts. Organization administrators can enable enterprise-wide capacity visibility or delegate access to specific accounts while maintaining appropriate permissions and access controls.\nNow available EC2 Capacity Manager eliminates the operational overhead of collecting and analyzing capacity data from multiple sources. The service provides automated optimization opportunities, centralized multi-account visibility, and direct access to capacity management tools. You can reduce manual analysis time while improving capacity utilization and cost optimization across your EC2 infrastructure.\nAmazon EC2 Capacity Manager is available at no additional cost. To begin using Amazon EC2 Capacity Manager, visit the Amazon EC2 console or access the service APIs. EC2 Capacity Manager is available in all commercial AWS Regions enabled by default.\nTo learn more, visit the EC2 Capacity Manager documentation.\n"
},
{
	"uri": "http://localhost:1313/workshop-template/3-blogstranslated/3.12-blog12/",
	"title": "Blog 12",
	"tags": [],
	"description": "",
	"content": "Introducing Amazon EBS Volume Clones: Create instant copies of your EBS volumes As someone that used to work at Sun Microsystems, where ZFS was invented, I’ve always loved working with storage systems that offer instant volume copies for my development and testing needs.\nToday, I’m excited to share that AWS is bringing similar capabilities to Amazon Elastic Block Store (Amazon EBS) with the launch of Amazon EBS Volume Clones, a new capability that lets you create instant point-in-time copies of your EBS volumes within the same Availability Zone.\nMany customers need to create copies of their production data to support development and testing activities in a separate nonproduction environment. Until now, this process required taking an EBS snapshot (stored in Amazon Simple Storage Service (Amazon S3)) and then creating a new volume from that snapshot. Although this approach works, the process creates operational overhead due to multiple steps.\nWith Amazon EBS Volume Clones, you can now create copies of your EBS volumes with a single API call or console click. The copied volumes are available within seconds and provide immediate access to your data with single-digit millisecond latency. This makes Volume Clones particularly useful for quickly setting up test environments with production data or creating temporary copies of databases for development purposes.\nLet me show you how Volume Clones works For this post, I created a small Amazon Elastic Compute Cloud (Amazon EC2) instance, with an attached volume. I created a file on the root file system with the command echo \u0026quot;Hello CopyVolumes\u0026quot; \u0026gt; hello.txt.\nTo initiate the copy, I open a browser on the AWS Management Console and I navigate to EC2, Elastic Block Store, Volumes. I select the volume I want to copy.\nNote that, at the time of publication of this post, only encrypted volumes can be copied.\nOn the Actions menu, I choose the Copy Volume option.\nNext, I choose the details of the target volume. I can change the Volume type and adjust the Size, IOPS, and Throughput parameters. I choose Copy volume to start the Volume Clone operation.\nThe copied volume enters the Creating state and becomes available within seconds. I can then attach it to an EC2 instance and start using it immediately.\nData blocks are copied from the source volume and written to the volume copy in the background. The volume remains in the Initializing state until the process is complete. I can monitor its progress with the describe-volume-status API. The initializing operation doesn’t affect the performance of the source volume. I can continue using it normally during the copy process.\nI love that the copied volume is available immediately. I don’t need to wait for its initialization to complete. During the initialization phase, my copied volume delivers performance based on the lowest of: a baseline of 3,000 IOPS and 125 MiB/s, the source volume’s provisioned performance, or the copied volume’s provisioned performance.\nAfter initialization is completed, the copied volume becomes fully independent of the source volume and delivers its full provisioned performance.\nAlternatively, I can use the AWS Command Line Interface (AWS CLI) to initiate the copy:\naws ec2 copy-volumes \\ --source-volume-id vol-1234567890abcdef0 \\ --size 500 \\ --volume-type gp3 After the volume copy is created, I attach it to my EC2 instance and mount it. I can check the file I created at start is present.\nFirst, I attach the volume from my laptop, using the attach-volume command:\naws ec2 attach-volume \\ --volume-id \u0026#39;vol-09b700e3a23a9b4ad\u0026#39; \\ --instance-id \u0026#39;i-079e6504ad25b029e\u0026#39; \\ --device \u0026#39;/dev/sdb\u0026#39; Then, I connect to the instance, and I type these commands:\n$ sudo lsblk -f NAME FSTYPE FSVER LABEL UUID FSAVAIL FSUSE% MOUNTPOINTS nvme0n1 ├─nvme0n1p1 xfs / 49e26d9d-0a9d-4667-b93e-a23d1de8eacd 6.2G 22% / └─nvme0n1p128 vfat FAT16 3105-2F44 8.6M 14% /boot/efi nvme1n1 ├─nvme1n1p1 xfs / 49e26d9d-0a9d-4667-b93e-a23d1de8eacd └─nvme1n1p128 vfat FAT16 3105-2F44 $ sudo mount -t xfs /dev/nvme1n1p1 /data $ df -h Filesystem Size Used Avail Use% Mounted on devtmpfs 4.0M 0 4.0M 0% /dev tmpfs 924M 0 924M 0% /dev/shm tmpfs 370M 476K 369M 1% /run /dev/nvme0n1p1 8.0G 1.8G 6.2G 22% / tmpfs 924M 0 924M 0% /tmp /dev/nvme0n1p128 10M 1.4M 8.7M 14% /boot/efi tmpfs 185M 0 185M 0% /run/user/1000 /dev/nvme1n1p1 8.0G 1.8G 6.2G 22% /data $ cat /data/home/ec2-user/hello.txt Hello CopyVolumes Things to know Volume Clones creates copies within the same Availability Zone as your source volume. You can create copies from encrypted volumes only, and the size of your copy must be equal to or greater than the source volume.\nVolume Clones creates crash-consistent copies of your volumes, exactly like snapshots. For application consistency, you need to pause application I/O operations before creating the copy. For example, with PostgreSQL databases, you can use the pg_start_backup() and pg_stop_backup() functions to pause writes and create a consistent copy. At the operating system level on Linux with XFS, you can use the xfs_freeze command to temporarily suspend and resume access to the file system and ensure all cached updates are written to disk.\nAlthough Volume Clones creates point-in-time copies, it complements rather than replaces EBS snapshots for backup purposes. EBS snapshots remain the recommended solution for data backup and protection against AZ-level and volume failures. Snapshots provide incremental backups to Amazon S3 with 11 nines of durability, compared to Volume Clones which maintains EBS volume durability (99.999% for io2, 99.9% for other volume types). Consider using Volume Clones specifically for test and development environment scenarios where you need instant access to volume copies.\nCopied volumes exist independently of their source volumes and continue to incur standard EBS volume charges until you delete them. To manage costs effectively, implement governance rules to identify and remove copied volumes that are no longer needed for your development or testing activities.\nPricing and availability Volume Clones supports all EBS volume types and works with volumes in the same AWS account and Availability Zone. This new capability is available in all AWS commercial Regions, selected Local Zones, and in the AWS GovCloud (US).\nFor pricing, you’re charged a one-time fee per GiB of data on the source volume at initiation and standard EBS pricing for the new volume.\nI find Volume Clones particularly valuable for database workloads and continuous integration (CI) scenarios. For instance, you can quickly create a copy of your production database for testing new features or troubleshooting issues without impacting your production environment or waiting for data to hydrate from Amazon S3.\nTo get started with Amazon EBS Volume Clones, visit the Amazon EBS section on the console or check out the EBS documentation. I look forward to hearing how you use this capability to improve your development workflows.\n"
},
{
	"uri": "http://localhost:1313/workshop-template/5-workshop/5.5-deployfe/5.5.2-connect-be-fe/",
	"title": "Configure to connect Frontend and Backend",
	"tags": [],
	"description": "",
	"content": "Configure ALB to accept traffic from HTTPS. Access EC2, select Load Balancer. Select the Backend\u0026rsquo;s ALB. Select Add listener to configure ALB to listen on HTTPS 443. Select HTTPS and enter Port as 443. Check Forward to target groups, select your Backend group. In the Certificate Source section, select From ACM and select the Certificate you created earlier. And click Add listener. Next is to configure the ALB\u0026rsquo;s Security Group to allow traffic from HTTPS 443. Access EC2 Security Groups. Select the ALB\u0026rsquo;s Security Group. Select Edit inbound rules. Add HTTPS. Then click Save rules. We have set up HTTPS for the domain and configured the ALB to accept HTTPS traffic (including from Amplify) to forward to ECS.\nConfigure Rewrites and redirects for Amplify The next step, we go back to Amplify to configure the Frontend so that when sending API requests, the Frontend application will call the Backend via the ALB\u0026rsquo;s HTTPS endpoint: https://api.webchat.mom.\nWe will go back to Amplify, select Hosting -\u0026gt; Rewrites and redirects. These are the initial Rewrites and redirects. In the current Rewrites and redirects section, if you call any path, it will return an index.html page. We can see that when the Frontend calls the path /api/auth/login, Amplify applies the rule in the Rewrites and Redirects section and rewrites that request to the index.html file. Therefore, the response returned is the index.html page, as illustrated below. We will return to the Rewrites and Redirects section of Amplify to configure it so that API requests are forwarded to the ALB\u0026rsquo;s domain, instead of being rewritten to the index.html page as currently.\nIn Rewrites and redirects, we select Manage redirects. We will delete all the old lines and add new lines as shown in the image below. Then click Save. According to the image below, the meaning of each rule is as follows:\nLine 1: When Frontend sends a request to the path /api/*, Amplify will rewrite that request to https://api.webchat.mom/api/*.\nLine 2: When Frontend accesses any other path (/*), Amplify will return the root file / (i.e., index.html) of the application.” Access the website again, enter the fields, and proceed to log in. In the image below, in the Network tab, we can see the response of the login request has returned a JSON from Backend instead of an HTML page as before. So we have completed the necessary steps for Frontend and Backend to connect with each other. Next, we will refine the domain name so that users can search easily.\n"
},
{
	"uri": "http://localhost:1313/workshop-template/5-workshop/5.3-infra-terraform/5.3.2-variables/",
	"title": "Configure Variables",
	"tags": [],
	"description": "",
	"content": "Define Variables in Variables.tf The variables.tf file defines all variables used in the Terraform configuration:\nvariable \u0026#34;project_name\u0026#34; { description = \u0026#34;Project name, used to prefix resources.\u0026#34; type = string default = \u0026#34;webchat-app\u0026#34; } variable \u0026#34;aws_region\u0026#34; { description = \u0026#34;AWS region to deploy.\u0026#34; type = string default = \u0026#34;ap-southeast-2\u0026#34; } variable \u0026#34;uploads_bucket_name\u0026#34; { description = \u0026#34;S3 bucket name for storing uploads.\u0026#34; type = string } variable \u0026#34;backend_image_tag\u0026#34; { description = \u0026#34;Docker image tag for backend on ECR.\u0026#34; type = string default = \u0026#34;latest\u0026#34; } variable \u0026#34;node_env\u0026#34; { description = \u0026#34;NODE_ENV value for backend.\u0026#34; type = string default = \u0026#34;production\u0026#34; } variable \u0026#34;jwt_secret\u0026#34; { description = \u0026#34;JWT secret if you still use self-signed JWT in backend.\u0026#34; type = string sensitive = true } variable \u0026#34;jwt_expires_in\u0026#34; { description = \u0026#34;JWT access token expiration time.\u0026#34; type = string default = \u0026#34;7d\u0026#34; } variable \u0026#34;jwt_refresh_expires_in\u0026#34; { description = \u0026#34;JWT refresh token expiration time.\u0026#34; type = string default = \u0026#34;30d\u0026#34; } variable \u0026#34;cognito_callback_urls\u0026#34; { description = \u0026#34;List of callback URLs for Cognito Hosted UI (e.g., frontend URL).\u0026#34; type = list(string) default = [] } variable \u0026#34;cognito_logout_urls\u0026#34; { description = \u0026#34;List of logout URLs for Cognito Hosted UI.\u0026#34; type = list(string) default = [] } variable \u0026#34;default_tags\u0026#34; { description = \u0026#34;Common tags applied to all resources.\u0026#34; type = map(string) default = { managed-by = \u0026#34;terraform\u0026#34; } } variable \u0026#34;mongodb_uri\u0026#34; { description = \u0026#34;MongoDB URI for backend.\u0026#34; type = string } # List of emails to receive AWS Budgets alerts (cost threshold exceeded) variable \u0026#34;budget_alert_emails\u0026#34; { description = \u0026#34;List of emails that will receive alerts when AWS costs exceed budget threshold.\u0026#34; type = list(string) default = [\u0026#34;nguyenngocthanhdai2003@gmail.com\u0026#34;] } #mail smtp: variable \u0026#34;email_host\u0026#34; { description = \u0026#34;SMTP server host.\u0026#34; type = string } variable \u0026#34;email_port\u0026#34; { description = \u0026#34;SMTP server port.\u0026#34; type = number } variable \u0026#34;email_secure\u0026#34; { description = \u0026#34;Whether to use SSL.\u0026#34; type = bool } variable \u0026#34;email_user\u0026#34; { description = \u0026#34;Email account.\u0026#34; type = string } variable \u0026#34;email_pass\u0026#34; { description = \u0026#34;Email password.\u0026#34; type = string sensitive = true } "
},
{
	"uri": "http://localhost:1313/workshop-template/5-workshop/5.2-prerequiste/",
	"title": "Prerequisite",
	"tags": [],
	"description": "",
	"content": "Setup Required IAM Permissions { \u0026#34;Version\u0026#34;: \u0026#34;2025-12-05\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;ProjectTaskPermission\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34;, \u0026#34;logs:DescribeLogGroups\u0026#34;, \u0026#34;logs:DescribeLogStreams\u0026#34;, \u0026#34;ecr:GetAuthorizationToken\u0026#34;, \u0026#34;ecr:BatchCheckLayerAvailability\u0026#34;, \u0026#34;ecr:GetDownloadUrlForLayer\u0026#34;, \u0026#34;ecr:BatchGetImage\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;s3:GetBucketLocation\u0026#34;, \u0026#34;s3:GetBucketVersioning\u0026#34;, \u0026#34;cloudwatch:PutMetricData\u0026#34;, \u0026#34;cloudwatch:GetMetricStatistics\u0026#34;, \u0026#34;cloudwatch:ListMetrics\u0026#34;, \u0026#34;cloudfront:CreateInvalidation\u0026#34;, \u0026#34;cloudfront:GetDistribution\u0026#34;, \u0026#34;cloudfront:ListDistributions\u0026#34;, \u0026#34;cloudfront:GetInvalidation\u0026#34;, \u0026#34;cloudfront:ListInvalidations\u0026#34;, \u0026#34;route53:ListHostedZones\u0026#34;, \u0026#34;route53:GetHostedZone\u0026#34;, \u0026#34;route53:ListResourceRecordSets\u0026#34;, \u0026#34;route53:GetChange\u0026#34;, \u0026#34;route53:ChangeResourceRecordSets\u0026#34;, \u0026#34;amplify:GetApp\u0026#34;, \u0026#34;amplify:ListApps\u0026#34;, \u0026#34;amplify:GetBranch\u0026#34;, \u0026#34;amplify:ListBranches\u0026#34;, \u0026#34;amplify:StartDeployment\u0026#34;, \u0026#34;amplify:GetJob\u0026#34;, \u0026#34;amplify:ListJobs\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Setup Working Environment In this workshop, we will use AWS region ap-southeast-2 (Sydney) as the default.\nTo prepare for the workshop environment, you need to set up the following tools and resources:\nAWS Account and AWS CLI Configuration Log in to AWS Console and ensure you have full access to create the following resources:\nECS (clusters, services, task definitions). ECR (repositories, images). ALB (load balancers, target groups, listeners). S3 (buckets, objects). CloudWatch (log groups, metrics). IAM (roles, policies). VPC and Networking. Install AWS CLI on your device if not already installed:\n# Windows (PowerShell) msiexec.exe /i https://awscli.amazonaws.com/AWSCLIV2.msi Configure AWS CLI with your Credentials:\naws configure Enter the following information when prompted:\nAWS Access Key ID AWS Secret Access Key Default region name: ap-southeast-2 Default output format: json Verify the configuration was successful:\naws sts get-caller-identity The result will display your Account ID, User ARN, and User ID:\n2. Install Terraform Download Terraform from HashiCorp-Terraform\nExtract and add to PATH (or use package manager):\nchoco install terraform Verify installation:\nterraform version The result will display the installed Terraform version (e.g., Terraform v1.14.0):\n3. Install Other Tools Install other tools such as Docker, MongoDB, Git, \u0026hellip;\nHow to set up MongoDB Atlas (Cloud):\nSign up for a free account at mongodb.com/cloud/atlas Create a free cluster (M0 Free Tier) Get the connection string from MongoDB Atlas dashboard: Click \u0026ldquo;Connect\u0026rdquo; on the cluster Select \u0026ldquo;Connect your application\u0026rdquo; Copy the connection string (e.g., mongodb+srv://username:password@cluster.mongodb.net/) Save this connection string to use in the terraform/dev.tfvars file later. "
},
{
	"uri": "http://localhost:1313/workshop-template/2-proposal/",
	"title": "Project Proposal",
	"tags": [],
	"description": "",
	"content": "Real-time Chat Application 1. Executive Summary The Serverless Web Chat Platform is developed to provide a fast, secure, and easy-to-operate internal communication solution. The application supports real-time messaging among team members through a lightweight web interface, with flexible scalability capabilities for future expansion. The platform leverages AWS Serverless services such as AWS API Gateway, AWS Lambda, DynamoDB, and Amazon Cognito to ensure stable operation, low costs, and no server management requirements. Access is restricted to lab room members, ensuring security and privacy during information exchange.\n2. Problem Statement Current Issue\nThe team is developing a chat application for educational and research purposes on building real-time web systems. If implemented using a traditional model (self-hosted servers, self-managed databases, and maintaining WebSocket connections), the team will face numerous challenges such as: complex infrastructure setup, handling scalability with concurrent connections, ensuring stability and security, as well as comprehensive logging and monitoring for the system. Not utilizing AWS services makes it difficult for the team to simulate modern infrastructure models, while also spending time on operational tasks instead of focusing on the application core and key technical lessons.\nSolution\nThe Web Chat application is deployed based on AWS Serverless services to simulate modern application architecture with maximum scalability. The solution focuses on eliminating server management needs, maximizing instant scalability, and reducing operational costs. By using WebSocket API through CloudFront and Lambda, the solution ensures high-speed WSS communication, while applying DynamoDB for efficient handling of large read/write operations for chat data. Cognito provides a robust authentication layer, protecting the entire application from the access layer (frontend) to the API layer.\nBenefits and Return on Investment (ROI)\nThe solution enables the team to practice building a complete chat application from frontend to backend, integrated with commonly used cloud services in enterprise environments. Leveraging Free Tier and test resources keeps deployment costs low while ensuring sufficient practicality for the team to understand infrastructure management, monitoring, scaling, and security. Deployment on AWS reduces manual configuration time and creates a solid foundation for advanced research such as chatbots, user activity data processing, or AI system integration. The return on investment is nearly immediate due to no hardware costs and significant reduction in operational efforts.\n3. Solution Architecture The Web Chat application is implemented using a containerized architecture on AWS, utilizing Amazon ECS Fargate as the backend runtime platform for NestJS, while the VueJS frontend is hosted on Amazon Amplify. This architecture ensures separation between frontend and backend, ease of scaling, security, and minimization of server operational tasks.\nOverall Access Flow Users access the application through a domain managed by Amazon Route 53. Frontend requests are routed to Amazon Amplify Hosting, while backend requests to the api.webchat.mom path are forwarded to the Application Load Balancer (ALB), which directs traffic to Fargate containers in different subnets to ensure high availability.\nKey Components in the Architecture Amazon Route 53: Manages DNS, resolves custom domains webchat.mom and api.webchat.mom. Route 53 routes frontend requests to Amplify and backend requests to the Application Load Balancer.\nAmazon Amplify Hosting (Frontend): Deploys and distributes the VueJS application. Amplify also integrates with Amazon Certificate Manager (ACM) to provide HTTPS for the web interface. Additionally, Amplify uses Rewrite \u0026amp; Redirect rules to direct users to the correct API domain when accessing the frontend.\nAmazon Certificate Manager (ACM): Provides SSL/TLS certificates for both Amplify and the Application Load Balancer to ensure all communication between users and the system is encrypted.\nApplication Load Balancer (ALB): Serves as the traffic director for the backend. ALB receives requests from api.webchat.mom and routes them to ECS Fargate tasks located in public subnets, ensuring scale-out capability when request volume increases.\nAmazon ECS Fargate (Backend): Runs NestJS backend containers without managing EC2 instances. Tasks are placed in multiple subnets to enhance availability. The backend application communicates directly with MongoDB and SMTP server via Internet Gateway or VPC routing.\nAmazon ECR: Stores Docker images for the NestJS backend. During each CI/CD update, Fargate pulls images directly from ECR.\nAmazon S3: Used for storing static files such as images, attachments, or shared content in chats.\nMongoDB Atlas / MongoDB Server: Stores all user information, messages, and application data. The Fargate backend connects to MongoDB via Internet Gateway.\nSMTP Server: Used by the backend to send notification emails (if applicable).\nAmazon CloudWatch (Shared Service): Collects logs from ECS Fargate and ALB, supporting monitoring, alerting, and system performance tracking.\nAmazon IAM: Manages access permissions between services such as Fargate → ECR, ALB → CloudWatch, Amplify → S3, and applies least privilege principles.\nOperational Overview User accesses webchat.mom → Route 53 → Amplify Web. The frontend interface is loaded from Amplify. Backend requests from the frontend are sent to the api.webchat.mom domain. Route 53 routes backend requests to ALB. ALB routes to NestJS containers running on ECS Fargate. Fargate containers connect to: MongoDB for chat data processing S3 for file storage SMTP for email sending All logs are pushed to CloudWatch. Access permissions are controlled by IAM, and all traffic is encrypted via certificates from ACM. Architecture Benefits No server management required (Serverless Container – Fargate). Automatic scaling with increasing user numbers. Separation of frontend and backend for independent development. CI/CD support via Amplify and ECR → Fargate. Easy monitoring, aligned with real-world enterprise Cloud Native models. Comprehensive security from DNS to backend. 4. Technical Implementation Implementation Phases\nThe Web Chat project consists of two main parts—building the backend and frontend for the web—and deploying to AWS Cloud—spanning 5 phases:\nPrototype Development: Research VueJS, NestJS, and plan a Web chat running on LAN (1 month before internship). Research and Architecture Design: Research AWS services and design architecture suitable for the WebChat project (Month 1). Cost Calculation and Feasibility Check: Use AWS Pricing Calculator to estimate costs for ECS Fargate, Application Load Balancer (ALB), DynamoDB, Amplify, CloudFront, and CloudWatch; simultaneously evaluate container resource usage to accurately forecast costs. (Month 2). Architecture Adjustment for Cost/Solution Optimization: Optimize ECS Service configuration (CPU/memory), minimum/maximum tasks and auto-scaling; fine-tune WebSocket design on Fargate via ALB; optimize DynamoDB (PK/SK, GSI) and set up frontend caching with CloudFront to reduce backend load. (Month 3). Development, Testing, Deployment: Build backend (NestJS) running in ECS Fargate containers; develop VueJS frontend; deploy full infrastructure (ECS Fargate, ALB, DynamoDB, Amplify + CloudFront, Route53, Cognito). Perform system testing (functional, integration, load testing) and bring into operation. (Months 3–4). Technical Requirements\nBackend: Run NestJS application in ECS Fargate containers, handling API and real-time WebSocket via Application Load Balancer (ALB). Chat and user data stored on DynamoDB; logging and monitoring via CloudWatch; domain integration via Route53. Frontend: Developed with VueJS; deployed via Amplify and distributed through CloudFront for optimized loading speed and real-time chat interface performance. Realtime \u0026amp; Performance: WebSocket connections via ALB to backend on Fargate for low latency and stable connections. CloudFront caches frontend to reduce backend load and improve response times. Security \u0026amp; User Management: Use AWS Cognito for user authentication, session management, and access control for chat data. 5. Roadmap \u0026amp; Implementation Milestones Pre-internship (Month 0): 1 month for requirement survey, scope analysis, technology selection (VueJS, NestJS, ECS Fargate, ALB, Amplify, DynamoDB, CloudFront, Route53, CloudWatch) and overall architecture planning. Internship (Months 1–3): Month 1: Learn and familiarize with AWS (EC2, ECS, DynamoDB, Amplify, CloudFront, Route53, CloudWatch). Set up development environment, create NestJS backend prototype and VueJS frontend. Month 2: Design and adjust system architecture, build core features (real-time chat, message storage, basic interface). Set up infrastructure: ECS service, ALB listener, DynamoDB tables, Amplify for frontend, CloudFront CDN, Route53 for domain. Month 3: Official deployment, testing, performance optimization, CloudWatch monitoring configuration, and bring into use. Post-deployment: Continue research and feature expansion within 1 year (chatbot, data analytics, UI/UX improvements, security and cost optimization). 6. Budget Estimate Costs can be viewed on AWS Pricing Calculator\nInfrastructure Costs\nECS Fargate: 9.50 USD/month (1 task 0.25 vCPU + 0.5GB RAM running 720 hours) Application Load Balancer: 16.00 USD/month (listener + LCU + low traffic) DynamoDB: 0.50 USD/month (~50,000 Read/Write on-demand) Amplify: 0.20 USD/month CloudFront: 0.70 USD/month (Data Transfer Out ~8GB) CloudWatch: 0.10 USD/month (50MB log) Route53: 0.50 USD/month Total: 27.50 USD/month, 330 USD/12 months\n7. Risk Assessment Risk Matrix\nNetwork outage / internet issues: Medium impact, medium probability. Data errors / DynamoDB: High impact, low probability. AWS budget overrun: Medium impact, low probability. Frontend / CloudFront errors: Low impact, medium probability. Backend / ECS Fargate or ALB errors: High impact, low probability. Mitigation Strategies\nNetwork outage / Internet: Use CloudFront for frontend caching; temporarily store messages locally (localStorage/IndexedDB). Data errors / DynamoDB: Enable Point-In-Time Recovery, validate schema, monitor via CloudWatch Logs and Metrics. AWS budget overrun: Set up CloudWatch billing alarms, limit log retention, optimize Fargate tasks (CPU/RAM). Frontend / CloudFront errors: Use versioned deployments for quick rollback. Backend / ECS Fargate errors: Deploy multiple tasks as needed, use ALB health checks for automatic faulty task replacement. Contingency Plan\nUse Infrastructure as Code (CloudFormation / Terraform) for quick recreation of ECS Service, ALB, DynamoDB, Amplify, CloudFront. In case of prolonged AWS outages, run a local version (VueJS + NestJS) to maintain internal exchanges. Periodically monitor CloudWatch Dashboard, ALB health checks, and ECS task status for early issue detection. 8. Expected Outcomes Technical Improvements\nStable real-time chat application running on container architecture (ECS Fargate + ALB), replacing email or manual note exchanges. Centralized storage of messages and user data via DynamoDB, easy to manage and retrieve. Modular architecture with NestJS backend (on Fargate), VueJS frontend (Amplify + CloudFront), and AWS infrastructure (ECS, ALB, DynamoDB, CloudFront, Amplify, Route53, CloudWatch) scalable to 50–100 users. Long-term Value\nThe system can store chat data and logs for 1 year to support research, user evaluation, or AI/ML integration (chatbot, behavior analysis). Architecture and codebase can be reused for internal projects, other microservices, or as a DevOps/Cloud learning platform. Helps the team master deployment, optimization, and monitoring of cloud-native container systems on AWS. "
},
{
	"uri": "http://localhost:1313/workshop-template/1-worklog/1.10-week10/",
	"title": "Week 10 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 10 Objectives: Test, polish UI, and prepare for deployment. Add missing backend features and thoroughly test API \u0026amp; real-time. Finalize frontend UI and handle connection/state errors. Design AWS deployment architecture. Prepare API documentation and system diagrams. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Note 2 - Test backend: REST API and WebSocket (real-time).\n11/10/2025 11/10/2025 https://www.postman.com/ 3 - Finalize frontend UI.\n- Implement error handling and reconnection logic. 11/11/2025 11/11/2025 4 - Design AWS architecture:\n• Frontend: S3 + CloudFront\n• Backend: Serverless(AWS Lambda, Amazon API Gateway)\n• Database: DynamoDB\n• Real-time: WebSocket 11/12/2025 11/12/2025 AWS Architecture Icons\nDraw.io / Lucidchart 5 - Prepare documentation:\n• API docs (Swagger)\n• Deployment guide 11/13/2025 11/13/2025 Swagger UI\nC4-PlantUML 6 - Team review: review documents, diagrams, and local demo.\n- Fix bugs, optimize performance (lazy loading, debounce).\n- Prepare deployment scripts (Docker + AWS CLI). 11/14/2025 11/14/2025 Week 10 Achievements: Backend stable and feature-complete: 100% REST API coverage, WebSocket connection never drops. Frontend fully polished: Beautiful, responsive UI and emoji support. Clear AWS architecture: Detailed system diagram (frontend → CDN → backend → DB). Services chosen for optimal cost and performance. Complete documentation: Auto-generated API docs via Swagger. Step-by-step deployment guide. All code reviewed and successfully tested locally with multiple users. Fully ready for AWS deployment. "
},
{
	"uri": "http://localhost:1313/workshop-template/1-worklog/1.11-week11/",
	"title": "Week 11 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 11 Objectives: Build the workshop and finalize the proposal on Hugo. Deploy the project to AWS Cloud: networking, database, backend, frontend. Monitor the system with CloudWatch, perform live testing, and track costs. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Note 2 - Build the workshop.\n- Finalize the proposal on Hugo: write content, add diagrams, deploy the site. 11/17/2025 11/17/2025 3 - Configure AWS networking: create VPC, public subnet, Internet Gateway, Route Table.\n- Set up Security Groups for backend/frontend. 11/18/2025 11/18/2025 4 - Deploy database on DynamoDB: create cluster, user, connection string.\n- Connect Nest.js backend to DynamoDB 11/19/2025 11/19/2025 5 - Deploy backend to AWS Lambda, Amazon API Gateway\n- Deploy frontend to S3 + CloudFront: build Vue.js, upload to S3, create distribution. 11/20/2025 11/20/2025 6 - Set up CloudWatch: log groups, dashboard (CPU, memory, connections), alarms (high latency).\n- Live testing: test real-time chat with 5+ users, fix issues.\n- Track AWS costs (Cost Explorer), clean up if needed. 11/21/2025 11/21/2025 Week 11 Achievements: Finalized proposal with system diagrams. Successfully deployed to AWS: Stable VPC network with public subnet for frontend/backend. Good DynamoDB connection to backend. Backend running on EC2 with Docker, frontend accessible via CloudFront. System monitoring: CloudWatch dashboard tracking real-time metrics/logs. Alarms sending alerts for high load and low health. Comprehensive live testing: Chat works without errors with multiple users, real-time sync. Latency \u0026lt; 200ms, no dropped messages. "
},
{
	"uri": "http://localhost:1313/workshop-template/5-workshop/5.5-deployfe/5.5.3-custom-domain-be/",
	"title": "Configure Custom Domain with Route53 and SSL verification with AWS Certificate Manager",
	"tags": [],
	"description": "",
	"content": "Prepare your Domain First, you need to buy a Domain from any Domain provider you want; here I bought a Domain named webchat.mom from Porkbun. Create a Hosted Zone in Route53 and point that Domain to this HostedZone. Access Route53 and select Create hosted zones -\u0026gt; Get Started Enter the domain name you bought into the Domain Name box. Then click Create hosted zone. Enter the domain name you bought into the Domain Name box. Then click Create hosted zone. After creating the Hosted Zone, click on the newly created Hosted zone details, then copy the 4 Name Servers lines. Return to the Domain management page where you bought the Domain; here I bought it at Porkbun, I will proceed to change the 4 Name Servers of the domain to the 4 Name Servers of the Hosted Zone. Select NameServers Delete the 4 existing DNS in NameServers. Replace with the 4 DNS of the Hosted Zone from earlier. Then click Submit. Request certificate for Domain with Amazon Certificate Manager Access Amazon Certificate Manager, select Request a certificate. Select Request a public certificate, then click Next. We proceed to enter the Domain into Domain names, with line 1 being the Domain for the backend and Domain 2 being for the frontend. Scroll down and select Request. Select Create records in Route53 and click Create Record to add those 2 Domain lines to Route53\u0026rsquo;s Hosted Zone. Select the 2 Domains and click Create records. Wait a few minutes for the status to complete creating the Certificate. Return to Route53, find the created Hosted Zone, proceed to select Create record to create an Alias to the Backend ALB\u0026rsquo;s DNS. Proceed to enter the subdomain for the backend and enable Alias, then select Alias to Application and Classic Load Balancer, then select the Region and ALB you created earlier for the Backend. Then click Create records. We have completed the DNS configuration on Route 53 by creating an Alias record pointing the domain api.webchat.mom to the Application Load Balancer. So we have completed the setup and issuance of SSL certificates for the purchased Domain. Next, we will configure the necessary steps to connect Frontend and Backend.\n"
},
{
	"uri": "http://localhost:1313/workshop-template/5-workshop/5.3-infra-terraform/5.3.3-env-variable/",
	"title": "Configure Local Variables",
	"tags": [],
	"description": "",
	"content": "Define Local Variables The dev.tfvars file contains specific values for the locally used variables defined:\nproject_name = \u0026#34;webchat-app-dev\u0026#34; aws_region = \u0026#34;ap-southeast-2\u0026#34; mongodb_uri = \u0026#34;mongodb+srv://username:password@cluster.mongodb.net/\u0026#34; uploads_bucket_name = \u0026#34;webchat-app-dev-uploads-2025-sg123456\u0026#34; backend_image_tag = \u0026#34;latest\u0026#34; node_env = \u0026#34;production\u0026#34; jwt_secret = \u0026#34;your-secret-key-here\u0026#34; jwt_expires_in = \u0026#34;7d\u0026#34; jwt_refresh_expires_in = \u0026#34;30d\u0026#34; email_host = \u0026#34;smtp.gmail.com\u0026#34; email_port = 587 email_secure = false email_user = \u0026#34;your-email@gmail.com\u0026#34; email_pass = \u0026#34;your-app-password\u0026#34; default_tags = { managed-by = \u0026#34;terraform\u0026#34; env = \u0026#34;dev\u0026#34; app = \u0026#34;webchat-app\u0026#34; } The dev.tfvars file needs to be adjusted with your actual values: Replace MongoDB URI with the actual connection string. Set a unique S3 bucket name (S3 bucket names must be globally unique). Provide VPC ID and subnet IDs from your AWS account. Configure email SMTP credentials. "
},
{
	"uri": "http://localhost:1313/workshop-template/5-workshop/5.3-infra-terraform/",
	"title": "Infrastructure Setup with Terraform",
	"tags": [],
	"description": "",
	"content": "Infrastructure as Code Overview In this section, we will use Terraform to define and deploy the entire AWS infrastructure for the WebChat application. Terraform allows us to manage infrastructure in a consistent, reusable, and easily version-controlled manner.\nThe deployed infrastructure includes the following main components:\nS3 Bucket: Store files uploaded by users. ECR Repository: Registry for backend Docker images. ECS Cluster and Service: Container orchestration for backend application. Application Load Balancer: Traffic distribution and routing. IAM Roles and Policies: Access control management. CloudWatch Logs: Log collection and storage. AWS Budgets: Cost monitoring. Content Configure Provider Configure Variables Configure Local Variables Configure S3 Bucket Configure ECR Repository Configure ECS Cluster Configure Application Load Balancer and Target Group Configure IAM Roles and Policies Configure CloudWatch Logs \u0026amp; ECS Service Configure AWS Budgets Configure VPC Deploy Infrastructure After configuring all Terraform files, follow these steps to deploy the infrastructure:\nStep 1: Initialize Terraform\ncd terraform terraform init This command will download the AWS provider and initialize the backend. The result will display \u0026ldquo;Terraform has been successfully initialized!\u0026rdquo;:\nStep 2: View deployment plan\nterraform plan -var-file=\u0026#34;dev.tfvars\u0026#34; This command will display all resources that will be created. Review carefully to ensure there are no errors and resources are as expected:\nStep 3: Deploy infrastructure\nterraform apply -var-file=\u0026#34;dev.tfvars\u0026#34; Terraform will ask for confirmation. Type yes to continue. The deployment process may take 5-10 minutes:\nStep 4: Check results\nterraform output This command will display important outputs such as ALB DNS name, ECR repository URL, S3 bucket name:\nAfter successful deployment, you will have:\n1 S3 bucket with encryption and versioning. 1 ECR repository. 1 ECS cluster. 1 VPC with public subnets. 1 Application Load Balancer with target group. IAM roles and policies. CloudWatch log group. AWS Budgets. After successful deployment, you can quickly verify using AWS CLI:\naws s3 ls aws ecr describe-repositories aws ecs list-clusters aws elbv2 describe-load-balancers aws iam list-roles aws logs describe-log-groups # Replace \u0026lt;ACCOUNT_ID\u0026gt; with your AWS account ID aws budgets describe-budgets --account-id \u0026lt;ACCOUNT_ID\u0026gt; The results will display the created resources if your CLI is configured with correct permissions and region: "
},
{
	"uri": "http://localhost:1313/workshop-template/3-blogstranslated/",
	"title": "Translated Blogs",
	"tags": [],
	"description": "",
	"content": "Blog 1 - Offline caching with AWS Amplify, Tanstack, AppSync and MongoDB Atlas The article demonstrates how to build an offline-first application with optimistic UI using AWS Amplify, AWS AppSync, TanStack Query, and MongoDB Atlas. The sample to-do application showcases instant rendering of CRUD operation results on the UI before server requests complete, enhancing user experience. TanStack Query manages local caching to ensure data availability offline, while AWS Amplify and AppSync provide a full-stack framework and GraphQL API. MongoDB Atlas handles data storage, integrated with AWS Lambda and Cognito for serverless functionality and user management. The application is easily deployed via Amplify Gen 2, supporting data synchronization and a simple \u0026ldquo;first-come, first-served\u0026rdquo; conflict resolution mechanism, suitable for applications with minimal update conflicts.\nBlog 2 - Introducing AWS CDK Community Meetings AWS announces a new community meeting series for the AWS Cloud Development Kit (CDK), providing opportunities for developers, from novices to experts, to learn, ask questions, and share feedback directly with the CDK team. Held twice quarterly, these virtual meetings cover roadmap updates, feature demos, RFC reviews, and open Q\u0026amp;A, with the first sessions scheduled for June 24, 2025, at 8–9 AM and 5–6 PM PDT to accommodate the global community. Details, agendas, and recordings will be shared on GitHub and YouTube. Community members can participate via Slack, propose topics on GitHub, and provide input through surveys to shape the CDK’s future.\nBlog 3 - Secure your Express application APIs in 5 minutes with Cedar The article introduces the authorization-for-expressjs package from the open-source Cedar project, enabling Express applications on Node.js to integrate policy-based authorization in minutes, reducing code by 90% compared to manual integration. Using the PetStore sample app, Cedar decouples authorization logic, allowing fine-grained policy definitions, e.g., restricting write operations (POST /pets, POST /pets/{petId}/sale) to employees while allowing read operations (GET /pets, GET /pets/{petId}) for customers. The package auto-generates Cedar schemas from OpenAPI specs, creates sample policies, and integrates middleware for authorization without remote calls. Security is enhanced via JWT and OIDC, with policy validation easily tested using curl commands, improving developer productivity and simplifying access audits.\nBlog 4 - Amazon Bedrock baseline architecture in an AWS landing zone The article describes a reference architecture for securely managing and governing access to Amazon Bedrock\u0026rsquo;s capabilities within a multi-account AWS environment, known as an AWS landing zone. This centralized architecture consists of three primary account types: a service network account, a generative AI account, and various workload accounts. The solution leverages Amazon VPC Lattice to streamline connectivity, security, and monitoring of communications between services. This approach establishes a multi-layered defense strategy, enforcing granular access control through network-level security and VPC Lattice authentication policies, while also emphasizing the importance of monitoring usage and compliance with tools like Amazon CloudWatch and AWS CloudTrail.\nBlog 5 - Modernizing SAP Procurement Processes with Amazon Appflow, SAP BTP Integration Suite, and Amazon Bedrock The article presents a solution for modernizing SAP procurement by integrating AWS services like Amazon AppFlow and Amazon Bedrock with SAP BTP Integration Suite. This architecture automates and enhances the procurement workflow, specifically focusing on a use case for a Sustainability Sourcing Assistant. The system uses a chatbot interface and generative AI to analyze quotations based on both price and sustainability metrics. By leveraging Amazon Bedrock for natural language processing and integrating with SAP systems, the solution provides a flexible, serverless approach to transform procurement processes, making them more efficient and aligned with sustainability goals.\nBlog 6 - Optimizing Unicode Conversion Downtime for SAP Systems on Oracle to AWS The article discusses a method to minimize downtime during the Unicode conversion of SAP systems on Oracle databases when migrating to AWS. It highlights a case study of Bell Canada, which successfully migrated an 11 TB non-Unicode SAP ERP system to AWS in under 5 hours, achieving a 75% reduction in technical downtime compared to a traditional one-step conversion and migration process. The approach involves setting up a temporary non-Unicode SAP system on Amazon EC2 and replicating data from the on-premises system using Oracle Data Guard. During the planned downtime, this temporary instance becomes the source for the Unicode conversion, with export and import processes running in parallel on different EC2 instances. This method, combined with best practices like pre-checks and network reliability, leverages AWS\u0026rsquo;s scalable infrastructure to reduce business disruption and optimize operational efficiency.\nBlog 7 - GNOME has a new infrastructure partner: welcome AWS! GNOME has partnered with AWS through the Open Source Credits program to migrate its infrastructure to the cloud, addressing severe technical limitations of the legacy system that could no longer keep up with rapid growth. By leveraging AWS\u0026rsquo;s flexible infrastructure, GNOME\u0026rsquo;s small SRE team has completely overcome networking and storage issues, while simultaneously minimizing the maintenance burden, enhancing security, and optimizing costs through the use of Graviton chips. This transition enables GNOME to deliver a stable, high-performance, and scalable platform to serve its global user community.\nBlog 8 - Leveraging LLMs as an Augmentation to Traditional Hyperparameter Tuning The article introduces a breakthrough method utilizing Large Language Models (LLMs) to automatically redesign neural network architectures to improve performance, replacing resource-intensive hyperparameter tuning. Through a multi-agent workflow (using LangGraph and Amazon Bedrock), the system analyzes training metrics such as gradient norms to detect bottlenecks, thereby allowing the LLM to act as an \u0026rsquo;expert\u0026rsquo; to provide dynamic network structural adjustments. Experimental results on the CIFAR dataset demonstrate superior effectiveness, with the LLM-optimized architecture achieving an accuracy of 83% (compared to the 10% level of the baseline model), affirming the potential of applying AI to automate the development process of complex machine learning models.\nBlog 9 - Your unified view of travelers and guests from Amazon Connect Customer Profiles The article introduces Amazon Connect Customer Profiles specifically for the travel and hospitality industry, a solution that helps overcome the issue of fragmented data by automatically unifying information from over 75 sources into a unified 360-degree customer profile in real-time. This tool not only ensures compliance with global security standards but also integrates generative AI to analyze data and predict behavior, helping staff provide deeper personalized service. With ultra-low latency processing capabilities and a flexible pay-as-you-go pricing model, this is the key to helping businesses effectively enhance experiences and increase customer loyalty\nBlog 10 - Accelerate Microservices Development with DAPR and Amazon EKS The article presents a solution to accelerate microservices development by combining Dapr (Distributed Application Runtime) and Amazon EKS, helping developers focus on business logic instead of worrying about complex infrastructure. Dapr provides platform-agnostic building blocks to handle common technical issues such as state management or service communication; when combined with the cluster management and flexible scaling capabilities of Amazon EKS, it creates a highly portable system with strong resiliency and easy integration with the AWS ecosystem (such as DynamoDB, S3). This solution helps abstract away the complexity of distributed systems, delivering a stable, secure, and production-ready enterprise-grade operational platform.\nBlog 11 - Monitor, analyze, and manage capacity usage from a single interface with Amazon EC2 Capacity Manager The article introduces Amazon EC2 Capacity Manager, a free centralized management solution that helps monitor and analyze EC2 capacity across all AWS accounts and regions without the need for complex manual aggregation tools. This service consolidates data from On-Demand, Spot, and Capacity Reservations into a single interface, providing insights into usage trends and automatically detecting wasted resources to optimize costs. Besides the ability to directly adjust capacity reservations and export data to S3 for advanced analysis, the tool also integrates tightly with AWS Organizations, helping businesses make more accurate and effective infrastructure decisions.\nBlog 12 - Introducing Amazon EBS Volume Clones: Create instant copies of your EBS volumes The article introduces Amazon EBS Volume Clones, a new feature that helps create instant copies of EBS volumes within the same Availability Zone with just a simple operation, allowing for immediate data access with low latency without affecting the performance of the source volume. This solution is ideal for quickly setting up testing or development environments using real data, effectively replacing the previously time-consuming manual snapshot creation process. Although it requires the source volume to be encrypted and applies a one-time initialization fee based on capacity, Volume Clones are positioned as a workflow acceleration tool rather than a replacement for the backup and long-term data protection role of EBS Snapshots.\n"
},
{
	"uri": "http://localhost:1313/workshop-template/5-workshop/5.4-docker-deploybe/",
	"title": "Build Docker Image and Deploy Backend",
	"tags": [],
	"description": "",
	"content": "Objectives Build Docker image for backend (NestJS) using the provided Dockerfile. Push image to ECR. Update ECS service to use the new image. Verify health check and functionality after deployment. 1. Check Dockerfile File: backend/Dockerfile Start command: node dist/main.js. FROM node:20-alpine AS deps WORKDIR /app COPY package*.json ./ RUN npm ci --no-audit --no-fund FROM node:20-alpine AS build WORKDIR /app COPY --from=deps /app/node_modules ./node_modules COPY package*.json ./ COPY tsconfig*.json ./ COPY src ./src RUN npm run build FROM node:20-alpine AS prod WORKDIR /app COPY package*.json ./ RUN npm ci --omit=dev --no-audit --no-fund COPY --from=build /app/dist ./dist ENV NODE_ENV=production # Port Nest listens on – ECS/ALB is pointing to port 3000 ENV PORT=3000 EXPOSE 3000 # Start Nest app HTTP (main.ts) CMD [\u0026#34;node\u0026#34;, \u0026#34;dist/main.js\u0026#34;] 2. Login to ECR aws ecr get-login-password --region ap-southeast-2 \\ | docker login --username AWS --password-stdin 986446886396.dkr.ecr.ap-southeast-2.amazonaws.com Replace repository URI if different account/region. Terminal login to ECR successful: 3. Build image docker build -t webchat-app-dev-backend . Execute in the backend/ directory. Output build successful:\n4. Check image repository in AWS ECR Login to AWS Console and navigate to ECR repository.\nCheck the image repository in AWS ECR, in the image shown below, you can see the image webchat-app-dev-backend.\nClick on the image webchat-app-dev-backend to see the details of the image. In the picture shown above, you can see multiple tags for the image webchat-app-dev-backend. The tag latest is the latest image pushed to the repository. Click on view push commands to see the commands to push the image to the repository. 5. Tag image with ECR docker tag webchat-app-dev-backend:latest \\ 986446886396.dkr.ecr.ap-southeast-2.amazonaws.com/webchat-app-dev-backend:latest 6. Push image to ECR docker push 986446886396.dkr.ecr.ap-southeast-2.amazonaws.com/webchat-app-dev-backend:latest ECR repository with new tag/digest:\n7. Verify after deployment Quick API check via /api/auth/login: "
},
{
	"uri": "http://localhost:1313/workshop-template/5-workshop/5.3-infra-terraform/5.3.4-s3-bucket/",
	"title": "Configure S3 Bucket",
	"tags": [],
	"description": "",
	"content": "Create S3 Bucket for File Storage The s3.tf file defines an S3 bucket with security and versioning configurations:\nresource \u0026#34;aws_s3_bucket\u0026#34; \u0026#34;uploads\u0026#34; { bucket = var.uploads_bucket_name tags = merge( var.default_tags, { Name = \u0026#34;${var.project_name}-uploads\u0026#34; } ) } resource \u0026#34;aws_s3_bucket_versioning\u0026#34; \u0026#34;uploads\u0026#34; { bucket = aws_s3_bucket.uploads.id versioning_configuration { status = \u0026#34;Enabled\u0026#34; } } resource \u0026#34;aws_s3_bucket_server_side_encryption_configuration\u0026#34; \u0026#34;uploads\u0026#34; { bucket = aws_s3_bucket.uploads.id rule { apply_server_side_encryption_by_default { sse_algorithm = \u0026#34;AES256\u0026#34; } } } resource \u0026#34;aws_s3_bucket_public_access_block\u0026#34; \u0026#34;uploads\u0026#34; { bucket = aws_s3_bucket.uploads.id block_public_acls = true block_public_policy = true ignore_public_acls = true restrict_public_buckets = true } The S3 bucket is configured with:\nVersioning: Enabled to store multiple versions of files Encryption: AES256 server-side encryption to secure data Public Access Block: All public access is blocked to ensure security Bucket name must be globally unique. Ensure uploads_bucket_name in dev.tfvars is unique.\n"
},
{
	"uri": "http://localhost:1313/workshop-template/5-workshop/5.5-deployfe/5.5.4-custom-domain-fe/",
	"title": "Customize Domain for Amplify Frontend",
	"tags": [],
	"description": "",
	"content": "Add Custom Domain for Amplify Frontend. Access Amplify, select Add custom domain. Continue to select Add domain. Select the domain added to the Hosted Zone in Route53 then select Configure domain. Check Set up redirect v.v to ensure that any user entering www.webchat.mom will automatically be redirected to webchat.mom, helping to provide a consistent and optimal experience. Then click Add domain.\nWait a few minutes for the Custom Domain to be ready. And we are done. Enter webchat.mom in the browser to view the result. "
},
{
	"uri": "http://localhost:1313/workshop-template/4-eventparticipated/",
	"title": "Events Participated",
	"tags": [],
	"description": "",
	"content": "During my internship, I participated in an event that provided valuable insights into AWS Cloud technologies and industry practices.\nEvent 1 Event Name: Kick-off AWS FCJ Workforce - OJT FALL 2025\nDate \u0026amp; Time: September 06, 2025 at 09:00 AM\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\n"
},
{
	"uri": "http://localhost:1313/workshop-template/5-workshop/5.3-infra-terraform/5.3.5-ecr/",
	"title": "Configure ECR Repository",
	"tags": [],
	"description": "",
	"content": "Create ECR Repository for Docker Images The ecr.tf file defines an ECR repository to store Docker images:\nresource \u0026#34;aws_ecr_repository\u0026#34; \u0026#34;backend\u0026#34; { name = \u0026#34;${var.project_name}-backend\u0026#34; image_scanning_configuration { scan_on_push = true } tags = merge( var.default_tags, { Name = \u0026#34;${var.project_name}-backend-ecr\u0026#34; } ) } The ECR repository is configured with:\nImage Scanning: Enabled to automatically scan images when pushed. Repository name: ${var.project_name}-backend. After creating the repository, you will use the repository URL to push Docker images in the backend deployment section.\n"
},
{
	"uri": "http://localhost:1313/workshop-template/5-workshop/5.5-deployfe/",
	"title": "Deploy Frontend with Amazon Amplify and establish a secure connection with the Backend API",
	"tags": [],
	"description": "",
	"content": "Overview of Frontend Deployment with Amazon Amplify and Connecting to the Backend API . In this section, we will step-by-step deploy the Frontend application (using Vue.js) to the static Hosting service of AWS Amplify, while simultaneously establishing a secure and efficient connection with the Backend API running on ECS/ALB by using Custom Domain, AWS Route 53, AWS Certificate Manager (ACM), and Amplify\u0026rsquo;s Rewrite/Redirect mechanism.\nThe services we will need to use during the deployment process:\nAmazon Amplify : Used to deploy the Vue.js Frontend to Amplify Hosting, automatically build and distribute Frontend content to users via CDN (the underlying service of Amplify Hosting, including CloudFront). At the same time, use the Rewrite/Redirects mechanism to forward API requests to the Backend. Route 53 : Used to create a Hosted Zone and manage DNS records for the custom domain (e.g., webchat.mom). Used to point Name Servers from the third-party domain provider (Porkbun) to Route 53. Finally, create an Alias record to point the API subdomain (api.webchat.mom) to the Application Load Balancer (ALB) of the Backend. Amazon Certificate Management : Used to request and issue public SSL/TLS certificates (Public Certificate) for domains (Frontend and Backend). This certificate is then attached to the Backend\u0026rsquo;s ALB and used by Amplify Hosting to enable HTTPS. Contents Deploy Frontend to AWS Amplify Configure to connect Frontend and Backend Configure Custom Domain with Route53 and SSL verification with AWS Certificate Manager Customize Domain for Amplify Frontend "
},
{
	"uri": "http://localhost:1313/workshop-template/5-workshop/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": "Developing Web Chat Application using AWS Services Overview This workshop provides detailed guidance to build a complete WebChat application using modern technologies and services. The application is developed following a monorepo architecture with backend containerized and deployed on the AWS cloud platform.\nFrom a technology perspective, the workshop uses NestJS as the backend framework, packaged in Docker containers and deployed on Amazon ECS Fargate to ensure scalability and automatic management. The frontend is built with VueJS using the Vite build tool and deployed on CloudFront, using Socket.io-client to connect real-time with the backend and using Pinia to manage state.\nReal-time messaging functionality is implemented through Socket.io, enabling efficient bidirectional communication between client and server. Amazon S3 serves as a centralized storage repository for files uploaded by users, ensuring data availability and durability.\nContent Workshop Overview Prerequisites Infrastructure Setup with Terraform Build Docker Image and Deploy Backend Deploy Frontend Resource Cleanup "
},
{
	"uri": "http://localhost:1313/workshop-template/5-workshop/5.6-cleanup/",
	"title": "Clean up",
	"tags": [],
	"description": "",
	"content": "1. Clean up the backend service generated by terraform To clean up the backend service created by terraform, run the following command in the terminal:\nterraform destroy -var-file=\u0026#34;dev.tfvars\u0026#34; The command will prompt you to confirm the destruction of the resources. Type yes and press Enter to proceed.\nUpon successful completion, you will see a message indicating that the resources have been destroyed, but in this case, there are 2 errors which are deleting-s3-bucket and ECR-Repository. This is because the S3 bucket is not empty and the ECR repository contains images.\nIn order to completely remove the S3 bucket and ECR repository, you need to manually delete the objects in the S3 bucket and the images in the ECR repository on AWS Management Console.\n2. Delete objects in the S3 bucket Go to the S3 console.\nFind the bucket named webchat-app-dev-uploads-2025-sg123456 like in the screenshot below and click on it.\nWe need to empty the bucket before deleting it. Click on the \u0026ldquo;Empty\u0026rdquo; button.\nConfirm the action by typing \u0026ldquo;permanently delete\u0026rdquo; in the input box and click on the \u0026ldquo;Empty bucket\u0026rdquo; button. After a few moments, the bucket will be emptied.\nNow, go back to the S3 bucket list, select the bucket again, and click on the \u0026ldquo;Delete\u0026rdquo; button to delete the empty bucket.\nConfirm the deletion by typing the bucket name and clicking on the \u0026ldquo;Delete\u0026rdquo; button.\n3. Delete images in the ECR repository Go to the ECR console.\nFind the repository named webchat-app-dev-backend and click on it.\nSelect all images in the repository by checking the checkbox next to \u0026ldquo;Image Tag\u0026rdquo;.\nClick on the \u0026ldquo;Delete\u0026rdquo; button to delete the selected images.\nConfirm the image deletion by typing \u0026ldquo;delete\u0026rdquo; in the input box and clicking on the \u0026ldquo;Delete\u0026rdquo; button.\nAfter deleting all images in the repository, go back to the terminal and run the terraform destroy command again:\nterraform destroy -var-file=\u0026#34;dev.tfvars\u0026#34; This time, the command should complete successfully without any errors.\nAfter completing these steps, you have successfully cleaned up the backend service and all associated resources created by terraform.\n4. Clean up the deployed Frontend service with Amplify. Access Amazon Amplify, select the App you deployed. From the left sidebar, select App Settings -\u0026gt; General settings. In the Delete app section, select the Delete app button. Enter \u0026ldquo;delete\u0026rdquo; to confirm you want to delete this App, then click the Delete app button. A notification confirms the App deletion was successful, so the Frontend App has been cleaned up. After completing the above steps, we have finished cleaning up the Frontend on the Amplify service, next we will clean up the Hosted Zone that we created for the Domain to point to this Hosted Zone.\n5. Clean up the Hosted Zone in Route53. Access Route53 -\u0026gt; Hosted Zone, select the Hosted Zone you need to clean up. Select all Records except the 2 default Records which are NS and SOA. Select Delete records. Review and click Delete. After successful deletion, go back to Hosted Zone, select the Hosted Zone to be cleaned up, click Delete. Enter delete to confirm you want to delete this Hosted Zone, click Delete. Thus, the resources used on the AWS service have been successfully cleaned up.\n"
},
{
	"uri": "http://localhost:1313/workshop-template/5-workshop/5.3-infra-terraform/5.3.6-ecs-cluster/",
	"title": "Configure ECS Cluster",
	"tags": [],
	"description": "",
	"content": "Create ECS Cluster The ecs-cluster.tf file defines an ECS cluster:\nresource \u0026#34;aws_ecs_cluster\u0026#34; \u0026#34;backend\u0026#34; { name = \u0026#34;${var.project_name}-ecs-cluster\u0026#34; tags = var.default_tags } The ECS cluster is a container orchestration platform where ECS tasks will run. "
},
{
	"uri": "http://localhost:1313/workshop-template/6-self-evaluation/",
	"title": "Self-Evaluation",
	"tags": [],
	"description": "",
	"content": "During my internship at Amazon Web Services Vietnam Limited from September 8, 2025, to December 12, 2025, I had the valuable opportunity to apply the knowledge acquired at university to a real-world working environment while gaining numerous practical experiences.\nI participated in the development of a Web Chat application and the deployment of the product on the AWS platform. Through this process, I significantly improved my programming skills, enhanced my ability to quickly grasp new technologies, mastered project configuration and AWS service deployment, and further developed teamwork and communication skills among team members.\nIn terms of professional attitude, I always strived to complete assigned tasks to the best of my ability, strictly followed company rules and regulations, and actively discussed with colleagues to improve work efficiency.\nTo provide an objective reflection of my internship journey, I would like to self-assess myself based on the following criteria:\nNo. Criteria Description Excellent Good Average 1 Professional knowledge \u0026amp; skills Understanding of the field, application of knowledge in practice, tool proficiency, work quality ✅ ☐ ☐ 2 Learning ability Ability to absorb new knowledge quickly ✅ ☐ ☐ 3 Proactiveness Self-research and taking initiative in tasks without waiting for instructions ✅ ☐ ☐ 4 Sense of responsibility Completing tasks on time with guaranteed quality ✅ ☐ ☐ 5 Discipline Compliance with working hours, rules, and processes ✅ ☐ ☐ 6 Willingness to improve Openness to receiving feedback and continuously improving oneself ☐ ✅ ☐ 7 Communication Clear presentation of ideas and work reporting ☐ ✅ ☐ 8 Team collaboration Effective teamwork and active participation in the group ✅ ☐ ☐ 9 Professional conduct Respect toward colleagues, partners, and the working environment ✅ ☐ ☐ 10 Problem-solving mindset Ability to identify issues, propose solutions, and think creatively ☐ ✅ ☐ 11 Contribution to project/organization Work effectiveness, improvement suggestions, and recognition from the team ☐ ✅ ☐ 12 Overall performance General evaluation of the entire internship period ✅ ☐ ☐ Areas for Improvement Further strengthen problem-solving thinking and logical analysis skills. Continuously practice daily and professional communication skills, especially in handling real situations. "
},
{
	"uri": "http://localhost:1313/workshop-template/5-workshop/5.3-infra-terraform/5.3.7-alb/",
	"title": "Configure Application Load Balancer and Target Group",
	"tags": [],
	"description": "",
	"content": "Create Application Load Balancer and Target Group The alb.tf file defines ALB, security groups, target group, and listener:\nresource \u0026#34;aws_security_group\u0026#34; \u0026#34;alb_sg\u0026#34; { name = \u0026#34;${var.project_name}-alb-sg\u0026#34; description = \u0026#34;ALB security group\u0026#34; vpc_id = module.vpc.vpc_id ingress { from_port = 80 to_port = 80 protocol = \u0026#34;tcp\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] } egress { from_port = 0 to_port = 0 protocol = \u0026#34;-1\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] } tags = var.default_tags } resource \u0026#34;aws_lb\u0026#34; \u0026#34;app_alb\u0026#34; { name = \u0026#34;${var.project_name}-alb\u0026#34; load_balancer_type = \u0026#34;application\u0026#34; security_groups = [aws_security_group.alb_sg.id] subnets = module.vpc.public_subnets tags = var.default_tags } resource \u0026#34;aws_lb_target_group\u0026#34; \u0026#34;backend_tg\u0026#34; { name = \u0026#34;${var.project_name}-tg\u0026#34; port = 3000 protocol = \u0026#34;HTTP\u0026#34; vpc_id = module.vpc.vpc_id target_type = \u0026#34;ip\u0026#34; health_check { path = \u0026#34;/api/ping\u0026#34; healthy_threshold = 2 unhealthy_threshold = 2 matcher = \u0026#34;200\u0026#34; interval = 30 timeout = 5 } tags = var.default_tags } resource \u0026#34;aws_lb_listener\u0026#34; \u0026#34;http\u0026#34; { load_balancer_arn = aws_lb.app_alb.arn port = 80 protocol = \u0026#34;HTTP\u0026#34; default_action { type = \u0026#34;forward\u0026#34; target_group_arn = aws_lb_target_group.backend_tg.arn } } ALB configuration includes:\nSecurity Group: Allows inbound HTTP (port 80) from everywhere, outbound all traffic Load Balancer: Application Load Balancer type, deployed on public subnets Target Group: Listens on port 3000 (backend port), health check at /api/ping Listener: HTTP listener on port 80, forwards traffic to target group The health check path /api/ping needs to be implemented in the Backend code so that ALB can verify tasks are healthy.\n"
},
{
	"uri": "http://localhost:1313/workshop-template/7-feedback/",
	"title": "Sharing and Feedback",
	"tags": [],
	"description": "",
	"content": "Overall Evaluation 1. Working Environment\nThe working environment is extremely friendly and open. All FCJ members are always willing to help whenever I face difficulties, even outside working hours. The workspace is tidy, comfortable, and helps me stay focused. However, I suggest adding a few more casual gatherings or team-bonding activities so everyone can get to know each other better.\n2. Support from Mentor \u0026amp; Team Admin\nMy mentor provided very detailed guidance, explained concepts clearly when I didn’t understand, and always encouraged me to ask questions. The team admin was excellent in handling paperwork, preparing documents, and creating favorable conditions for my work. I truly appreciate that the mentor let me try and solve problems on my own first instead of giving direct answers.\n3. Alignment between Tasks and Academic Background\nThe tasks assigned were well-aligned with the knowledge I gained at university while also exposing me to completely new areas I had never encountered before. This combination allowed me to both reinforce my foundation and acquire practical real-world skills.\n4. Learning Opportunities \u0026amp; Skill Development\nThroughout the internship, I acquired many new skills such as using project management tools, teamwork, and professional communication in a corporate environment. My mentor also shared a lot of real-world experience that has helped me better shape my future career direction.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other, works seriously yet remains cheerful. When there’s a tight deadline, the whole team pulls together and supports one another regardless of position. This made me feel like a true part of the team, even as “just an intern.”\n6. Policies \u0026amp; Benefits for Interns\nThe company provides internship allowances and offers flexible scheduling when needed. Being able to join internal training sessions for free is a huge plus.\nAdditional Questions What satisfied you the most during the internship?\nThe feeling of learning and absorbing new knowledge every single day.\nWhat do you think the company should improve for future interns?\nIt would be helpful to prepare a more detailed learning roadmap from the very first week (e.g., weeks 1–2: learn specific tools; weeks 3–4: work on certain tasks, etc.) so that newcomers don’t feel overwhelmed during the initial 1–2 weeks.\nIf you were to recommend this internship to your friends, would you encourage them to join? Why?\nAbsolutely yes! This program doesn’t only teach technical skills—it also teaches professional working habits and problem-solving mindset.\nSuggestions \u0026amp; Expectations What suggestions do you have to improve the internship experience?\nIt would be great if the mentors could align more consistently with each other when making decisions and giving instructions.\nWould you like to continue with this program in the future?\nDefinitely yes!\n"
},
{
	"uri": "http://localhost:1313/workshop-template/5-workshop/5.3-infra-terraform/5.3.8-iam/",
	"title": "Configure IAM Roles and Policies",
	"tags": [],
	"description": "",
	"content": "Create IAM Roles and Policies The iam.tf file defines IAM roles and policies:\ndata \u0026#34;aws_iam_policy_document\u0026#34; \u0026#34;lambda_assume_role\u0026#34; { statement { effect = \u0026#34;Allow\u0026#34; principals { type = \u0026#34;Service\u0026#34; identifiers = [ \u0026#34;lambda.amazonaws.com\u0026#34;, \u0026#34;ecs-tasks.amazonaws.com\u0026#34;, ] } actions = [\u0026#34;sts:AssumeRole\u0026#34;] } } resource \u0026#34;aws_iam_role\u0026#34; \u0026#34;lambda_exec\u0026#34; { name = \u0026#34;${var.project_name}-lambda-exec\u0026#34; assume_role_policy = data.aws_iam_policy_document.lambda_assume_role.json tags = var.default_tags } resource \u0026#34;aws_iam_role_policy_attachment\u0026#34; \u0026#34;lambda_basic_execution\u0026#34; { role = aws_iam_role.lambda_exec.name policy_arn = \u0026#34;arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole\u0026#34; } data \u0026#34;aws_iam_policy_document\u0026#34; \u0026#34;lambda_app\u0026#34; { statement { effect = \u0026#34;Allow\u0026#34; actions = [ \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34;, ] resources = [\u0026#34;${aws_s3_bucket.uploads.arn}/*\u0026#34;] } } resource \u0026#34;aws_iam_policy\u0026#34; \u0026#34;lambda_app\u0026#34; { name = \u0026#34;${var.project_name}-lambda-app\u0026#34; policy = data.aws_iam_policy_document.lambda_app.json } resource \u0026#34;aws_iam_role_policy_attachment\u0026#34; \u0026#34;lambda_app\u0026#34; { role = aws_iam_role.lambda_exec.name policy_arn = aws_iam_policy.lambda_app.arn } resource \u0026#34;aws_iam_role_policy_attachment\u0026#34; \u0026#34;ecs_task_execution\u0026#34; { role = aws_iam_role.lambda_exec.name policy_arn = \u0026#34;arn:aws:iam::aws:policy/service-role/AmazonECSTaskExecutionRolePolicy\u0026#34; } The IAM role is configured with:\nAssume Role Policy: Allows ECS tasks and Lambda to assume this role AWSLambdaBasicExecutionRole: Managed policy for CloudWatch Logs permissions AmazonECSTaskExecutionRolePolicy: Managed policy for ECR pull permissions Custom Policy: Permissions for S3 (get/put/delete object) This role is used as both execution role and task role for ECS tasks.\n"
},
{
	"uri": "http://localhost:1313/workshop-template/5-workshop/5.3-infra-terraform/5.3.9-cloudwatch-ecsservices/",
	"title": "Configure CloudWatch Logs &amp; ECS Service",
	"tags": [],
	"description": "",
	"content": "CloudWatch Logs \u0026amp; ECS Service CloudWatch Log Group (in ecs-service.tf):\nresource \u0026#34;aws_cloudwatch_log_group\u0026#34; \u0026#34;backend\u0026#34; { name = \u0026#34;/ecs/${var.project_name}-backend\u0026#34; retention_in_days = 14 tags = var.default_tags } Log group name: /ecs/${var.project_name}-backend Retention: 14 days (automatically deleted to save costs) ECS Task \u0026amp; Service (in ecs-service.tf):\nresource \u0026#34;aws_security_group\u0026#34; \u0026#34;ecs_service_sg\u0026#34; { name = \u0026#34;${var.project_name}-ecs-sg\u0026#34; description = \u0026#34;ECS service security group\u0026#34; vpc_id = module.vpc.vpc_id ingress { from_port = 3000 to_port = 3000 protocol = \u0026#34;tcp\u0026#34; security_groups = [aws_security_group.alb_sg.id] } egress { from_port = 0 to_port = 0 protocol = \u0026#34;-1\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] } tags = var.default_tags } resource \u0026#34;aws_cloudwatch_log_group\u0026#34; \u0026#34;backend\u0026#34; { name = \u0026#34;/ecs/${var.project_name}-backend\u0026#34; retention_in_days = 14 tags = var.default_tags } resource \u0026#34;aws_ecs_task_definition\u0026#34; \u0026#34;backend\u0026#34; { family = \u0026#34;${var.project_name}-backend-task\u0026#34; requires_compatibilities = [\u0026#34;FARGATE\u0026#34;] network_mode = \u0026#34;awsvpc\u0026#34; cpu = \u0026#34;512\u0026#34; memory = \u0026#34;1024\u0026#34; # Có thể tạo riêng 1 role ECS task; tạm tái sử dụng role Lambda để đơn giản execution_role_arn = aws_iam_role.lambda_exec.arn task_role_arn = aws_iam_role.lambda_exec.arn container_definitions = jsonencode([ { name = \u0026#34;backend\u0026#34; image = \u0026#34;${aws_ecr_repository.backend.repository_url}:${var.backend_image_tag}\u0026#34; essential = true portMappings = [ { containerPort = 3000 protocol = \u0026#34;tcp\u0026#34; } ] environment = [ { name = \u0026#34;NODE_ENV\u0026#34; value = var.node_env }, { name = \u0026#34;MONGODB_URI\u0026#34; value = var.mongodb_uri }, { name = \u0026#34;UPLOADS_BUCKET_NAME\u0026#34; value = var.uploads_bucket_name }, { name = \u0026#34;JWT_SECRET\u0026#34; value = var.jwt_secret }, { name = \u0026#34;JWT_EXPIRES_IN\u0026#34; value = var.jwt_expires_in }, { name = \u0026#34;JWT_REFRESH_EXPIRES_IN\u0026#34; value = var.jwt_refresh_expires_in }, { name = \u0026#34;EMAIL_HOST\u0026#34; value = var.email_host }, { name = \u0026#34;EMAIL_PORT\u0026#34; value = tostring(var.email_port) }, { name = \u0026#34;EMAIL_SECURE\u0026#34; value = tostring(var.email_secure) }, { name = \u0026#34;EMAIL_USER\u0026#34; value = var.email_user }, { name = \u0026#34;EMAIL_PASS\u0026#34; value = var.email_pass }, ] logConfiguration = { logDriver = \u0026#34;awslogs\u0026#34; options = { awslogs-group = aws_cloudwatch_log_group.backend.name awslogs-region = var.aws_region awslogs-stream-prefix = \u0026#34;backend\u0026#34; } } } ]) tags = var.default_tags } resource \u0026#34;aws_ecs_service\u0026#34; \u0026#34;backend\u0026#34; { name = \u0026#34;${var.project_name}-ecs-service\u0026#34; cluster = aws_ecs_cluster.backend.id task_definition = aws_ecs_task_definition.backend.arn desired_count = 1 launch_type = \u0026#34;FARGATE\u0026#34; network_configuration { subnets = module.vpc.public_subnets security_groups = [aws_security_group.ecs_service_sg.id] assign_public_ip = true } load_balancer { target_group_arn = aws_lb_target_group.backend_tg.arn container_name = \u0026#34;backend\u0026#34; container_port = 3000 } depends_on = [aws_lb_listener.http] tags = var.default_tags } Security Group: only allows traffic from ALB to port 3000; outbound open for ECS tasks to access internet/AWS services. Task Definition: Fargate, 0.5 vCPU, 1GB RAM; uses shared role lambda_exec for execution/task; injects all env vars for DB, S3, JWT, email. Logging: awslogs driver, log group /ecs/${var.project_name}-backend, stream prefix backend. Service: runs 1 task, attached to ALB target group on port 3000, assigns public IP via public subnets. Dependencies: service depends on ALB HTTP listener (depends_on = [aws_lb_listener.http]). "
},
{
	"uri": "http://localhost:1313/workshop-template/5-workshop/5.3-infra-terraform/5.3.10-budgets/",
	"title": "Configure AWS Budgets",
	"tags": [],
	"description": "",
	"content": "Create AWS Budgets The bill-n-cost.tf file configures multiple budgets to monitor costs:\n# Monthly budget for all services resource \u0026#34;aws_budgets_budget\u0026#34; \u0026#34;monthly_cost\u0026#34; { name = \u0026#34;monthly-cost\u0026#34; limit_amount = 120 budget_type = \u0026#34;COST\u0026#34; limit_unit = \u0026#34;USD\u0026#34; time_unit = \u0026#34;MONTHLY\u0026#34; notification { comparison_operator = \u0026#34;GREATER_THAN\u0026#34; threshold = 80 threshold_type = \u0026#34;PERCENTAGE\u0026#34; notification_type = \u0026#34;ACTUAL\u0026#34; subscriber_email_addresses = var.budget_alert_emails } } # Daily budget for all services resource \u0026#34;aws_budgets_budget\u0026#34; \u0026#34;daily_cost\u0026#34; { name = \u0026#34;daily-cost\u0026#34; limit_amount = 3 budget_type = \u0026#34;COST\u0026#34; limit_unit = \u0026#34;USD\u0026#34; time_unit = \u0026#34;DAILY\u0026#34; notification { comparison_operator = \u0026#34;GREATER_THAN\u0026#34; threshold = 80 threshold_type = \u0026#34;PERCENTAGE\u0026#34; notification_type = \u0026#34;ACTUAL\u0026#34; subscriber_email_addresses = var.budget_alert_emails } } # Budget by service (daily \u0026amp; monthly) resource \u0026#34;aws_budgets_budget\u0026#34; \u0026#34;cost_by_service\u0026#34; { name = \u0026#34;cost-by-service\u0026#34; limit_amount = 0.5 budget_type = \u0026#34;COST\u0026#34; limit_unit = \u0026#34;USD\u0026#34; time_unit = \u0026#34;DAILY\u0026#34; } resource \u0026#34;aws_budgets_budget\u0026#34; \u0026#34;cost_by_service_monthly\u0026#34; { name = \u0026#34;cost-by-service-monthly\u0026#34; limit_amount = 5 budget_type = \u0026#34;COST\u0026#34; limit_unit = \u0026#34;USD\u0026#34; time_unit = \u0026#34;MONTHLY\u0026#34; } # Budget by region resource \u0026#34;aws_budgets_budget\u0026#34; \u0026#34;cost_by_region\u0026#34; { name = \u0026#34;cost-by-region\u0026#34; budget_type = \u0026#34;COST\u0026#34; limit_unit = \u0026#34;USD\u0026#34; limit_amount = 150 time_unit = \u0026#34;MONTHLY\u0026#34; notification { comparison_operator = \u0026#34;GREATER_THAN\u0026#34; threshold = 100 threshold_type = \u0026#34;PERCENTAGE\u0026#34; notification_type = \u0026#34;ACTUAL\u0026#34; subscriber_email_addresses = var.budget_alert_emails } } Monthly: 120 USD, alert at 80% (all services). Daily: 3 USD, alert at 80% (all services). By service: 0.5 USD/day, 5 USD/month (per service). By region: 150 USD/month, alert at 100%. Alert email recipients: var.budget_alert_emails. "
},
{
	"uri": "http://localhost:1313/workshop-template/5-workshop/5.3-infra-terraform/5.3.11-vpc/",
	"title": "Configure AWS VPC",
	"tags": [],
	"description": "",
	"content": "Create VPC The vpc.tf file defines a basic VPC with public subnets:\nmodule \u0026#34;vpc\u0026#34; { source = \u0026#34;terraform-aws-modules/vpc/aws\u0026#34; version = \u0026#34;~\u0026gt; 5.0\u0026#34; name = \u0026#34;${var.project_name}-vpc\u0026#34; cidr = \u0026#34;10.0.0.0/16\u0026#34; azs = [\u0026#34;ap-southeast-2a\u0026#34;, \u0026#34;ap-southeast-2b\u0026#34;] public_subnets = [\u0026#34;10.0.101.0/24\u0026#34;, \u0026#34;10.0.102.0/24\u0026#34;] private_subnets = [\u0026#34;10.0.1.0/24\u0026#34;, \u0026#34;10.0.2.0/24\u0026#34;] enable_nat_gateway = false enable_vpn_gateway = false tags = var.default_tags } Create VPC using the terraform-aws-modules/vpc/aws module to simplify network configuration. In this example, we create a VPC with CIDR 10.0.0.0/16, two Availability Zones, and public subnets. Parameters such as enable_nat_gateway and enable_vpn_gateway are set to false to keep the configuration simple. "
},
{
	"uri": "http://localhost:1313/workshop-template/1-worklog/1.12-week12/",
	"title": "Week 12 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 12 Objectives: Summarize the entire web chat internship project. Finalize the final report, workshop, and proposal. Clean up all AWS resources and analyze costs. Evaluate learning outcomes and draw lessons. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Note 2 - Compile the final report: describe the system, AWS architecture, deployment results.\n11/24/2025 11/24/2025 3 - Conduct workshop demo: present the proposal, run the live web chat system (frontend, backend, real-time).\n11/25/2025 11/25/2025 5 - Update Hugo site: add final report, demo link, system screenshots.\n- Backup all code and documents to GitHub. 11/27/2025 11/27/2025 6 - Submit Hugo workshop report 11/28/2025 11/28/2025 Week 12 Achievements: Final report completed: Detailed description covering all 11 weeks, complete proposal, and polished workshop. Live results: smooth real-time chat, multi-user support, low latency. Workshop successful: Clear presentation, stable live demo. Hugo site finalized: Full worklogs from Week 1–12, proposal, diagrams, demo screenshots. Completed the First Cloud Journey internship program. "
},
{
	"uri": "http://localhost:1313/workshop-template/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost:1313/workshop-template/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]